@inbook{Howard-Matheson1984,
    author          = {Ronald A. Howard and James E. Matheson},
    chapter        = {Influence Diagrams},
    editor         = {},
    pages          = {721-762},
    publisher      = {SDG Decision Systems},
    title          = {},
    year           = {1984}
}

@book{Diestel2017,
    author         = {Reinhard Diestel},
    year           = {2017},
    title          = {Graph Theory},
    series         = {Graduate Texts in Mathematics},
    volume         = {173},
    edition        = {5},
    url            = {https://link.springer.com/book/10.1007/978-3-662-53622-3},
    publisher      = {Springer Berlin Heidelberg}
}

@book{Preston1974,
    author         = {Christopher J. Preston},
    year           = {1974},
    title          = {Gibbs States on Countable Sets},
    series         = {Cambdirge Tracts in Mathematics},
    volume         = {68},
    edition        = {},
    url            = {https://www.cambridge.org/core/books/gibbs-states-on-countable-sets/970A88C66DA6E06BA1828142B3399272},
    publisher      = {Cambridge University Press}
}

@book{Kindermann-Snell1980,
    author         = {Ross Kindermann and J. Laurie Snell},
    year           = {1980},
    title          = {Markov Random Fields and Their Applications},
    series         = {},
    volume         = {},
    edition        = {},
    url            = {https://www.ams.org/books/conm/001/},
    publisher      = {American Mathematical Society}
}

@unpublished{Hammersley-Clifford1971,
    author = {J. Hammersley and P. Clifford},
    year   = {1971},
    title  = {Markov Fields on Finite Graphs and Lattices},
    url    = {https://ora.ox.ac.uk/objects/uuid:4ea849da-1511-4578-bb88-6a8d02f457a6}
}

@book{Baxter1982,
    author         = {Rodney J. Baxter},
    year           = {1982},
    title          = {Exactly Solved Models in Statistical Mechanics},
    series         = {},
    volume         = {},
    edition        = {},
    url            = {https://physics.anu.edu.au/research/ftp/mpg/baxter_book.php},
    publisher      = {Academic Press}
}
@book{Kittel2018,
    author         = {Charles Kittel},
    year           = {2018},
    title          = {Introduction to Solid State Physics},
    series         = {},
    volume         = {},
    edition        = {8},
    url            = {https://www.wiley.com/en-ie/Kittel%27s+Introduction+to+Solid+State+Physics%2C+Global+Edition%2C+8th+Edition-p-9781119454168},
    publisher      = {John Wiley}
}

@book{Huebener2019,
    author         = {Rudolf P. Huebener},
    year           = {2019},
    title          = {Conductors, Semiconductors, Superconductors: An Introduction to Solid-State Physics},
    series         = {Undergraduate Lecture Notes in Physics},
    volume         = {},
    edition        = {3},
    url            = {https://link-springer-com.utokyo.idm.oclc.org/book/10.1007/978-3-030-31420-0},
    publisher      = {Springer Cham}
}

@book{Boer-Pohl2018,
    author         = {Karl W Böer and Udo W. Pohl},
    year           = {2018},
    title          = {Semiconductor Physics},
    series         = {},
    volume         = {},
    edition        = {},
    url            = {https://link.springer.com/referencework/10.1007/978-3-319-69150-3},
    publisher      = {Springer Cham}
}

@book{Richard2023,
    author         = {Corey Richard},
    year           = {2023},
    title          = {Understanding Semiconductors: A Technical Guide for Non-Technical People},
    series         = {Maker Innovations Series},
    volume         = {},
    edition        = {},
    url            = {https://link.springer.com/book/10.1007/978-1-4842-8847-4},
    publisher      = {Apress Berkeley}
}

@book{Miller2022,
    author         = {Chris Miller},
    year           = {2022},
    title          = {Chip War: The Fight for the World's Most Critical Technology},
    series         = {},
    volume         = {},
    edition        = {},
    url            = {https://www.simonandschuster.com/books/Chip-War/Chris-Miller/9781982172008},
    publisher      = {Scribner}
}

@book{Evstigneev2022,
    author         = {Mykhaylo Evstigneev},
    year           = {2022},
    title          = {Introduction to Semiconductor Physics and Devices},
    series         = {},
    volume         = {},
    edition        = {},
    url            = {https://link.springer.com/book/10.1007/978-3-031-08458-4},
    publisher      = {Springer Cham}
}

@book{Lau2021,
    author         = {John H. Lau},
    year           = {2021},
    title          = {Semiconductor Advanced Packaging},
    series         = {},
    volume         = {},
    edition        = {},
    url            = {https://link.springer.com/book/10.1007/978-981-16-1376-0},
    publisher      = {Springer Singapore}
}

@book{SemiconductorHandbook2023,
    author         = {},
    editor         = {Massimo Rudan and Rossella Brunetti and Susanna Reggiani},
    year           = {2023},
    title          = {Springer Handbook of Semiconductor Devices},
    series         = {Springer Handbooks},
    volume         = {},
    edition        = {},
    url            = {https://link.springer.com/book/10.1007/978-3-030-79827-7},
    publisher      = {Springer Cham}
}

@book{菊池正典2023,
    author         = {菊池正典},
    year           = {2023},
    month          = {3},
    title          = {半導体産業のすべて：世界の先端企業から日本メーカーの展望まで},
    series         = {},
    volume         = {},
    edition        = {},
    url            = {https://www.diamond.co.jp/book/9784478117118.html},
    publisher      = {ダイヤモンド社}
}

@book{ずーぼ2024,
    author         = {ずーぼ},
    year           = {2024},
    month          = {1},
    title          = {今と未来がわかる半導体},
    series         = {},
    volume         = {},
    edition        = {},
    url            = {https://www.natsume.co.jp/books/19325},
    publisher      = {ナツメ社}
}

@book{May-Spanos2006,
    author         = {Gary S. May and Costas J. Spanos},
    year           = {2006},
    title          = {Fundamentals of Semiconductor Manufacturing and Process Control},
    series         = {},
    volume         = {},
    edition        = {},
    url            = {https://onlinelibrary.wiley.com/doi/book/10.1002/0471790281},
    publisher      = {John Wiley \& Sons}
}

@book{Sze-Lee2012,
    author         = {Simon M. Sze and Ming-Kwei Lee},
    year           = {2012},
    title          = {Semiconductor Devices: Physics and Technology},
    series         = {},
    volume         = {},
    edition        = {3},
    url            = {https://www.wiley.com/en-us/Semiconductor+Devices:+Physics+and+Technology,+3rd+Edition-p-9780470537947},
    publisher      = {John Wiley \& Sons}
}

@book{May-Sze2003,
    author         = {Gary S. May and Simon M. Sze},
    year           = {2003},
    title          = {Fundamentals of Semiconductor Fabrication},
    series         = {},
    volume         = {},
    edition        = {},
    url            = {https://www.wiley.com/en-us/Fundamentals+of+Semiconductor+Fabrication-p-9780471232797},
    publisher      = {John Wiley \& Sons}
}


@incollection{VanRossum2005,
	address = {Oxford},
	author = {M. {Van Rossum}},
	booktitle = {Encyclopedia of Condensed Matter Physics (Second Edition)},
	doi = {10.1016/B978-0-323-90800-9.00292-4},
	edition = {Second Edition},
	editor = {Tapash Chakraborty},
	isbn = {978-0-323-91408-6},
	keywords = {85.30.--z, 85.40.--e, 85.40.Hp, 85.40.Ls, 85.40.Qx, 85.40.Ry, 85.40.Sz},
	pages = {748-756},
	publisher = {Academic Press},
	title = {Integrated Circuits},
	url = {https://www.sciencedirect.com/science/article/pii/B9780323908009002924},
	year = {2005},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/B9780323908009002924},
	bdsk-url-2 = {https://doi.org/10.1016/B978-0-323-90800-9.00292-4}}

@book{Patterson-Hennessy2014,
    author         = {David A. Patterson and John L. Hennessy},
    year           = {2014},
    title          = {Computer Organization and Design: The Hardware/Software Interface},
    series         = {},
    volume         = {},
    edition        = {5},
    url            = {},
    publisher      = {Elsevier}
}

@book{Pierret2003,
    author         = {Robert F. Pierret},
    year           = {2003},
    title          = {Advanced Semiconductor Fundamentals},
    series         = {Modular Series on Solid State Devices},
    volume         = {VI},
    edition        = {2},
    url            = {https://www.pearson.com/en-us/subject-catalog/p/advanced-semiconductor-fundamentals/P200000003285/9780130617927},
    publisher      = {Pearson}
}

@book{Ng2002,
    author         = {Kwok K. Ng},
    year           = {2002},
    title          = {Complete Guide to Semiconductor Devices},
    series         = {},
    volume         = {},
    edition        = {},
    url            = {https://onlinelibrary.wiley.com/doi/book/10.1002/9781118014769},
    publisher      = {John Wiley \& Sons}
}

@misc{Dennard1967,
    author       = {Robert H. Dennard},
    howpublished = {U.S. Patent 3,387,286A},
    title        = {Field Effect Transistor Memory},
    year         = {1967},
    url          = {https://patents.google.com/patent/US3387286A/en},
}

@misc{Andrus1957,
    author       = {Jules Andrus},
    howpublished = {U.S. Patent 3,122,817A},
    title        = {Fabrication of Semiconductor Devices},
    year         = {1957},
    url          = {https://patents.google.com/patent/US3122817A/en},
}

@article{Czochralski1918,
    author          = {J. Czochralski},
    year            = {1918},
    title           = {Ein neues Verfahren zur Messung der Kristallisationsgeschwindigkeit der Metalle},
    journal         = {Zeitschrift für Physikalische Chemie},
    volume          = {92U},
    number          = {1},
    pages           = {219-221},
    url             = {https://doi.org/10.1515/zpch-1918-9212}
}

@article{Bridgman1925,
    author          = {P. W. Bridgman},
    year            = {1925},
    title           = {Certain Physical Properties of Single Crystals of Tungsten, Antimony, Bismuth, Tellurium, Cadmium, Zinc, and Tin},
    journal         = {Proceedings of the American Academy of Arts and Sciences},
    volume          = {60},
    number          = {6},
    pages           = {305-383},
    url             = {https://www.jstor.org/stable/25130058}
}

@article{Welker1952,
    author          = {H. Welker},
    year            = {1952},
    title           = {Über neue halbleitende Verbindungen},
    journal         = {Zeitschrift für Naturforschung A},
    volume          = {7},
    number          = {11},
    pages           = {744-749},
    url             = {https://doi.org/10.1515/zna-1952-1110}
}

@article{Frosch-Derick1957,
    author          = {C. J. Frosch and L. Derick},
    year            = {1957},
    title           = {Surface Protection and Selective Masking during Diffusion in Silicon},
    journal         = {Journal of the Electrochemical Society},
    volume          = {104},
    number          = {9},
    pages           = {547},
    url             = {https://iopscience.iop.org/article/10.1149/1.2428650}
}

@misc{Kilby1959,
    author       = {Jack S. Kilby},
    howpublished = {U.S. Patent 3,138,743A},
    title        = {Miniaturized Electronic Circuits},
    year         = {1959},
    url          = {https://patents.google.com/patent/US3138743A/en},
}

@misc{Noyce1959,
    author       = {Robert N. Noyce},
    howpublished = {U.S. Patent 2,981,877A},
    title        = {Semiconductor Device-and-Lead Structure},
    year         = {1959},
    url          = {https://patents.google.com/patent/US2981877A/en},
}

@article{Hoff+1996,
    author          = {M. E. Hoff and F. Faggin and S. Mazor and M. Shima},
    year            = {1996},
    title           = {The history of the 4004},
    journal         = {IEEE Micro},
    volume          = {16},
    number          = {6},
    pages           = {10-20},
    url             = {https://ieeexplore.ieee.org/document/546561}
}

@inproceedings{Hoerni1960,
    author          = {J. A. Hoerni},
    year            = {1960},
    title           = {Planar silicon diodes and transistors},
    booktitle       = {1960 International Electron Devices Meeting},
    volume          = {},
    pages           = {},
    url             = {https://ieeexplore.ieee.org/document/1472833}
}


@article{Round1907,
    author          = {H. J. Round},
    year            = {1907},
    title           = {A Note On Carborundum},
    journal         = {Electrical World},
    volume          = {19},
    number          = {},
    pages           = {309-310},
    url             = {https://www.worldscientific.com/doi/abs/10.1142/9789814503464_0116}
}

@article{Shockley1949,
    author          = {W. Shockley},
    year            = {1949},
    title           = {The Theory of $p$-$n$ Junction in Semiconductors and $p$-$n$ Hunction Transistors},
    journal         = {The Bell System Technical Journal},
    volume          = {28},
    number          = {3},
    pages           = {435-489},
    url             = {https://ieeexplore.ieee.org/document/6773080}
}

@article{Kahng-Atalla1960,
    author          = {D. Kahng and M. M. Atalla},
    year            = {1960},
    title           = {Silicon-Silicon Dioxide Surface Device},
    journal         = {IRE Device Reserach Conference, Pittsburgh},
    volume          = {},
    number          = {},
    pages           = {},
    url             = {https://www.worldscientific.com/doi/10.1142/9789814503464_0076}
}

@inproceedings{Abts+2022,
author = {Abts, Dennis and Kimmell, Garrin and Ling, Andrew and Kim, John and Boyd, Matt and Bitar, Andrew and Parmar, Sahil and Ahmed, Ibrahim and DiCecco, Roberto and Han, David and Thompson, John and Bye, Michael and Hwang, Jennifer and Fowers, Jeremy and Lillian, Peter and Murthy, Ashwin and Mehtabuddin, Elyas and Tekur, Chetan and Sohmers, Thomas and Kang, Kris and Maresh, Stephen and Ross, Jonathan},
title = {A software-defined tensor streaming multiprocessor for large-scale machine learning},
year = {2022},
isbn = {9781450386104},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3470496.3527405},
doi = {10.1145/3470496.3527405},
abstract = {We describe our novel commercial software-defined approach for large-scale interconnection networks of tensor streaming processing (TSP) elements. The system architecture includes packaging, routing, and flow control of the interconnection network of TSPs. We describe the communication and synchronization primitives of a bandwidth-rich substrate for global communication. This scalable communication fabric provides the backbone for large-scale systems based on a software-defined Dragonfly topology, ultimately yielding a parallel machine learning system with elasticity to support a variety of workloads, both training and inference. We extend the TSP's producer-consumer stream programming model to include global memory which is implemented as logically shared, but physically distributed SRAM on-chip memory. Each TSP contributes 220 MiBytes to the global memory capacity, with the maximum capacity limited only by the network's scale --- the maximum number of endpoints in the system. The TSP acts as both a processing element (endpoint) and network switch for moving tensors across the communication links. We describe a novel software-controlled networking approach that avoids the latency variation introduced by dynamic contention for network links. We describe the topology, routing and flow control to characterize the performance of the network that serves as the fabric for a large-scale parallel machine learning system with up to 10,440 TSPs and more than 2 TeraBytes of global memory accessible in less than 3 microseconds of end-to-end system latency.},
booktitle = {Proceedings of the 49th Annual International Symposium on Computer Architecture},
pages = {567–580},
numpages = {14},
keywords = {dragonfly, machine learning, software scheduling, tensor streaming processor},
location = {New York, New York},
series = {ISCA '22}
}

@book{戸田+2011,
    author         = {戸田盛和 and 斎藤信彦 and 久保亮五 and 橋爪夏樹},
    year           = {2011},
    title          = {統計物理学},
    series         = {現代物理学の基礎},
    volume         = {5},
    edition        = {新装版},
    url            = {https://www.iwanami.co.jp/book/b259545.html},
    publisher      = {岩波書店}
}

@book{Gibbs1902,
    author         = {Josiah Willard Gibbs},
    year           = {1902},
    title          = {Elementary Principles in Statistical Mechanics: Developed with Especial Reference to the Rational Foundation of Thermodynamics},
    series         = {},
    volume         = {},
    edition        = {},
    url            = {},
    publisher      = {Charles Scribner's Sons}
}

@article{Weiss1907,
    author          = {Pierre Weiss},
    year            = {1907},
    title           = {L'hypothèse du champ moléculaire et la propriété ferromagnétique},
    journal         = {Journal of Physics: Theories and Applications},
    volume          = {6},
    number          = {1},
    pages           = {661-690},
    url             = {https://hal.science/jpa-00241247/en}
}

@inbook{Villani-Mouhot2015,
    author         = {Cédric Villani and Clément Mouhot},
    chapter        = {Kinetic Theory},
    editor         = {Nicholas J. Higham},
    pages          = {},
    publisher      = {Princeton University Press},
    title          = {The Princeton Companion to Applied Mathematics},
    year           = {2015},
    url            = {https://press.princeton.edu/books/hardcover/9780691150390/the-princeton-companion-to-applied-mathematics},
}

@inbook{Villani2002,
    author         = {Cédric Villani},
    chapter        = {A Review of Mathemaical Topics in Collisional Kinetic Theory},
    editor         = {S. Friedlander and D. Serre},
    pages          = {71-305},
    publisher      = {North-Holland, Amsterdam},
    title          = {Handbook of Mathematical Fluid Dynamics},
    volume         = {I},
    year           = {2002},
    url            = {https://www.sciencedirect.com/science/article/abs/pii/S1874579202800040?via%3Dihub},
}

@article{Zadeh1989,
    author          = {Lotfi A. Zadeh},
    year            = {1989},
    title           = {Knowledge representation in fuzzy logic},
    journal         = {IEEE Transactions on Knowledge and Data Engineering},
    volume          = {1},
    number          = {1},
    pages           = {89-100},
    url             = {https://ieeexplore.ieee.org/document/43406}
}

@book{Li2009,
    author         = {Stan Z. Li},
    year           = {2009},
    title          = {Markov Random Field Modeling in Image Analysis},
    series         = {Advances in Computer Vision and Pattern Recognition},
    volume         = {},
    edition        = {3},
    url            = {https://link.springer.com/book/10.1007/978-1-84800-279-1},
    publisher      = {Springer London}
}

@article{Besag1974,
    author          = {Julian Besag},
    year            = {1974},
    title           = {Spatial Interaction and the Statistical Analysis of Lattice Systems},
    journal         = {Journal of the Royal Statistical Society. Series B (Methodological)},
    volume          = {36},
    number          = {2},
    pages           = {192-236},
    url             = {https://www.jstor.org/stable/2984812}
}

@unpublished{Grenander1983,
    author = {U. Grenander},
    year   = {1983},
    title  = {Tutorial in Pattern Theory},
    url    = {},
    publisher = {Brown University}
}
@article{Grenander-Miller1994,
 ISSN = {00359246},
 URL = {http://www.jstor.org/stable/2346184},
 abstract = {Modern sensor technologies, especially in biomedicine, produce increasingly detailed and informative image ensembles, many extremely complex. It will be argued that pattern theory can supply mathematical representations of subject-matter knowledge that can be used as a basis for algorithmic `understanding' of such pictures. After a brief survey of the basic principles of pattern theory we shall illustrate them by an application to a concrete situation: high magnification (greater than 15 000 ×) electron micrographs of cardiac muscle cells. The aim is to build algorithms for automatic hypothesis formation concerning the number, location, orientation and shape of mitochondria and membranes. For this we construct a pattern theoretic model in the form of a prior probability measure on the space of configurations describing these hypotheses. This measure is synthesized by solving sequentially a jump-diffusion equation of generalized Langevin form. The jumps occur for the creation-annihilation of hypotheses, corresponding to a jump from one continuum to another in configuration (hypothesis) space. These continua (subhypotheses) are expressed in terms of products of low dimensional Lie groups acting on the generators of a template. We use a modified Bayes approach to obtain the hypothesis formation, also organized by solving a generalized Langevin equation. To justify this it is shown that the resulting jump-diffusion process is ergodic so that the solution converges to the desired probability measure. To speed up the convergence we reduce the computation of the drift term in the stochastic differential equation analytically to a curvilinear integral, with the random term computed almost instantaneously. The algorithms thus obtained are implemented, both for mitochondria and membranes, on a 4000 processor parallel machine. Photographs of the graphics illustrate how automatic hypothesis formation is achieved. This approach is applied to deformable neuroanatomical atlases and tracking recognition from narrow band and high resolution sensor arrays.},
 author = {Ulf Grenander and Michael I. Miller},
 journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
 number = {4},
 pages = {549--603},
 publisher = {[Royal Statistical Society, Wiley]},
 title = {Representations of Knowledge in Complex Systems},
 urldate = {2024-06-04},
 volume = {56},
 year = {1994}
}


@article{Besag-Green1993,
	abstract = {SUMMARY Markov chain Monte Carlo (MCMC) algorithms, such as the Gibbs sampler, have provided a Bayesian inference machine in image analysis and in other areas of spatial statistics for several years, founded on the pioneering ideas of Ulf Grenander. More recently, the observation that hyperparameters can be included as part of the updating schedule and the fact that almost any multivariate distribution is equivalently a Markov random field has opened the way to the use of MCMC in general Bayesian computation. In this paper, we trace the early development of MCMC in Bayesian inference, review some recent computational progress in statistical physics, based on the introduction of auxiliary variables, and discuss its current and future relevance in Bayesian applications. We briefly describe a simple MCMC implementation for the Bayesian analysis of agricultural field experiments, with which we have some practical experience.},
	author = {Besag, Julian and Green, Peter J.},
	doi = {https://doi.org/10.1111/j.2517-6161.1993.tb01467.x},
	eprint = {https://rss.onlinelibrary.wiley.com/doi/pdf/10.1111/j.2517-6161.1993.tb01467.x},
	journal = {Journal of the Royal Statistical Society: Series B (Methodological)},
	keywords = {agricultural field experiments, antithetic variables, auxiliary variables, gibbs sampler, markov chain monte carlo, markov random fields, metropolis method, multigrid, multimodality, swendsen-wang method},
	number = {1},
	pages = {25-37},
	title = {Spatial Statistics and Bayesian Computation},
	url = {https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/j.2517-6161.1993.tb01467.x},
	volume = {55},
	year = {1993},
	bdsk-url-1 = {https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/j.2517-6161.1993.tb01467.x},
	bdsk-url-2 = {https://doi.org/10.1111/j.2517-6161.1993.tb01467.x}}


@article{Boltzmann1872,
    author          = {L. Boltzmann},
    year            = {1872},
    title           = {Weitere Studien über das Wärme gleichgenicht unfer Gasmoläkuler},
    journal         = {Sitzungsberichte der Akademie der Wissenschaften},
    volume          = {66},
    number          = {},
    pages           = {275-370},
    url             = {}
}

@article{Maxwell1867,
    author          = {J. Clerk Maxwell},
    year            = {1867},
    title           = {On the Dynamical Theory of Gases},
    journal         = {Philosophical Transactions of the Royal Society of London},
    volume          = {157},
    number          = {},
    pages           = {49-88},
    url             = {https://www.jstor.org/stable/108968}
}

@article{Maxwell1878,
    author          = {J. Clerk Maxwell},
    year            = {1878},
    title           = {On Stresses in Rarefied Gases Arising from Inequalities of Temperature},
    journal         = {Proceedings of the Royal Society of London},
    volume          = {27},
    number          = {},
    pages           = {304-308},
    url             = {https://www.jstor.org/stable/113680}
}

@article{Gidas1989,
    author          = {B. Gidas},
    year            = {1989},
    title           = {A Renormalization Group Approach to Image Processing Problems},
    journal         = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
    volume          = {11},
    number          = {2},
    pages           = {164-180},
    url             = {https://ieeexplore.ieee.org/document/16712}
}

@phdthesis{Li1991,
    author      = {S. Ziqing Li},
    school      = {University of Surrey, Guildford Surrey, UK},
    title       = {Towards 3D Vision from Range Images: An Optimisation Framework and Parallel Distributed Networks},
    year        = {1991},
    url         = {https://www.sciencedirect.com/science/article/pii/104996609290023V},
}

@article{Faraday1833,
    author          = {Michael Faraday},
    year            = {1833},
    title           = {Experimental Researches in Electricity. Third Series},
    journal         = {Philosophical Transactions of the Royal Society of London},
    volume          = {123},
    number          = {},
    pages           = {23-54},
    url             = {https://www.jstor.org/stable/107985}
}

@article{Braun1874,
    author          = {Ferdinand Braun},
    year            = {1874},
    title           = {Über die Stromleitung durch Schwefelmetalic},
    journal         = {Annalen der Physik and Chemie},
    volume          = {153},
    number          = {4},
    pages           = {556-563},
    url             = {}
}

@article{Griffiths1967,
    author          = {Robert B. Griffiths},
    year            = {1967},
    title           = {Thermodynamic Functions for Fluids and Ferromagnets near the Critical Point},
    journal         = {Physical Review},
    volume          = {158},
    number          = {1},
    pages           = {176-187},
    url             = {https://journals.aps.org/pr/abstract/10.1103/PhysRev.158.176}
}

@article{Lenz1920,
    author          = {W. Lenz},
    year            = {1920},
    title           = {Beiträg zum Verständnis der magnetischen Eigenschaften in festen Körpern},
    journal         = {Physikalische Zeitschrift},
    volume          = {21},
    number          = {},
    pages           = {613-615},
    url             = {}
}

@article{Peieris1936,
    author          = {R. E. Peieris},
    year            = {1936},
    title           = {On Ising's Model of Ferromagnetism},
    journal         = {Mathematical Proceedings of the Cambridge Philosophical Society},
    volume          = {32},
    number          = {3},
    pages           = {477-481},
    url             = {https://www.cambridge.org/core/journals/mathematical-proceedings-of-the-cambridge-philosophical-society/article/abs/on-isings-model-of-ferromagnetism/C0584C5711BC3D25830B63A4C2F09609}
}

@article{Ising1925,
    author          = {Ernst Ising},
    year            = {1925},
    title           = {Beitrag zur Theorie des Ferromagnetismus},
    journal         = {Zeitschrift für Physik},
    volume          = {31},
    number          = {},
    pages           = {253-258},
    url             = {https://link.springer.com/article/10.1007/BF02980577}
}

@article{Fisher1966,
    author          = {Michael E. Fisher},
    year            = {1966},
    title           = {Quantum Corrections to Critical-Point Behavior},
    journal         = {Physical Review Letters},
    volume          = {16},
    number          = {1},
    pages           = {11-14},
    url             = {https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.16.11}
}

@book{Madelung1978,
    author         = {Otfried Madelung},
    year           = {1978},
    title          = {Introduction to Solid-State Theory},
    series         = {Springer Series in Solid-State Sciences},
    volume         = {2},
    edition        = {},
    url            = {https://link.springer.com/book/10.1007/978-3-642-61885-7},
    publisher      = {Springer Berlin, Heidelberg}
}

@article{Heisenberg1931,
    author          = {W. Heisenberg},
    year            = {1931},
    title           = {Zum Paulischen Ausschließungsprinzip},
    journal         = {Annalen der Physik},
    volume          = {402},
    number          = {7},
    pages           = {888-904},
    url             = {https://onlinelibrary.wiley.com/doi/10.1002/andp.19314020710}
}

@article{Criens+2023,
    author          = {David Criens and Peter Pfaffelhuber and Thorsten Schmidt},
    year            = {2023},
    title           = {The Martingale Problem Method Revisited},
    journal         = {Electronic Journal of Probability},
    volume          = {28},
    number          = {},
    pages           = {1-46},
    url             = {https://projecteuclid.org/journals/electronic-journal-of-probability/volume-28/issue-none/The-martingale-problem-method-revisited/10.1214/23-EJP902.full}
}

@unpublished{Hoh1998,
    author         = {Walter Hoh},
    year           = {1998},
    title          = {Pseudo-Differential operators generating Markov processes},
    note           = {Habilitationsschrift Universität Bielefeld},
    url    = {http://www.mathematik.uni-bielefeld.de/%7Ehoh/pdo_mp.ps}
}

@article{Valiant1984,
    author          = {L. G. Valiant},
    year            = {1984},
    title           = {A Theory of the Learnable},
    journal         = {Communications of the ACM},
    volume          = {27},
    number          = {11},
    pages           = {1134-1142},
    url             = {https://dl.acm.org/doi/10.1145/1968.1972}
}

@article{Seeger2002,
    author          = {Matthias Seeger},
    year            = {2002},
    title           = {PAC-Bayesian Generalisation Error Bounds for Gaussian Process Classification},
    journal         = {Journal of Machine Learning Research},
    volume          = {3},
    number          = {},
    pages           = {233-269},
    url             = {https://www.jmlr.org/papers/v3/seeger02a.html}
}

@inbook{Haussler-Warmuth1993,
    author         = {David Haussler and Manfred Warmuth},
    chapter        = {The Probably Approximately Correct (PAC) and Other Learning Models},
    editor         = {Alan L. Meyrowitz and Susan Chipman},
    pages          = {291-312},
    publisher      = {Springer New York},
    title          = {Foundations of Knowledge Acquisition: Machine Learning},
    year           = {1993},
    url            = {https://link.springer.com/book/10.1007/b102257},
}

@inproceedings{Shawe-Taylor-Williamson1997,
    author          = {John Shawe-Taylor and Robert C. Williamson},
    booktitle       = {Proceedings of the Tenth Annual Conference on Computational Learning Theory},
    editor          = {},
    title           = {A PAC Analysis of a Bayesian Estimator},
    year            = {1997},
    pages           = {2-9},
    url             = {https://dl.acm.org/doi/10.1145/267460.267466},
}

@article{McAllester1999,
    author          = {David A. McAllester},
    year            = {1999},
    title           = {Some PAC-Bayesian Theorems},
    journal         = {Machine Learning},
    volume          = {37},
    number          = {},
    pages           = {355-363},
    url             = {https://link.springer.com/article/10.1023/A:1007618624809}
}

@book{Devroye+1996,
    author         = {Luc Devroye and László Györfi and Gábor Lugosi},
    year           = {1996},
    title          = {A Probabilistic Theory of Pattern Recognition},
    series         = {Stochastic Modelling and Applied Probability},
    volume         = {31},
    edition        = {},
    url            = {https://link.springer.com/book/10.1007/978-1-4612-0711-5},
    publisher      = {Springer New York}
}

@book{Vapnik1998,
    author         = {Vladimir N. Vapnik},
    year           = {1998},
    title          = {Statistical Learning Theory},
    series         = {},
    volume         = {},
    edition        = {},
    url            = {https://www.wiley.com/en-us/Statistical+Learning+Theory-p-9780471030034},
    publisher      = {Wiley-Blackwell}
}

@book{Catoni2007,
    author         = {Olivier Catoni},
    year           = {2007},
    title          = {PAC-Bayesian Supervised Classification: The Thermodynamics of Statistical Learning},
    series         = {IMS Lecture Notes - Monograph Series},
    volume         = {56},
    edition        = {},
    url            = {https://projecteuclid.org/ebooks/institute-of-mathematical-statistics-lecture-notes-monograph-series/Pac-Bayesian-Supervised-Classification/toc/10.1214/074921707000000391},
    publisher      = {Institute of Mathematical Statistics}
}

@article{Blei+2017,
    author          = {David M. Blei and Alp Kucukelbir and Jon D. McAuliffe},
    year            = {2017},
    title           = {Variational Inference: A Review for Statisticians},
    journal         = {Journal of the American Statistical Association},
    volume          = {112},
    number          = {518},
    pages           = {859-877},
    url             = {https://www.tandfonline.com/doi/full/10.1080/01621459.2017.1285773}
}

@article{Scheffe1947,
    author          = {H Scheffé},
    year            = {1947},
    title           = {A Useful Convergence Theorem for Probability Discributions},
    journal         = {The Annals of Mathematical Statistics},
    volume          = {18},
    number          = {3},
    pages           = {434-438},
    url             = {https://www.jstor.org/stable/2235739}
}

@book{Bishop2006,
    author         = {Christopher M. Bishop},
    year           = {2006},
    title          = {Pattern Recognition and Machine Learning},
    series         = {Information Science and Statistics},
    volume         = {},
    edition        = {},
    url            = {https://link.springer.com/book/9780387310732},
    publisher      = {Springer New York}
}

@book{須山敦志2019,
    author         = {須山敦志},
    year           = {2019},
    month          = {8},
    title          = {ベイズ深層学習},
    series         = {機械学習プロフェッショナルシリーズ},
    volume         = {},
    edition        = {},
    url            = {https://www.kspub.co.jp/book/detail/5168707.html},
    publisher      = {講談社サイエンティフィク}
}

@article{Hinton-Salakhutdinov2006,
    author          = {Geoffrey E. Hinton and R. R. Salakhutdinov},
    journal         = {Science},
    number          = {5786},
    title           = {Reducing the Dimensionality of Data with Neural Networks},
    volume          = {313},
    year            = {2006},
    pages           = {504-507},
    url             = {https://www.science.org/doi/10.1126/science.1127647},
}

@article{大王製紙CB発行事件,
    author          = {東京地判},
    year            = {2018},
    note            = {平成30年9月20日},
    title           = {判決〔控訴〕},
    journal         = {資料版商事法務},
    volume          = {415},
    number          = {},
    pages           = {83},
    url             = {https://www.shojihomu.co.jp/publishing/subscription_detail?id=509&category=2&sub_category=8&publish_id=509&cd=850415}
}

@article{川島いづみ2021,
    author          = {川島いづみ},
    year            = {2021},
    title           = {新株予約権付社債の有利発行・不公正発行該当性},
    journal         = {TKC ローライブラリー 新・判例解説 Watch 商法},
    volume          = {146},
    number          = {},
    pages           = {},
    url             = {http://lex.lawlibrary.jp/commentary/pdf/z18817009-00-051462025_tkc.pdf}
}

@article{潘阿憲2020,
    author          = {潘阿憲},
    year            = {2020},
    title           = {新株予約権付社債の不公正発行と取締役の責任},
    journal         = {法学教室},
    volume          = {},
    number          = {473},
    pages           = {129},
    url             = {https://www.yuhikaku.co.jp/hougaku/detail/020394}
}

@book{Pazy1983,
    author         = {A. Pazy},
    year           = {1983},
    title          = {Semigroups of Linear Operators and Applications to Partial Differential Equations},
    series         = {Applied Mathematical Sciences},
    volume         = {44},
    edition        = {},
    url            = {https://link.springer.com/book/10.1007/978-1-4612-5561-1},
    publisher      = {Springer New York}
}

@article{Davis1984,
    author          = {M. H. A. Davis},
    year            = {1984},
    title           = {Piecewise-Deterministic Markov Processes: A General Class of Non-Diffusion Stochastic Models},
    journal         = {Journal of the Royal Statistical Society. Series B (Methodological)},
    volume          = {46},
    number          = {3},
    pages           = {353-388},
    url             = {https://www.jstor.org/stable/2345677}
}

@article{Goldman-Singh2021,
    author          = {Jacob Vorstrup Goldman and Sumeetpal S. Singh},
    year            = {2021},
    title           = {Spatiotemporal Blocking of the Bouncy Particle Sampler for Efficient Inference in State-Space Models},
    journal         = {Statistics and Computing},
    volume          = {31},
    number          = {68},
    pages           = {67-81},
    url             = {https://link.springer.com/article/10.1007/s11222-021-10034-6}
}

@article{Bierkens+2020,
    author          = {Joris Bierkens and Sebastiano Grazzi and Kengo Kamatani and Gareth O. Roberts},
    year            = {2020},
    title           = {The Boomerang Sampler},
    journal         = {Proceedings of the 37th International Conference on Machine Learning},
    volume          = {119},
    number          = {},
    pages           = {908-918},
    url             = {https://proceedings.mlr.press/v119/bierkens20a.html}
}

@book{Sutton-Barto2018,
    author         = {Richard S. Sutton and Andrew G. Barto},
    year           = {2018},
    title          = {Reinforcement Learning: An Introduction},
    series         = {Adaptive Computation and Machine Learning Series},
    volume         = {},
    edition        = {2},
    url            = {https://mitpress.mit.edu/9780262352703/reinforcement-learning/},
    publisher      = {MIT Press}
}

@book{Powell2011,
    author         = {Warren B. Powell},
    year           = {2011},
    title          = {Approximate Dynamic Programming: Solving the Curses of Dimensionality},
    series         = {Wiley Series in Probability and Statistics},
    volume         = {},
    edition        = {2},
    url            = {https://onlinelibrary.wiley.com/doi/book/10.1002/9781118029176},
    publisher      = {John Wiley \& Sons}
}

@inproceedings{Singh-Bertsekas1996,
    author          = {Satinder Singh and Dimitri Bertsekas},
    year            = {1996},
    title           = {Reinforcement Learning for Dynamic Channel Allocation in Cellular Telephone Systems},
    booktitle       = {Advances in Neural Information Processing Systems 9},
    pages           = {},
    url             = {https://proceedings.neurips.cc/paper/1996/hash/3948ead63a9f2944218de038d8934305-Abstract.html}
}

@article{Silver+2018,
    author          = {David Silver and Thomas Hubert and Julian Schrittwieser and Ioannis Antonoglou and Matthew Lai and Arthur Guez and Marc Lanctot and Laurent Sifre and Dharshan Kumaran and Thore Graepel and Timothy Lillicrap and Karen Simonyan and Demis Hassabis},
    year            = {2018},
    title           = {A General Reinforcement Learning Algorithm that Masters Chess, Shogi, and Go through Self-Play},
    journal         = {Science},
    volume          = {362},
    number          = {6419},
    pages           = {1140-1144},
    url             = {https://www.science.org/doi/10.1126/science.aar6404}
}

@inproceedings{VanRoy+1997,
    author          = {B. Van{\ }Roy and D. P. Bertsekas and Y. Lee and J. N. Tsitsiklis},
    year            = {1997},
    title           = {A Neuro-dynamic Programming Approach to Retailer Inventory Management},
    booktitle       = {Proceedings of the 36th IEEE Conference on Decision and Control},
    pages           = {},
    url             = {https://ieeexplore.ieee.org/abstract/document/652501}
}

@article{Crites-Barto1998,
    author          = {Robert H. Crites and Andrew G. Barto},
    year            = {1998},
    title           = {Elevator Group Control Using Multiple Reinforcement Learning Agents},
    journal         = {Machine Learning},
    volume          = {33},
    number          = {},
    pages           = {235-262},
    url             = {https://link.springer.com/article/10.1023/A:1007518724497}
}

@article{Rolnick+2022,
    author = {Rolnick, David and Donti, Priya L. and Kaack, Lynn H. and Kochanski, Kelly and Lacoste, Alexandre and Sankaran, Kris and Ross, Andrew Slavin and Milojevic-Dupont, Nikola and Jaques, Natasha and Waldman-Brown, Anna and Luccioni, Alexandra Sasha and Maharaj, Tegan and Sherwin, Evan D. and Mukkavilli, S. Karthik and Kording, Konrad P. and Gomes, Carla P. and Ng, Andrew Y. and Hassabis, Demis and Platt, John C. and Creutzig, Felix and Chayes, Jennifer and Bengio, Yoshua},
    title = {Tackling Climate Change with Machine Learning},
    year = {2022},
    issue_date = {February 2023},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    volume = {55},
    number = {2},
    issn = {0360-0300},
    url = {https://doi.org/10.1145/3485128},
    doi = {10.1145/3485128},
    abstract = {Climate change is one of the greatest challenges facing humanity, and we, as machine learning (ML) experts, may wonder how we can help. Here we describe how ML can be a powerful tool in reducing greenhouse gas emissions and helping society adapt to a changing climate. From smart grids to disaster management, we identify high impact problems where existing gaps can be filled by ML, in collaboration with other fields. Our recommendations encompass exciting research questions as well as promising business opportunities. We call on the ML community to join the global effort against climate change.},
    journal = {ACM Computing Survey},
    month = {feb},
    articleno = {42},
    numpages = {96},
    keywords = {artificial intelligence, machine learning, adaptation, mitigation, Climate change}
}



@phdthesis{Watkins1989,
    author      = {Christopher J. C. H. Watkins},
    school      = {University of London},
    title       = {Learning from Delayed Rewards},
    year        = {1989},
    url         = {https://www.cs.rhul.ac.uk/~chrisw/thesis.html},
}

@article{Watkins-Dayan1992,
    author          = {Christopher J. C. H. Watkins and Peter Dayan},
    year            = {1992},
    title           = {Q-Leraning},
    journal         = {Machine Learning},
    volume          = {8},
    number          = {},
    pages           = {279-292},
    url             = {https://link.springer.com/article/10.1007/BF00992698}
}

@article{Sutton1988,
    author          = {Richard S. Sutton},
    year            = {1988},
    title           = {Learning to Predict by the Methods of Temporal Differences},
    journal         = {Machine Learning},
    volume          = {3},
    number          = {},
    pages           = {9-44},
    url             = {https://link.springer.com/article/10.1007/BF00115009}
}

@article{Sun+2016,
    author          = {Ying Sun and Prabhu Babu and Daniel P. Palomar},
    year            = {2016},
    title           = {Majorization-Minimization Algorithms in Signal Processing, Communications, and Machine Learning},
    journal         = {IEEE Transactions on Signal Processing},
    volume          = {65},
    number          = {3},
    pages           = {794-816},
    url             = {https://ieeexplore.ieee.org/document/7547360}
}

@article{Fisher1912,
    author          = {R. A. Fisher},
    year            = {1912},
    title           = {On an Absolute Criterion for Fitting Frequency Curves},
    journal         = {Messenger of Mathematics},
    volume          = {41},
    number          = {},
    pages           = {155-160},
    url             = {https://www.jstor.org/stable/2246266}
}

@article{Wu-Lange2010,
    author          = {Tong Tong Wu and Kenneth Lange},
    year            = {2010},
    title           = {The MM Alternative to EM},
    journal         = {Statistical Science},
    volume          = {25},
    number          = {4},
    pages           = {492-505},
    url             = {https://www.jstor.org/stable/23061097}
}

@book{Neal1996,
    author         = {Radford M. Neal},
    year           = {1996},
    title          = {Bayesian Learning for Neural Networks},
    series         = {Lecture Notes in Statistics},
    volume         = {118},
    edition        = {},
    url            = {https://link.springer.com/book/10.1007/978-1-4612-0745-0},
    publisher      = {Springer New York}
}


@InProceedings{Cobb-Jalaian2021,
  title = 	 {Scaling Hamiltonian Monte Carlo inference for Bayesian neural networks with symmetric splitting},
  author =       {Cobb, Adam D. and Jalaian, Brian},
  booktitle = 	 {Proceedings of the Thirty-Seventh Conference on Uncertainty in Artificial Intelligence},
  pages = 	 {675--685},
  year = 	 {2021},
  editor = 	 {de Campos, Cassio and Maathuis, Marloes H.},
  volume = 	 {161},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {27--30 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v161/cobb21a/cobb21a.pdf},
  url = 	 {https://proceedings.mlr.press/v161/cobb21a.html},
  abstract = 	 {Hamiltonian Monte Carlo (HMC) is a Markov chain Monte Carlo (MCMC) approach that exhibits favourable exploration properties in high-dimensional models such as neural networks. Unfortunately, HMC has limited use in large-data regimes and little work has explored suitable approaches that aim to preserve the entire Hamiltonian. In our work, we introduce a new symmetric integration scheme for split HMC that does not rely on stochastic gradients. We show that our new formulation is more efficient than previous approaches and is easy to implement with a single GPU. As a result, we are able to perform full HMC over common deep learning architectures using entire data sets. In addition, when we compare with stochastic gradient MCMC, we show that our method achieves better performance in both accuracy and uncertainty quantification. Our approach demonstrates HMC as a feasible option when considering inference schemes for large-scale machine learning problems.}
}

@inproceedings{Smith-Le2018,
title={A Bayesian Perspective on Generalization and Stochastic Gradient Descent},
author={Samuel L. Smith and Quoc V. Le},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=BJij4yg0Z},
}

@ARTICLE{Imaizumi-SchmidtHieber2023,
  author={Imaizumi, Masaaki and Schmidt-Hieber, Johannes},
  journal={IEEE Transactions on Information Theory}, 
  title={On Generalization Bounds for Deep Networks Based on Loss Surface Implicit Regularization}, 
  year={2023},
  volume={69},
  number={2},
  pages={1203-1223},
  keywords={Neural networks;Deep learning;Statistics;Sociology;Convergence;Complexity theory;Training data;Deep neural network;generalization error;uniform convergence;non-convex optimization},
  doi={10.1109/TIT.2022.3215088}}

@inproceedings{Tolstikhin+2018,
title={Wasserstein Auto-Encoders},
author={Ilya Tolstikhin and Olivier Bousquet and Sylvain Gelly and Bernhard Schoelkopf},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=HkL7n1-0b},
}


@inproceedings{Sabour+2017,
	author = {Sabour, Sara and Frosst, Nicholas and Hinton, Geoffrey E},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
	publisher = {Curran Associates, Inc.},
	title = {Dynamic Routing Between Capsules},
	url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/2cad8fa47bbef282badbb8de5374b894-Paper.pdf},
	volume = {30},
	year = {2017},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2017/file/2cad8fa47bbef282badbb8de5374b894-Paper.pdf}}

@inbook{Scholkopf2022,
   title={Causality for Machine Learning},
   ISBN={9781450395861},
   url={http://dx.doi.org/10.1145/3501714.3501755},
   DOI={10.1145/3501714.3501755},
   booktitle={Probabilistic and Causal Inference},
   publisher={ACM},
   author={Schölkopf, Bernhard},
   year={2022},
   month=feb, pages={765–804},
   url            = {https://arxiv.org/abs/1911.10500},
}


@book{Theodoridis2020,
    author         = {Sergios Theodoridis},
    year           = {2020},
    title          = {Machine Learning: A Bayesian and Optimization Perspective},
    series         = {},
    volume         = {},
    edition        = {2},
    url            = {https://doi.org/10.1016/C2019-0-03772-7},
    publisher      = {Academic Press}
}

@book{Shalev-Shwartz-Ben-David2014,
    author         = {Shai Shalev-Shwartz and Shai Ben-David},
    year           = {2014},
    title          = {Understanding Machine Learning: From Theory to Algorithms},
    series         = {},
    volume         = {},
    edition        = {},
    url            = {https://doi.org/10.1017/CBO9781107298019},
    publisher      = {Cambridge University Press}
}

@techreport{MacKay1994,
    author      = {D. J. C. MacKay},
    institution = {American Society of Heating, Refrigerating, and Air Conditioning Engineers (ASHRAE)},
    title       = {Bayesian nonlinear modeling for the prediction competition},
    volume          = {100},
    number          = {2},
    year        = {1994},
    url         = {https://www.osti.gov/biblio/33309},
}

@book{Rasmussen-Williams2006,
    author         = {Carl Edward Rasmussen and Christopher K. I. Williams},
    year           = {2006},
    title          = {Gaussian Processes for Machine Learning},
    series         = {},
    volume         = {},
    edition        = {},
    url            = {https://direct.mit.edu/books/book/2320/Gaussian-Processes-for-Machine-Learning},
    publisher      = {The MIT Press}
}

@article{Rumelhart+1986,
    author          = {David E. Rumelhart and Geoffrey E. Hinton and Ronald J. Williams},
    year            = {1986},
    title           = {Learning Representations by Back-Propagating Errors},
    journal         = {Nature},
    volume          = {323},
    number          = {},
    pages           = {533-536},
    url             = {https://www.nature.com/articles/323533a0}
}

@inproceedings{Williams1996,
    author          = {Christopher K. I. Williams},
    year            = {1996},
    title           = {Computing with Infinite Networks},
    booktitle       = {Advances in Neural Information Processing Systems 9},
    pages           = {295-301},
    url             = {https://papers.nips.cc/paper_files/paper/1996/hash/ae5e3ce40e0404a45ecacaaf05e5f735-Abstract.html}
}

@inproceedings{Lee+2018,
title={Deep Neural Networks as Gaussian Processes},
author={Jaehoon Lee and Jascha Sohl-dickstein and Jeffrey Pennington and Roman Novak and Sam Schoenholz and Yasaman Bahri},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=B1EA-M-0Z},
}

@book{Bishop-Bishop2024,
    author         = {Christopher M. Bishop and Hugo Bishop},
    year           = {2024},
    title          = {Deep Learning: Foundations and Concepts},
    url            = {https://link.springer.com/book/10.1007/978-3-031-45468-4},
    publisher      = {Springer Cham}
}

@article{LeCun1998,
    author          = {Yan LeCun and L. Bottou and Y. Bengio and P. Haffner},
    year            = {1998},
    title           = {Gradient-Based Learning Applied to Document Recognition},
    journal         = {Proceedings of the IEEE},
    volume          = {86},
    number          = {11},
    pages           = {2278-2324},
    url             = {https://ieeexplore.ieee.org/document/726791}
}

@article{Schmidhuber2015,
    author          = {Jürgen Schmidhuber},
    year            = {2015},
    title           = {Deep Learning in Neural Networks: An Overview},
    journal         = {Neural Networks},
    volume          = {61},
    number          = {},
    pages           = {85-117},
    url             = {https://www.sciencedirect.com/science/article/pii/S0893608014002135}
}

@inproceedings{Goodfellow+2014,
    author          = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
    year            = {2014},
    title           = {Generative Adversarial Nets},
    booktitle       = {Advances in Neural Information Processing Systems},
    volume          = {27},
    pages           = {1-9},
    url             = {https://papers.nips.cc/paper/2014/hash/5ca3e9b122f61f8f06494c97b1afccf3-Abstract.html}
}

@inproceedings{Krizhevsky+2012,
    author          = {Alex Krizhevsky and Ilya Sutskever and Geoffrey E. Hinton},
    year            = {2012},
    title           = {ImageNet Classification with Deep Convolutional Neural Networks},
    booktitle       = {Advances in Neural Information Processing Systems},
    volume          = {25},
    pages           = {},
    url             = {https://papers.nips.cc/paper_files/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html}
}

@unpublished{Hinton+2012,
    title={Improving neural networks by preventing co-adaptation of feature detectors}, 
    author={Geoffrey E. Hinton and Nitish Srivastava and Alex Krizhevsky and Ilya Sutskever and Ruslan R. Salakhutdinov},
    year   = {2012},
    url    = {https://arxiv.org/abs/1207.0580}
}

@article{Srivastava+2014,
    author          = {Nitish Srivastava and Geoffrey E. Hinton and Alex Krizhevsky and Ilya Sutskever and Ruslan Salakhutdinov},
    year            = {2014},
    title           = {Dropout: A Simple Way to Prevent Neural Networks from Overfitting},
    journal         = {The Journal of Machine Learning Research},
    volume          = {15},
    number          = {56},
    pages           = {1929-1958},
    url             = {https://jmlr.org/papers/v15/srivastava14a.html}
}

@inproceedings{Salakhutdinov-Hinton2009,
    author          = {Ruslan Salakhutdinov and Geoffrey Hinton},
    year            = {2009},
    title           = {Deep Boltzmann Machines},
    booktitle       = {Proceedings of the Twelth International Conference on Artificial Intelligence and Statistics},
    volume          = {5},
    pages           = {448-455},
    url             = {https://proceedings.mlr.press/v5/salakhutdinov09a.html}
}

@inproceedings{Bengio+2014,
    author          = {Yoshua Bengio and Eric Laufer and Guillaume Alain and Jason Yosinski},
    year            = {2014},
    title           = {Deep Generative Stochastic Networks Trainable by Backprop},
    booktitle       = {Proceedings of the 31st International Conference on Machine Learning},
    volume          = {32},
    number          = {2},
    pages           = {226-234},
    url             = {https://proceedings.mlr.press/v32/bengio14.html}
}

@article{Endres-Schindelin2003,
    author          = {D. M. Endres and J. E. Schindelin},
    year            = {2003},
    title           = {A New Metric for Probability Distributions},
    journal         = {IEEE Transactions on Information Theory},
    volume          = {49},
    number          = {7},
    pages           = {1858-1860},
    url             = {https://ieeexplore.ieee.org/document/1207388}
}

@article{Osan+2018,
    author          = {Tristán M. Osán and Diego G. Bussandri and Pedro W. Lamberti},
    year            = {2018},
    title           = {Monoparametric Family of Metrics Derived from Classical Jensen-Shannon Divergence},
    journal         = {Physica A: Statistical Mechanics and its Applications},
    volume          = {495},
    number          = {},
    pages           = {336-344},
    url             = {https://www.sciencedirect.com/science/article/pii/S0378437117313225}
}

@article{Rao1987,
    author          = {C. R. Rao},
    year            = {1987},
    title           = {Differential Metrics in Probability Spaces},
    journal         = {IMS Lecture Notes Monograph Series},
    volume          = {10},
    number          = {},
    pages           = {217-240},
    url             = {https://projecteuclid.org/ebooks/institute-of-mathematical-statistics-lecture-notes-monograph-series/Differential-geometry-in-statistical-inference/chapter/Chapter-5-Differential-Metrics-in-Probability-Spaces/10.1214/lnms/1215467062}
}

@article{Rao1982,
    author          = {C. R. Rao},
    year            = {1982},
    title           = {Diversity and Dissimilarity Coefficients: A Unified Approach},
    journal         = {Theoretical Population Biology},
    volume          = {21},
    number          = {1},
    pages           = {24-43},
    url             = {https://www.sciencedirect.com/science/article/abs/pii/0040580982900041}
}

@article{Lin1991,
    author          = {J. Lin},
    year            = {1991},
    title           = {Divergence Measures Based on the Shannon Entropy},
    journal         = {IEEE Transactions on Information Theory},
    volume          = {37},
    number          = {1},
    pages           = {145-151},
    url             = {https://ieeexplore.ieee.org/document/61115}
}


@article{Nielsen2021,
    author          = {Frank Nielsen},
    year            = {2021},
    title           = {On a Variational Definition for the Jensen-Shannon Symmetrization of Distances Based on the Information Radius},
    journal         = {Entropy},
    volume          = {23},
    number          = {4},
    pages           = {464},
    url             = {https://www.mdpi.com/1099-4300/23/4/464}
}

@article{Ali-Silvey1966,
    author          = {S. M. Ali and S. D. Silvey},
    year            = {1966},
    title           = {A General Class of Coefficients of Divergence of One Distribution from Another},
    journal         = {Journal of the Royal Statistical Society. Series B (Methodological)},
    volume          = {28},
    number          = {1},
    pages           = {131-142},
    url             = {https://www.jstor.org/stable/2984279}
}

@inproceedings{Renyi1961,
    author          = {Alfréd Rényi},
    year            = {1961},
    title           = {On Measures of Entropy and Information},
    booktitle       = {Proceedings of the Fourth Berkeley Symposium on Mathematical Statistics and Probability},
    volume          = {1},
    pages           = {547-561},
    url             = {https://projecteuclid.org/proceedings/berkeley-symposium-on-mathematical-statistics-and-probability/Proceedings-of-the-Fourth-Berkeley-Symposium-on-Mathematical-Statistics-and/Chapter/On-Measures-of-Entropy-and-Information/bsmsp/1200512181}
}

@article{Csiszar1963,
    author          = {Imere Csiszár},
    year            = {1963},
    title           = {Eine informationstheoretische Ungleichung und ihre Anwendung auf Beweis der Ergodizitaet von Markoffschen Ketten},
    journal         = {Magyár Tudomá Akadémia Mahematikai Kutató Intézetének Köezleményei},
    volume          = {6},
    number          = {},
    pages           = {85-108},
    url             = {https://www.fuw.edu.pl/~kostecki/scans/csiszar1963.pdf}
}

@article{Morimoto1963,
    author          = {Tetsuzo Morimoto},
    year            = {1963},
    title           = {Markov Processes and the $H$-Theorem},
    journal         = {Journal of the Physical Society of Japan},
    volume          = {18},
    number          = {3},
    pages           = {328-331},
    url             = {https://journals.jps.jp/doi/abs/10.1143/JPSJ.18.328?journalCode=jpsj}
}

@book{Robert2007,
    author         = {Christian P. Robert},
    year           = {2007},
    title          = {The Bayesian Choice: From Decision-Theoretic Foundations to Computational Implementation},
    series         = {Springer Texts in Statistics},
    volume         = {},
    edition        = {2},
    url            = {https://link.springer.com/book/10.1007/0-387-71599-1},
    publisher      = {Springer New York}
}

@article{Wu1983,
    author          = {C. F. Jeff Wu},
    year            = {1983},
    title           = {On the Convergence Properties of the EM Algorithm},
    journal         = {The Annals of Statistics},
    volume          = {11},
    number          = {1},
    pages           = {95-103},
    url             = {https://www.jstor.org/stable/2240463}
}

@article{Finch+1989,
    author          = {Stephen J. Finch and Nancy R. Mendell and Henry C. Thode{\ }Jr.},
    year            = {1989},
    title           = {Probabilistic Measures of Adequacy of a Numerical Search for a Global Maximum},
    journal         = {Journal of the American Statistical Association},
    volume          = {84},
    number          = {408},
    pages           = {1020-1023},
    url             = {https://www.jstor.org/stable/2290078}
}

@article{Boyles1983,
    author          = {Russell A. Boyles},
    year            = {1983},
    title           = {On the Convergence of the EM Algorithm},
    journal         = {Journal of the Royal Statistical Society. Series B (Methodological)},
    volume          = {45},
    number          = {1},
    pages           = {47-50},
    url             = {https://www.jstor.org/stable/2345622}
}

@inproceedings{Attias1999,
    author          = {Hagai Attias},
    year            = {1999},
    title           = {Inferring Parameters and Structure of Latent Variable Models by Variational Bayes},
    booktitle       = {Proceedings of the Fifteenth Conference in Artificial Intelligence},
    volume          = {},
    pages           = {21-30},
    url             = {https://dl.acm.org/doi/10.5555/2073796.2073799}
}

@article{Kingma-Welling2019,
    author          = {Diederik Kingma and Max Welling},
    year            = {2019},
    title           = {An Introduction to Variational Autoencoders},
    journal         = {Foundations and Treands in Machine Learning},
    volume          = {12},
    number          = {4},
    pages           = {307-392},
    url             = {https://www.nowpublishers.com/article/Details/MAL-056}
}

@inproceedings{Kingma-Welling2014,
    author          = {Diederik Kingma and Max Welling},
    year            = {2014},
    title           = {Auto-Encoding Variational Bayes},
    booktitle       = {International Conference on Learning Representations},
    volume          = {2},
    pages           = {},
    url             = {https://openreview.net/forum?id=33X9fd2-9FyZd}
}

@article{Jordan+1999,
    author          = {Michael I. Jordan and Zoubin Ghahramani and Tommi S. Jaakkola and Lawrence K. Saul},
    year            = {1999},
    title           = {An Introduction to Variational Methods for Graphical Models},
    journal         = {Machine Learning},
    volume          = {37},
    number          = {},
    pages           = {183-233},
    url             = {https://link.springer.com/article/10.1023/A:1007665907178}
}

@article{Peterson-Anderson1987,
    author          = {Carsten Peterson and James R. Anderson},
    year            = {1987},
    title           = {A Mean Field Theory Learning Algorithm for Neural Networks},
    journal         = {Complex Systems},
    volume          = {1},
    number          = {5},
    pages           = {1987},
    url             = {https://www.complex-systems.com/abstracts/v01_i05_a06/}
}

@book{Parisi1988,
    author         = {Giorgio Parisi},
    year           = {1988},
    title          = {Statistical Field Theory},
    series         = {},
    volume         = {},
    edition        = {},
    url            = {},
    publisher      = {Addison-Wesley}
}

@book{Bathe1996,
    author         = {Klaus-Jürgen Bathe},
    year           = {1996},
    title          = {Finite Element Procedures},
    series         = {},
    volume         = {},
    edition        = {},
    url            = {},
    publisher      = {Prentice-Hall}
}

@book{Sakurai1985,
    author         = {J. Sakurai},
    year           = {1985},
    title          = {Modern Quantum Mechanics},
    series         = {},
    volume         = {},
    edition        = {},
    url            = {},
    publisher      = {Addison-Wesley}
}

@book{Rustagi1976,
    author         = {J. Rustagi},
    year           = {1976},
    title          = {Variational Methods in Statistics},
    series         = {},
    volume         = {},
    edition        = {},
    url            = {},
    publisher      = {Academic Press}
}

@inbook{Neal-Hinton1998,
    author         = {Radford M. Neal and Geoffrey E. Hinton},
    chapter        = {A View of the EM Algorithm that Justifies Incremental, Sparse and Other Variants},
    editor         = {Michael I. Jordan},
    pages          = {355-368},
    publisher      = {Springer Dordrecht},
    title          = {Learning in Graphical Models},
    year           = {1998},
    url            = {https://link.springer.com/chapter/10.1007/978-94-011-5014-9_12},
}

@inproceedings{Rezende+2014,
    author          = {Danilo Jimenez Rezende and Shakir Mohamed and Daan Wierstra},
    year            = {2014},
    title           = {Approximate Inference in Deep Generative Models},
    booktitle       = {Proceedings of the 31st International Conference on Machine Learning},
    volume          = {32},
    pages           = {1278-1286},
    url             = {https://proceedings.mlr.press/v32/rezende14.html}
}


@InProceedings{Rezebde0Niganed2015,
  title = 	 {Variational Inference with Normalizing Flows},
  author = 	 {Rezende, Danilo and Mohamed, Shakir},
  booktitle = 	 {Proceedings of the 32nd International Conference on Machine Learning},
  pages = 	 {1530--1538},
  year = 	 {2015},
  editor = 	 {Bach, Francis and Blei, David},
  volume = 	 {37},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Lille, France},
  month = 	 {07--09 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v37/rezende15.pdf},
  url = 	 {https://proceedings.mlr.press/v37/rezende15.html},
  abstract = 	 {The choice of the approximate posterior distribution is one of the core problems in variational inference. Most applications of variational inference employ simple families of posterior approximations in order to allow for efficient inference, focusing on mean-field or other simple structured approximations. This restriction has a significant impact on the quality of inferences made using variational methods. We introduce a new approach for specifying flexible, arbitrarily complex and scalable approximate posterior distributions. Our approximations are distributions constructed through a normalizing flow, whereby a simple initial density is transformed into a more complex one by applying a sequence of invertible transformations until a desired level of complexity is attained. We use this view of normalizing flows to develop categories of finite and infinitesimal flows and provide a unified view of approaches for constructing rich posterior approximations. We demonstrate that the theoretical advantages of having posteriors that better match the true posterior, combined with the scalability of amortized variational approaches, provides a clear improvement in performance and applicability of variational inference.}
}


@article{Rosenblatt1958,
    author          = {F. Rosenblatt},
    year            = {1958},
    title           = {The Perceptron: A Probabilistic Model for Information Storage and Organization in the Brain},
    journal         = {Psychological Review},
    volume          = {65},
    number          = {6},
    pages           = {386-408},
    url             = {https://psycnet.apa.org/record/1959-09865-001}
}

@book{Minsky-Papert1969,
    author         = {Marvin Minsky and Seymour A. Papert},
    year           = {1969},
    title          = {Perceptrons: An Introduction to Computational Geometry},
    series         = {},
    volume         = {},
    edition        = {},
    url            = {https://direct.mit.edu/books/book/3132/PerceptronsAn-Introduction-to-Computational},
    publisher      = {The MIT Press}
}

@book{Hebb1949,
    author         = {D. O. Hebb},
    year           = {1949},
    title          = {The Organization of Behavior: A Neuropsychological Theory},
    series         = {},
    volume         = {},
    edition        = {},
    url            = {https://www.taylorfrancis.com/books/mono/10.4324/9781410612403/organization-behavior-hebb},
    publisher      = {John Wiley \& Sons, Chapman and Hall}
}

@article{McCulloch-Pitts1943,
    author          = {W. McCulloch and W. Pitts},
    year            = {1943},
    title           = {A Logical Calculus of the Ideas Immanent in Nervous Activity},
    journal         = {Bulletin of Mathematical Biophysics},
    volume          = {7},
    number          = {},
    pages           = {115-133},
    url             = {https://link.springer.com/article/10.1007/BF02478259}
}

@article{Siegelmann-Sontag1991,
    author          = {Hava T. Siegelmann and Eduardo D. Sontag},
    year            = {1991},
    title           = {Turing Computability with Neural Nets},
    journal         = {Applied Mathematics Letters},
    volume          = {4},
    number          = {6},
    pages           = {77-80},
    url             = {https://www.sciencedirect.com/science/article/pii/089396599190080F}
}

@book{人工知能学会2015,
    author         = {麻生英樹 and 安田宗樹 and 前田新一 and 岡野原大輔 and 岡谷貴之 and 久保陽太郎 and ボレガラダヌシカ},
    year           = {2015},
    title          = {深層学習},
    series         = {},
    volume         = {},
    edition        = {},
    url            = {https://www.kindaikagaku.co.jp/book_list/detail/9784764904873/},
    publisher      = {近代科学社}
}

@book{甘利俊一1989,
    author         = {甘利俊一},
    year           = {1989},
    title          = {神経回路網モデルとコネクショニズム},
    series         = {認知科学選書},
    volume         = {22},
    edition        = {},
    url            = {https://www.utp.or.jp/book/b305707.html},
    publisher      = {東京大学出版会}
}

@article{Amari1967,
    author          = {Shunichi Amari},
    year            = {1967},
    title           = {A Theory of Adaptive Pattern Classifiers},
    journal         = {IEEE Transactions on Electronic Computers},
    volume          = {EC-16},
    number          = {3},
    pages           = {299-307},
    url             = {https://ieeexplore.ieee.org/document/4039068}
}

@book{Amari1985,
    author         = {Shun-ichi Amari},
    year           = {1985},
    title          = {Differential-Geometrical Methods in Statistics},
    series         = {Lecture Notes in Statistics},
    volume         = {28},
    edition        = {},
    url            = {https://link.springer.com/book/10.1007/978-1-4612-5056-2},
    publisher      = {Springer New York}
}

@article{Fukushima1980,
    author          = {K. Fukushima},
    year            = {1980},
    title           = {Neural Network Model for a Mechanism of Pattern Recognition Unaffected by Shift in Position--Neocognitron},
    journal         = {Biological Cybernetics},
    volume          = {36},
    number          = {},
    pages           = {193-202},
    url             = {https://link.springer.com/article/10.1007/BF00344251}
}

@article{Hubel-Wiesel1959,
    author          = {D. H. Wiesel and T. N. Hubel},
    year            = {1959},
    title           = {Receptive Fields of Single Neurones in the Cat's Striate Cortex},
    journal         = {Journal of Physiology},
    volume          = {148},
    number          = {3},
    pages           = {574-591},
    url             = {https://physoc.onlinelibrary.wiley.com/doi/10.1113/jphysiol.1959.sp006308}
}

@article{Fukushima1975,
    author          = {K. Fukushima},
    year            = {1975},
    title           = {Cognitron: a Self-Organizing Multilayered Neural Network},
    journal         = {Biological Cybernetics},
    volume          = {20},
    number          = {},
    pages           = {121-136},
    url             = {}
}

@article{Malsburg1973,
    author          = {C. von{\ }der{\ }Malsburg},
    year            = {1973},
    title           = {Self-Organization of Orientation Sensitive Cells in the Striate Cortex},
    journal         = {Kybernetik},
    volume          = {14},
    number          = {},
    pages           = {85-100},
    url             = {}
}

@article{Hubel1967,
    author          = {David H. Hubel},
    year            = {1967},
    title           = {Effects of Distortion of Sensory Input on the Visual System of Kittens},
    journal         = {The Physiologist},
    volume          = {10},
    number          = {},
    pages           = {17-45},
    url             = {}
}

@article{Sejnowski-Rosenberg1987,
    author          = {Terrence J. Sejnowski and Charles R. Rosenberg},
    year            = {1987},
    title           = {Parallel Networks that Learn to Pronounce English Text},
    journal         = {Complex Systems},
    volume          = {1},
    number          = {1},
    pages           = {145-168},
    url             = {https://www.complex-systems.com/abstracts/v01_i01_a10/}
}

@inproceedings{Bengio+2006,
    author          = {Yoshua Bengio and Pascal Lamblin and Dan Popovici and Hugo Larochelle},
    year            = {2006},
    title           = {Greedy Layer-Wise Training of Deep Networks},
    booktitle       = {Advances in Neural Information Processing Systems},
    volume          = {19},
    pages           = {153-160},
    url             = {https://papers.nips.cc/paper_files/paper/2006/hash/5da713a690c067105aeb2fae32403405-Abstract.html}
}

@inproceedings{Cottrell-Munro1988,
    author          = {Garrison W. Cottrell and Paul Munro},
    year            = {1988},
    title           = {Principal Component Analysis of Images via Back Propagation},
    booktitle       = {Proceedings of SPIE Visual Communications and Image Processings},
    volume          = {1001},
    pages           = {1070-1076},
    url             = {https://www.spiedigitallibrary.org/conference-proceedings-of-spie/1001/1/Principal-Components-Analysis-Of-Images-Via-Back-Propagation/10.1117/12.969060.short}
}

@article{Baldi-Hornik1989,
    author          = {Pierre Baldi and Kurt Hornik},
    year            = {1989},
    title           = {Neural Networks and Principal Component Analysis: Learning from Examples without Local Minima},
    journal         = {Neural Networks},
    volume          = {2},
    number          = {1},
    pages           = {53-58},
    url             = {https://www.sciencedirect.com/science/article/pii/0893608089900142}
}

@inproceedings{DeMers-Cottrell1992,
    author          = {David DeMers and Garrison Cottrell},
    year            = {1992},
    title           = {Non-Linear Dimensionality Reduction},
    booktitle       = {Advances in Neural Information Processing Systems},
    volume          = {5},
    pages           = {580-587},
    url             = {https://proceedings.neurips.cc/paper/1992/hash/cdc0d6e63aa8e41c89689f54970bb35f-Abstract.html}
}

@inproceedings{He+2016,
    author          = {Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
    year            = {2016},
    title           = {Deep Residual Learning for Image Recognition},
    booktitle       = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
    volume          = {},
    pages           = {770-778},
    url             = {https://openaccess.thecvf.com/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html}
}

@inproceedings{Vaswani+2017,
    author          = {Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Łukasz Kaiser and Illia Polosukhin},
    year            = {2017},
    title           = {Attention is All you Need},
    booktitle       = {Advances in Neural Information Processing Systems},
    volume          = {30},
    pages           = {},
    url             = {https://papers.nips.cc/paper_files/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html}
}

@inproceedings{Hecht-Nielsen1989,
    author          = {R. Hecht-Nielsen},
    year            = {1989},
    title           = {Theory of the Backpropagation Neural Network},
    booktitle       = {International 1989 Joint Conference on Neural Networks},
    volume          = {},
    pages           = {},
    url             = {https://ieeexplore.ieee.org/document/118638}
}

@article{Ballard1987,
    author          = {Dana H. Ballard},
    year            = {1987},
    title           = {Modular Learning in Neural Networks},
    journal         = {Proceedings of the Sixth National Conference on Artificial Intelligence},
    volume          = {1},
    number          = {},
    pages           = {279-284},
    url             = {https://dl.acm.org/doi/10.5555/1863696.1863746}
}

@article{Hinton+2006,
    author          = {Geoffrey E. Hinton and Simon Esindero and Yee-Whye Teh},
    year            = {2006},
    title           = {A Fast Learning Algorithm for Deep Belief Nets},
    journal         = {Naural Computation},
    volume          = {18},
    number          = {7},
    pages           = {1527-1554},
    url             = {https://direct.mit.edu/neco/article/18/7/1527/7065/A-Fast-Learning-Algorithm-for-Deep-Belief-Nets}
}

@article{Hopfield1982,
    author          = {J. J. Hopfield},
    year            = {1982},
    title           = {Neural Networks and Physical Systems with Emergent Collective Computational Abilities},
    journal         = {Proceedings of the National Academy of Science},
    volume          = {79},
    number          = {8},
    pages           = {2554-2558},
    url             = {https://www.pnas.org/doi/10.1073/pnas.79.8.2554}
}

@article{Hopfield-Tank1985,
    author          = {J. J. Hopfield and D. W. Tank},
    year            = {1985},
    title           = {"Neural" Computation of Decisions in Optimization Problems},
    journal         = {Biological Cybernetics},
    volume          = {52},
    number          = {},
    pages           = {141-152},
    url             = {https://link.springer.com/article/10.1007/BF00339943}
}

@article{Robbins-Monro1951,
    author          = {Herbert Robbins and Sutton Monro},
    year            = {1951},
    title           = {A Stochastic Approximation Method},
    journal         = {The Annals of Mathematical Statistics},
    volume          = {22},
    number          = {3},
    pages           = {400-407},
    url             = {https://www.jstor.org/stable/2236626}
}

@inproceedings{Robbins1956,
    author          = {Herbert Robbins},
    year            = {1956},
    title           = {{An Empirical Bayes Approach to Statistics}},
    booktitle       = {Proceedings of the Third Berkeley Symposium on Mathematical Statistics and Probability},
    volume          = {1},
    pages           = {157-163},
    url             = {https://projecteuclid.org/ebooks/berkeley-symposium-on-mathematical-statistics-and-probability/Proceedings-of-the-Third-Berkeley-Symposium-on-Mathematical-Statistics-and-Probability/chapter/An-Empirical-Bayes-Approach-to-Statistics/bsmsp/1200512992}
}

@article{Kiefer-Wolfowitz1952,
    author          = {J. Kiefer and J. Wolfowitz},
    year            = {1952},
    title           = {Stochastic Estimation of the Maximum of a Regression Function},
    journal         = {The Annals of Mathematical Statistics},
    volume          = {22},
    number          = {3},
    pages           = {462-466},
    url             = {https://www.jstor.org/stable/2236690}
}

@inproceedings{Duchi+2011,
    author          = {John Duchi and Elad Hazan and Yoram Singer},
    year            = {2011},
    title           = {Adaptive Subgradient Methods for Online Learning and Stochastic Optimization},
    booktitle       = {Journal of Machine Learning Research},
    volume          = {12},
    number          = {61},
    pages           = {2121-2159},
    url             = {https://jmlr.org/papers/v12/duchi11a.html}
}

@unpublished{Tieleman-Hinton2012,
    author = {T. Tieleman and Geoffrey Hinton},
    year   = {2012},
    title  = {Neural Networks for Machine Learning. Lecture 6.5: Divide the Gradient by a Running Average of its Recent Magnitude},
    url    = {https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf}
}

@inproceedings{Nowozin+2016,
    author          = {Sebastian Nowozin and Botond Cseke and Ryota Tomioka},
    year            = {2016},
    title           = {f-GAN: Training Generative Neural Samplers using Variational Divergence Minimization},
    booktitle       = {Advances in Neural Information Processing Systems},
    volume          = {29},
    pages           = {},
    url             = {https://papers.nips.cc/paper/2016/hash/cedebb6e872f539bef8c3f919874e9d7-Abstract.html}
}

@inproceedings{Arjovsky+2017,
    author          = {Martin Arjovsky and Soumith Chintala and Léon Bottou},
    year            = {2017},
    title           = {Wasserstein Generative Adversarial Networks},
    booktitle       = {Proceedings of the 34th International Conference on Machine Learning},
    volume          = {70},
    pages           = {214-223},
    url             = {http://proceedings.mlr.press/v70/arjovsky17a.html}
}

@inbook{Robert1996,
    author         = {C. P. Robert},
    chapter        = {Inference in Mixture Models},
    editor         = {W. R. Gilks and David Spiegelhalter},
    pages          = {441-464},
    publisher      = {Chapman \& Hall, London},
    title          = {Markov Chain Monte Carlo in Practice},
    year           = {1996}
}

@inproceedings{Mengersen-Robert1996,
    author          = {Kerrie L. Mengersen and Christian P. Robert},
    year            = {1996},
    title           = {Testing for Mixtures: A Bayesian Entropic Approach},
    booktitle       = {Bayesian Statistics 5: Proceedings of the Fifth Valencia International Meetings},
    pages           = {255-276},
    url             = {https://academic.oup.com/book/54042/chapter-abstract/422209682}
}

@article{Wei-Tanner1990,
    author          = {Greg C. G. Wei and Martin A. Tanner},
    year            = {1990},
    title           = {A Monte Carlo Implementation of the EM Algorithm and the Poor Man's Data Augmentation Algorithm},
    journal         = {Journal of the American Statistical Association},
    volume          = {85},
    number          = {411},
    pages           = {699-704},
    url             = {https://www.jstor.org/stable/2290005}
}

@article{Wei-Tanner1990b,
    author          = {Greg C. G. Wei and Martin A. Tanner},
    year            = {1990},
    title           = {Posterior Computations for Censored Regression Data},
    journal         = {Journal of the American Statistical Association},
    volume          = {85},
    number          = {411},
    pages           = {829-839},
    url             = {https://www.jstor.org/stable/2290022}
}

@article{Dieblot-Robert1994,
    author          = {Jean Diebolt and Christian P. Robert},
    year            = {1994},
    title           = {Estimation of Finite Mixture Distributions through Bayesian Sampling},
    journal         = {Journal of the Royal Statistical Society. Series B (Methodological)},
    volume          = {56},
    number          = {2},
    pages           = {363-375},
    url             = {https://www.jstor.org/stable/2345907}
}

@article{Lloyd1982,
    author          = {S. Lloyd},
    year            = {1982},
    title           = {Least Squares Quantization in PCM},
    journal         = {IEEE Transactions on Information Theory},
    volume          = {28},
    number          = {2},
    pages           = {129-137},
    url             = {https://ieeexplore.ieee.org/document/1056489}
}

@book{Fletcher1987,
    author         = {R. Fletcher},
    year           = {1987},
    title          = {Practical Methods of Optimization},
    series         = {},
    volume         = {},
    edition        = {2},
    url            = {https://onlinelibrary.wiley.com/doi/book/10.1002/9781118723203},
    publisher      = {John Wiley \& Sons}
}

@article{Meng-Rubin1993,
    author          = {Xiao-Li Meng and Donald B. Rubin},
    year            = {1993},
    title           = {Maximum Likelihood Estimation via the ECM Algorithm: A General Framework},
    journal         = {Biometrika},
    volume          = {80},
    number          = {2},
    pages           = {267-278},
    url             = {https://www.jstor.org/stable/2337198}
}

@book{Hrafnkelsson2023,
    author         = {},
    editor         = {Birgir Hrafnkelsson},
    year           = {2023},
    title          = {Statistical Modeling Using Bayesian Latent Gaussian Models: With Applications in Geophysics and Encironmental Sciences},
    series         = {},
    volume         = {},
    edition        = {},
    url            = {https://link.springer.com/book/10.1007/978-3-031-39791-2},
    publisher      = {Springer Cham}
}

@inproceedings{MacQueen1967,
    author          = {J. MacQueen},
    year            = {1967},
    title           = {Some Methods for Classification and Analysis of Multivariate Observations},
    booktitle       = {Proceedings of the Fifth Berkeley Symposium on Mathematical Statistics and Probability},
    volume          = {1},
    pages           = {281-297},
    url             = {https://projecteuclid.org/ebooks/berkeley-symposium-on-mathematical-statistics-and-probability/Proceedings-of-the-Fifth-Berkeley-Symposium-on-Mathematical-Statistics-and/chapter/Some-methods-for-classification-and-analysis-of-multivariate-observations/bsmsp/1200512992}
}

@book{Kapur1989,
    author         = {J. N. Kapur},
    year           = {1989},
    title          = {Maximum-Entropy Models in Science and Engineering},
    series         = {},
    volume         = {},
    edition        = {2},
    url            = {},
    publisher      = {Wiley}
}

@inbook{Fenyman+1964,
    author         = {Richard P. Feynman and R. B. Leighton and M. Sands},
    chapter        = {19. The Principle of Least Action},
    volume         = {II},
    editor         = {},
    pages          = {},
    publisher      = {Addison-Wesley},
    title          = {The Feynman Lectures of Physics},
    year           = {1964},
    url            = {https://www.feynmanlectures.caltech.edu/II_19.html},
}

@inbook{Fenyman+1963,
    author         = {Richard P. Feynman and R. B. Leighton and M. Sands},
    chapter        = {1. Atoms in Motion},
    volume         = {I},
    editor         = {},
    pages          = {},
    publisher      = {Addison-Wesley},
    title          = {The Feynman Lectures of Physics},
    year           = {1964},
    url            = {https://www.feynmanlectures.caltech.edu/I_01.html},
}

@book{Schwarz1988,
    author         = {H. R. Schwarz},
    year           = {1988},
    title          = {Finite Element Methods},
    series         = {},
    volume         = {},
    edition        = {},
    url            = {},
    publisher      = {Academic Press}
}

@book{Boyd-Vandenberghe2004,
    author         = {Stephen Boyd and Lieven Vandenberghe},
    year           = {2004},
    title          = {Convex Optimization},
    series         = {},
    volume         = {},
    edition        = {},
    url            = {https://www.cambridge.org/core/books/convex-optimization/E413CEF23D00BD463DCCE0600810D3FA},
    publisher      = {Cambridge University Press}
}

@inproceedings{Minka2001,
    author          = {Thomas P. Minka},
    year            = {2001},
    title           = {Expectation Propagation for Approximate Bayesian Inference},
    booktitle       = {Proceedings of the Seventeenth Conference on Uncertainty in Artificial Intelligence},
    volume          = {},
    pages           = {362-369},
    url             = {https://dl.acm.org/doi/10.5555/2074022.2074067}
}

@phdthesis{Minka2001b,
    author      = {Thomas P. Minka},
    school      = {MIT},
    title       = {A Family of Approximation Algorithms for Bayesian Inference},
    year        = {2001}
}

@article{Cichocki+2008,
    author          = {Andrzej Cichocki and Hyekyoung Lee and Yong-Deok Kim and Seungjin Choi},
    year            = {2008},
    title           = {Non-negative Matrix Factorization with $\alpha$-divergence},
    journal         = {Pattern Recognition Letters},
    volume          = {29},
    number          = {9},
    pages           = {1433-1440},
    url             = {https://www.sciencedirect.com/science/article/abs/pii/S0167865508000767}
}

@techreport{Minka2004,
    author      = {Tomas P. Minka},
    institution = {Microsoft Research Cambridge},
    title       = {Power EP},
    year        = {2004},
    url         = {https://www.microsoft.com/en-us/research/publication/power-ep/},
}

@techreport{Minka2005,
    author      = {Tomas P. Minka},
    institution = {Microsoft Research Cambridge},
    title       = {Divergence Measures and Message Passing},
    year        = {2005},
    url         = {https://www.microsoft.com/en-us/research/publication/divergence-measures-and-message-passing/},
}

@techreport{Brooks+2024,
    author      = {Tim Brooks and Bill Peebles and Connor Holmes and Will DePue and Yufei Guo and Li Jing and David Schnurr and Joe Taylor and Troy Luhman and Eric Luhman and Clarence Wing Yin Ng and Ricky Wang and Aditya Ramesh},
    institution = {OpenAI},
    title       = {Video generation models as world simulators},
    year        = {2024},
    url         = {https://openai.com/research/video-generation-models-as-world-simulators},
}

@inproceedings{Razavi+2019,
    author          = {Ali Razavi and Aaron van{\ }den{\ }Oord and Oriol Vinyals},
    year            = {2019},
    title           = {Generating Diverse High-Fidelity Images with VQ-VAE-2},
    booktitle       = {Advances in Neural Information Processing Systems},
    volume          = {32},
    pages           = {},
    url             = {https://papers.nips.cc/paper_files/paper/2019/hash/5f8e2fa1718d1bbcadf1cd9c7a54fb8c-Abstract.html}
}

@inproceedings{vandenOord+2017,
    author          = {Aaron van{\ }den{\ }Oord and Oriol Vinyals and Koray Kavukcuoglu},
    year            = {2017},
    title           = {Neural Discrete Representation Learning},
    booktitle       = {Advances in Neural Information Processing Systems},
    volume          = {30},
    pages           = {},
    url             = {https://papers.nips.cc/paper_files/paper/2017/hash/7a98af17e63a0ac09ce2e96d03992fbc-Abstract.html}
}

@inproceedings{Wallace1992,
    author          = {G. K. Wallace},
    year            = {1992},
    title           = {The JPEG Still Picture Compression Standard},
    booktitle       = {IEEE Transactions on Consumer Electronics},
    volume          = {38},
    pages           = {1},
    url             = {https://ieeexplore.ieee.org/document/125072}
}

@inproceedings{Paisley+2012,
    author          = {John Paisley and David M. Blei and Michael I. Jordan},
    year            = {2012},
    title           = {Variational Bayesian Inference with Stochastic Search},
    booktitle       = {Proceedings of the 29th International Conference on Machine Learning},
    volume          = {},
    pages           = {1363-1370},
    url             = {https://dl.acm.org/doi/10.5555/3042573.3042748}
}

@inbook{Diebolt-Ip1996,
    author         = {J. Diebolt and E. Ip},
    chapter        = {Stochastic EM: Method and Application},
    editor         = {W. R. Gilks and S. Richardson and David Spiegelhalter},
    pages          = {259-274},
    publisher      = {Chapman and Hall},
    title          = {Markov Chain Monte Carlo in Practice},
    year           = {1996},
    url            = {https://www.taylorfrancis.com/books/mono/10.1201/b14835/markov-chain-monte-carlo-practice-david-spiegelhalter-gilks-richardson},
}

@article{Celeux-Diebolt1985,
    author          = {G. Celeux and J. Diebolt},
    year            = {1985},
    title           = {The SEM Algorithm: A Probabilistic Teacher Algorithm Derived from the EM Algorithm for the Mixture Problem},
    journal         = {Computational Statistics Quarterly},
    volume          = {2},
    number          = {},
    pages           = {73-82},
    url             = {}
}

@article{Meng-Rubin1991,
    author          = {Xiao-Li Meng and Donald B. Rubin},
    year            = {1991},
    title           = {Using EM to Obtain Asymptotic Variance-Covariance Matrices: The SEM Algorithm},
    journal         = {Journal of the American Statistical Association},
    volume          = {86},
    number          = {416},
    pages           = {899-909},
    url             = {https://www.jstor.org/stable/2290503}
}

@article{Doucet+2002,
    author          = {Arnaud Doucet and Simon J. Godsill and Christian P. Robert},
    year            = {2002},
    title           = {Marginal Maximum a Posteriori Estimation using Markov Chain Monte Carlo},
    journal         = {Statistics and Computing},
    volume          = {12},
    number          = {},
    pages           = {77-84},
    url             = {https://link.springer.com/article/10.1023/A:1013172322619}
}

@book{Bouleau-Lepingle1993,
    author         = {Nicolas Bouleau and Dominique Lépingle},
    year           = {1993},
    title          = {Numerical Methods for Stochastic Processes},
    series         = {},
    volume         = {},
    edition        = {},
    url            = {https://www.wiley.com/en-ca/Numerical+Methods+for+Stochastic+Processes-p-9780471546412},
    publisher      = {Wiley}
}

@inbook{Geyer1996,
    author         = {C. Geyer},
    chapter        = {Estimation and Optimization of Functions},
    editor         = {W. R. Gilks and S. Richardson and David Spiegelhalter},
    pages          = {241-258},
    publisher      = {Chapman and Hall},
    title          = {Markov Chain Monte Carlo in Practice},
    year           = {1996},
    url            = {https://www.taylorfrancis.com/books/mono/10.1201/b14835/markov-chain-monte-carlo-practice-david-spiegelhalter-gilks-richardson},
}

@unpublished{Radford+2019,
    author = {Alec Radford and Jeffrey Wu and Rewon Child and David Luan and Dario Amodei and Ilya Sutskever},
    year   = {2019},
    title  = {Language Models are Unsupervised Multitask Learners},
    url    = {https://github.com/openai/gpt-2?tab=readme-ov-file}
}

@inproceedings{Brown+2020,
    author          = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
    year            = {2020},
    title           = {Language Models are Few-Shot Learners},
    booktitle       = {Advances in Neural Information Processing Systems},
    volume          = {33},
    pages           = {1877-1901},
    url             = {https://papers.nips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html}
}

@unpublished{OpenAI2023,
    author = {OpenAI},
    year   = {2023},
    title  = {GPT-4 Technical Report},
    url    = {https://arxiv.org/abs/2303.08774}
}

@unpublished{Sutton2019,
    author = {Rich Sutton},
    year   = {2019},
    title  = {The Bitter Lesson},
    url    = {http://www.incompleteideas.net/IncIdeas/BitterLesson.html}
}

@inproceedings{Zhou+2023,
title={{LIMA}: Less Is More for Alignment},
author={Chunting Zhou and Pengfei Liu and Puxin Xu and Srini Iyer and Jiao Sun and Yuning Mao and Xuezhe Ma and Avia Efrat and Ping Yu and LILI YU and Susan Zhang and Gargi Ghosh and Mike Lewis and Luke Zettlemoyer and Omer Levy},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023},
url={https://openreview.net/forum?id=KBMOKmX2he}
}

@unpublished{Kaplan+2020,
author       = {Jared Kaplan and
                  Sam McCandlish and
                  Tom Henighan and
                  Tom B. Brown and
                  Benjamin Chess and
                  Rewon Child and
                  Scott Gray and
                  Alec Radford and
                  Jeffrey Wu and
                  Dario Amodei},
    year   = {2020},
    title  = {Scaling Laws for Neural Language Models},
    url    = {https://arxiv.org/abs/2001.08361}
}

@article{Bommasani+2021,
title={On the Opportunities and Risks of Foundation Models},
author={Rishi Bommasani and Drew A. Hudson and Ehsan Adeli and Russ Altman and Simran Arora and Sydney von Arx and Michael S. Bernstein and Jeannette Bohg and Antoine Bosselut and Emma Brunskill and Erik Brynjolfsson and S. Buch and Dallas Card and Rodrigo Castellon and Niladri S. Chatterji and Annie S. Chen and Kathleen A. Creel and Jared Davis and Dora Demszky and Chris Donahue and Moussa Doumbouya and Esin Durmus and Stefano Ermon and John Etchemendy and Kawin Ethayarajh and Li Fei-Fei and Chelsea Finn and Trevor Gale and Lauren E. Gillespie and Karan Goel and Noah D. Goodman and Shelby Grossman and Neel Guha and Tatsunori Hashimoto and Peter Henderson and John Hewitt and Daniel E. Ho and Jenny Hong and Kyle Hsu and Jing Huang and Thomas F. Icard and Saahil Jain and Dan Jurafsky and Pratyusha Kalluri and Siddharth Karamcheti and Geoff Keeling and Fereshte Khani and O. Khattab and Pang Wei Koh and Mark S. Krass and Ranjay Krishna and Rohith Kuditipudi and Ananya Kumar and Faisal Ladhak and Mina Lee and Tony Lee and Jure Leskovec and Isabelle Levent and Xiang Lisa Li and Xuechen Li and Tengyu Ma and Ali Malik and Christopher D. Manning and Suvir P. Mirchandani and Eric Mitchell and Zanele Munyikwa and Suraj Nair and Avanika Narayan and Deepak Narayanan and Benjamin Newman and Allen Nie and Juan Carlos Niebles and Hamed Nilforoshan and J. F. Nyarko and Giray Ogut and Laurel Orr and Isabel Papadimitriou and Joon Sung Park and Chris Piech and Eva Portelance and Christopher Potts and Aditi Raghunathan and Robert Reich and Hongyu Ren and Frieda Rong and Yusuf H. Roohani and Camilo Ruiz and Jack Ryan and Christopher R'e and Dorsa Sadigh and Shiori Sagawa and Keshav Santhanam and Andy Shih and Krishna Parasuram Srinivasan and Alex Tamkin and Rohan Taori and Armin W. Thomas and Florian Tram{\`e}r and Rose E. Wang and William Wang and Bohan Wu and Jiajun Wu and Yuhuai Wu and Sang Michael Xie and Michihiro Yasunaga and Jiaxuan You and Matei A. Zaharia and Michael Zhang and Tianyi Zhang and Xikun Zhang and Yuhui Zhang and Lucia Zheng and Kaitlyn Zhou and Percy Liang},
journal={ArXiv},
year={2021},
url={https://crfm.stanford.edu/report.html}
}

@unpublished{Hu+2021,
  author       = {Edward J. Hu and
                  Yelong Shen and
                  Phillip Wallis and
                  Zeyuan Allen{-}Zhu and
                  Yuanzhi Li and
                  Shean Wang and
                  Weizhu Chen},
    year   = {2021},
    title  = {LoRA: Low-Rank Adaptation of Large Language Models},
    url    = {https://arxiv.org/abs/2106.09685}
}

@inproceedings{Aghajanyan+2021,
    author          = {Armen Aghajanyan and Sonal Gupta and Luke Zettlemoyer},
    year            = {2021},
    title           = {Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning},
    booktitle       = {Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing},
    volume          = {1},
    pages           = {7319-7328},
    url             = {https://aclanthology.org/2021.acl-long.568/}
}

@inproceedings{Christiano+2017,
    author          = {Paul F. Christiano and Jan Leike and Tom Brown and Miljan Martic and Shane Legg and Dario Amodei},
    year            = {2017},
    title           = {Deep Reinforcement Learning from Human Preferences},
    booktitle       = {Advances in Neural Information Processing Systems},
    volume          = {30},
    pages           = {},
    url             = {https://papers.nips.cc/paper_files/paper/2017/hash/d5e2c0adad503c91f91df240d0cd4e49-Abstract.html}
}

@article{Liu+2023-PPP,
    author          = {Pengfei Liu and Weizhe Yuan and Jinlan Fu and Zhengbao Jiang and Hiroaki Hayashi and Graham Neubig},
    year            = {2023},
    title           = {Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing},
    journal         = {ACM Computing Surveys},
    volume          = {55},
    number          = {9},
    pages           = {1-35},
    url             = {https://dl.acm.org/doi/full/10.1145/3560815}
}


@InProceedings{Liu+2023I2SB,
  title = 	 {{I}$^2${SB}: Image-to-Image Schrödinger Bridge},
  author =       {Liu, Guan-Horng and Vahdat, Arash and Huang, De-An and Theodorou, Evangelos and Nie, Weili and Anandkumar, Anima},
  booktitle = 	 {Proceedings of the 40th International Conference on Machine Learning},
  pages = 	 {22042--22062},
  year = 	 {2023},
  editor = 	 {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  volume = 	 {202},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {23--29 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v202/liu23ai/liu23ai.pdf},
  url = 	 {https://proceedings.mlr.press/v202/liu23ai.html},
  abstract = 	 {We propose Image-to-Image Schrödinger Bridge (I$^2$SB), a new class of conditional diffusion models that directly learn the nonlinear diffusion processes between two given distributions. These diffusion bridges are particularly useful for image restoration, as the degraded images are structurally informative priors for reconstructing the clean images. I$^2$SB belongs to a tractable class of Schrödinger bridge, the nonlinear extension to score-based models, whose marginal distributions can be computed analytically given boundary pairs. This results in a simulation-free framework for nonlinear diffusions, where the I$^2$SB training becomes scalable by adopting practical techniques used in standard diffusion models. We validate I$^2$SB in solving various image restoration tasks, including inpainting, super-resolution, deblurring, and JPEG restoration on ImageNet 256$\times$256 and show that I$^2$SB surpasses standard conditional diffusion models with more interpretable generative processes. Moreover, I$^2$SB matches the performance of inverse methods that additionally require the knowledge of the corruption operators. Our work opens up new algorithmic opportunities for developing efficient nonlinear diffusion models on a large scale. Project page and codes: https://i2sb.github.io/}
}


@unpublished{Bubeck+2023,
      author={Sébastien Bubeck and Varun Chandrasekaran and Ronen Eldan and Johannes Gehrke and Eric Horvitz and Ece Kamar and Peter Lee and Yin Tat Lee and Yuanzhi Li and Scott Lundberg and Harsha Nori and Hamid Palangi and Marco Tulio Ribeiro and Yi Zhang},
    year   = {2023},
    title  = {Sparks of Artificial General Intelligence: Early Experiments with GPT-4},
    url    = {https://arxiv.org/abs/2303.12712}
}
@article{Bubeck2015,
	author = {S{\'e}bastien Bubeck},
	doi = {10.1561/2200000050},
	issn = {1935-8237},
	journal = {Foundations and Trends{\textregistered} in Machine Learning},
	number = {3-4},
	pages = {231-357},
	title = {Convex Optimization: Algorithms and Complexity},
	url = {http://dx.doi.org/10.1561/2200000050},
	volume = {8},
	year = {2015},
	bdsk-url-1 = {http://dx.doi.org/10.1561/2200000050}}
@book{Beck2017,
    author = {Amir Beck},
    year = {2017},
    title = {First-Order Methods in Optimization},
    series = {MOS-SIAM Series on Optimization},
    volume = {},
    edition = {},
    url = {https://doi.org/10.1137/1.9781611974997},
    publisher = {Society for Industrial and Applied Mathematics}
}
@book{Ekeland-Temam1999,
    author = {Ivar Ekeland and Roger Témam},
    year = {1999},
    title = {Convex Analysis and Variational Problems},
    series = {Classics in Applied Mathematics},
    volume = {},
    edition = {},
    url = {https://doi.org/10.1137/1.9781611971088},
    publisher = {Society for Industrial and Applied Mathematics}
}

@unpublished{Bahdanau+2015,
    author = {Dzmitry Bahdanau and Kyunghyun Cho and Yoshua Bengio},
    year   = {2015},
    title  = {Neural Machine Translation by Jointly Learning to Align and Translate},
    url    = {https://arxiv.org/abs/1409.0473}
}

@unpublished{Ba+2016,
    author = {Jimmy Lei Ba and Jamie Ryan Kiros and Geoffrey E. Hinton},
    year   = {2016},
    title  = {Layer Normalization},
    url    = {https://arxiv.org/abs/1607.06450}
}

@unpublished{Dufter+2021,
    author = {Philipp Dufter and Martin Schmitt and Hinrich Schütze},
    year   = {2021},
    title  = {Position Information in Transformers: An Overview},
    url    = {https://arxiv.org/abs/2102.11090}
}

@unpublished{Mikolov2013,
    author = {Tomas Mikolov and Kai Chen and Greg Corrado and Jeffrey Dean},
    year   = {2013},
    title  = {Efficient Estimation of Word Representations in Vector Space},
    url    = {https://arxiv.org/abs/1301.3781}
}


@inproceedings{Mikolov2013b,
	author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S and Dean, Jeff},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
	publisher = {Curran Associates, Inc.},
	title = {Distributed Representations of Words and Phrases and their Compositionality},
	url = {https://proceedings.neurips.cc/paper_files/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf},
	volume = {26},
	year = {2013},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf}}


@inproceedings{Bengio+2000,
    author          = {Yoshua Bengio and Réjean Ducharme and Pascal Vincent},
    year            = {2000},
    title           = {A Neural Probabilistic Language Model},
    booktitle       = {Advances in Neural Information Processing Systems},
    volume          = {13},
    pages           = {},
    url             = {https://papers.nips.cc/paper_files/paper/2000/hash/728f206c2a01bf572b5940d7d9a8fa4c-Abstract.html}
}

@inproceedings{Sennrich+2016,
    author          = {Rico Sennrich and Barry Haddow and Alexandra Birch},
    year            = {2016},
    title           = {Neural Machine Translation of Rare Words with Subword Units},
    booktitle       = {Proceedings of the 54th Annual Meetings of the Association for Computational Linguistics},
    volume          = {1},
    pages           = {1715-1725},
    url             = {https://aclanthology.org/P16-1162/}
}

@unpublished{Zhao+2023,
          author={Wayne Xin Zhao and Kun Zhou and Junyi Li and Tianyi Tang and Xiaolei Wang and Yupeng Hou and Yingqian Min and Beichen Zhang and Junjie Zhang and Zican Dong and Yifan Du and Chen Yang and Yushuo Chen and Zhipeng Chen and Jinhao Jiang and Ruiyang Ren and Yifan Li and Xinyu Tang and Zikang Liu and Peiyu Liu and Jian-Yun Nie and Ji-Rong Wen},
    year   = {2023},
    title  = {A Survey of Large Language Models},
    url    = {https://arxiv.org/abs/2303.18223}
}

@inproceedings{Devlin+2019,
    author          = {Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
    year            = {2019},
    title           = {BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
    booktitle       = {Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human language Technologies},
    volume          = {1},
    pages           = {4171-4186},
    url             = {https://aclanthology.org/N19-1423/}
}

@inproceedings{Holtzman+2020,
    author          = {Ari Holtzman and Jan Buys and Li Du and Maxwell Forbes and Yejin Choi},
    year            = {2020},
    title           = {The Curious Case of Neural Text Degeneration},
    booktitle       = {International Conference on Learning Representations},
    volume          = {},
    pages           = {},
    url             = {https://arxiv.org/abs/1904.09751}
}

@inproceedings{Mikolov+2010,
    author          = {T. Mikolov and J. Kopecky and L. Burget and J. \u{C}ernocky and S. Khudanpur},
    year            = {2010},
    title           = {Recurrent Neural Network Based Language Model},
    booktitle       = {Proceedings of Interspeech},
    volume          = {},
    pages           = {},
    url             = {http://www.fit.vutbr.cz/research/groups/speech/servite/2010/rnnlm_mikolov.pdf}
}

@article{Hochreiter-Schmidhuber1997,
    author          = {Sepp Hochreiter and Jürgen Schmidhuber},
    year            = {1997},
    title           = {Long Short-Time Memory},
    journal         = {Neural Computation},
    volume          = {9},
    number          = {8},
    pages           = {1735-1780},
    url             = {https://direct.mit.edu/neco/article-abstract/9/8/1735/6109/Long-Short-Term-Memory?redirectedFrom=fulltext}
}

@inproceedings{Cho+2014,
    title = "Learning Phrase Representations using {RNN} Encoder{--}Decoder for Statistical Machine Translation",
    author = {Cho, Kyunghyun  and
      van Merri{\"e}nboer, Bart  and
      Gulcehre, Caglar  and
      Bahdanau, Dzmitry  and
      Bougares, Fethi  and
      Schwenk, Holger  and
      Bengio, Yoshua},
    editor = "Moschitti, Alessandro  and
      Pang, Bo  and
      Daelemans, Walter",
    booktitle = "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP})",
    month = oct,
    year = "2014",
    address = "Doha, Qatar",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D14-1179",
    doi = "10.3115/v1/D14-1179",
    pages = "1724--1734",
    url             = {https://aclanthology.org/D14-1179/},
}

@inproceedings{Dosovitskiy+2021,
    author={Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},
    year            = {2021},
    title           = {An Image is Worth 16×16 Words: Transformers for Image Recognition at Scale},
    booktitle       = {International Conference on Learning Representations},
    volume          = {},
    pages           = {},
    url             = {https://openreview.net/forum?id=YicbFdNTTy}
}

@inproceedings{Radford+2023,
author = {Radford, Alec and Kim, Jong Wook and Xu, Tao and Brockman, Greg and McLeavey, Christine and Sutskever, Ilya},
title = {Robust speech recognition via large-scale weak supervision},
year = {2023},
publisher = {JMLR.org},
abstract = {We study the capabilities of speech processing systems trained simply to predict large amounts of transcripts of audio on the internet. When scaled to 680,000 hours of multilingual and multitask supervision, the resulting models generalize well to standard benchmarks and are often competitive with prior fully supervised results without the need for any dataset specific fine-tuning. When compared to humans, the models approach their accuracy and robustness. We are releasing models and inference code to serve as a foundation for further work on robust speech processing.},
booktitle = {Proceedings of the 40th International Conference on Machine Learning},
articleno = {1182},
numpages = {27},
location = {Honolulu, Hawaii, USA},
series = {ICML'23},
url             = {https://dl.acm.org/doi/10.5555/3618408.3619590},
}

@inproceedings{Ioffe-Szegedy2015,
author = {Ioffe, Sergey and Szegedy, Christian},
title = {Batch normalization: accelerating deep network training by reducing internal covariate shift},
year = {2015},
publisher = {JMLR.org},
abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization, and in some cases eliminates the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.82\% top-5 test error, exceeding the accuracy of human raters.},
booktitle = {Proceedings of the 32nd International Conference on International Conference on Machine Learning - Volume 37},
pages = {448–456},
numpages = {9},
location = {Lille, France},
series = {ICML'15},
url             = {https://dl.acm.org/doi/10.5555/3045118.3045167},
}

@inproceedings{Bjorck+2018,
    author          = {Bjorck, Nils and Gomes, Carla P and Selman, Bart and Weinberger, Kilian Q},
    year            = {2018},
    title           = {Understanding Batch Normalization},
    booktitle       = {Advances in Neural Information Processing Systems},
    volume          = {31},
    pages           = {},
    url             = {https://papers.nips.cc/paper_files/paper/2018/hash/36072923bfc3cf47745d704feb489480-Abstract.html}
}

@inbook{LeCun+2012,
    author         = {Yann A. LeCun and Léon Bottou and Genevieve B. Orr and Klaus-Robert Müller},
    chapter        = {Efficient BackProp},
    editor         = {Grégoire Montavon and Geneviéve B. Orr and Klaus-Robert Müller},
    pages          = {9-48},
    publisher      = {Springer Berlin, Heidelberg},
    title          = {Neural Networks: Tricks of the Trade},
    year           = {2012},
    edition        = {2},
    url            = {https://link.springer.com/book/10.1007/978-3-642-35289-8},
}

@inproceedings{Lepikhin+2021,
      author       = {Dmitry Lepikhin and
                  HyoukJoong Lee and
                  Yuanzhong Xu and
                  Dehao Chen and
                  Orhan Firat and
                  Yanping Huang and
                  Maxim Krikun and
                  Noam Shazeer and
                  Zhifeng Chen},
    year            = {2021},
    title           = {GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding},
    booktitle       = {International Conference on Learning Representations},
    volume          = {},
    pages           = {},
    url             = {https://openreview.net/forum?id=qrwe7XHTmYb}
}

@article{Fedus+2022,
    author          = {William Fedus and Barret Zoph and Noam Shazeer},
    year            = {2022},
    title           = {Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity},
    journal         = {The Journal of Machine Learning Research},
    volume          = {23},
    number          = {120},
    pages           = {1-39},
    url             = {https://jmlr.org/papers/v23/21-0998.html}
}

@techreport{Rae+2021,
    author       = {Jack W. Rae and
                  Sebastian Borgeaud and
                  Trevor Cai and
                  Katie Millican and
                  Jordan Hoffmann and
                  H. Francis Song and
                  John Aslanides and
                  Sarah Henderson and
                  Roman Ring and
                  Susannah Young and
                  Eliza Rutherford and
                  Tom Hennigan and
                  Jacob Menick and
                  Albin Cassirer and
                  Richard Powell and
                  George van den Driessche and
                  Lisa Anne Hendricks and
                  Maribeth Rauh and
                  Po{-}Sen Huang and
                  Amelia Glaese and
                  Johannes Welbl and
                  Sumanth Dathathri and
                  Saffron Huang and
                  Jonathan Uesato and
                  John Mellor and
                  Irina Higgins and
                  Antonia Creswell and
                  Nat McAleese and
                  Amy Wu and
                  Erich Elsen and
                  Siddhant M. Jayakumar and
                  Elena Buchatskaya and
                  David Budden and
                  Esme Sutherland and
                  Karen Simonyan and
                  Michela Paganini and
                  Laurent Sifre and
                  Lena Martens and
                  Xiang Lorraine Li and
                  Adhiguna Kuncoro and
                  Aida Nematzadeh and
                  Elena Gribovskaya and
                  Domenic Donato and
                  Angeliki Lazaridou and
                  Arthur Mensch and
                  Jean{-}Baptiste Lespiau and
                  Maria Tsimpoukelli and
                  Nikolai Grigorev and
                  Doug Fritz and
                  Thibault Sottiaux and
                  Mantas Pajarskas and
                  Toby Pohlen and
                  Zhitao Gong and
                  Daniel Toyama and
                  Cyprien de Masson d'Autume and
                  Yujia Li and
                  Tayfun Terzi and
                  Vladimir Mikulik and
                  Igor Babuschkin and
                  Aidan Clark and
                  Diego de Las Casas and
                  Aurelia Guy and
                  Chris Jones and
                  James Bradbury and
                  Matthew J. Johnson and
                  Blake A. Hechtman and
                  Laura Weidinger and
                  Iason Gabriel and
                  William Isaac and
                  Edward Lockhart and
                  Simon Osindero and
                  Laura Rimell and
                  Chris Dyer and
                  Oriol Vinyals and
                  Kareem Ayoub and
                  Jeff Stanway and
                  Lorrayne Bennett and
                  Demis Hassabis and
                  Koray Kavukcuoglu and
                  Geoffrey Irving},
    institution = {Google DeepMind},
    title       = {Scaling Language Models: Methods, Analysis & Insights from Training Gopher},
    year        = {2021},
    url         = {https://arxiv.org/abs/2112.11446},
}

@techreport{Radford+2018,
    author      = {Alec Radford and Karthik narasimhan and Tim Salimans and Ilya Sutskever},
    institution = {OpenAI},
    title       = {Improving Language Understanding with Unsupervised Learning},
    year        = {2018},
    url         = {https://openai.com/research/language-unsupervised},
}

@article{vandenOord+2016,
    author          = {Aäron van{\ }den{\ }Oord and Nal Kalchbrenner and Koray Kavukcuoglu},
    year            = {2016},
    title           = {Pixel Recurrent Neural Networks},
    journal         = {Proceedings of the 33rd International Conference on Machine Learning},
    volume          = {},
    number          = {},
    pages           = {},
    url             = {https://proceedings.mlr.press/v48/oord16.html}
}

@inproceedings{vandenOord+2016b,
author = {A\"{a}ron van{\ }den{\ }Oord and Kalchbrenner, Nal and Vinyals, Oriol and Espeholt, Lasse and Graves, Alex and Kavukcuoglu, Koray},
title = {Conditional image generation with PixelCNN decoders},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {This work explores conditional image generation with a new image density model based on the PixelCNN architecture. The model can be conditioned on any vector, including descriptive labels or tags, or latent embeddings created by other networks. When conditioned on class labels from the ImageNet database, the model is able to generate diverse, realistic scenes representing distinct animals, objects, landscapes and structures. When conditioned on an embedding produced by a convolutional network given a single image of an unseen face, it generates a variety of new portraits of the same person with different facial expressions, poses and lighting conditions. We also show that conditional PixelCNN can serve as a powerful decoder in an image autoencoder. Additionally, the gated convolutional layers in the proposed model improve the log-likelihood of PixelCNN to match the state-of-the-art performance of PixelRNN on ImageNet, with greatly reduced computational cost.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {4797–4805},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16},
url             = {https://dl.acm.org/doi/10.5555/3157382.3157633},
}

@article{Bengio+2013,
author = {Bengio, Yoshua and Courville, Aaron and Vincent, Pascal},
title = {Representation Learning: A Review and New Perspectives},
year = {2013},
issue_date = {August 2013},
publisher = {IEEE Computer Society},
address = {USA},
volume = {35},
number = {8},
issn = {0162-8828},
url = {https://doi.org/10.1109/TPAMI.2013.50},
doi = {10.1109/TPAMI.2013.50},
abstract = {The success of machine learning algorithms generally depends on data representation, and we hypothesize that this is because different representations can entangle and hide more or less the different explanatory factors of variation behind the data. Although specific domain knowledge can be used to help design representations, learning with generic priors can also be used, and the quest for AI is motivating the design of more powerful representation-learning algorithms implementing such priors. This paper reviews recent work in the area of unsupervised feature learning and deep learning, covering advances in probabilistic models, autoencoders, manifold learning, and deep networks. This motivates longer term unanswered questions about the appropriate objectives for learning good representations, for computing representations (i.e., inference), and the geometrical connections between representation learning, density estimation, and manifold learning.},
journal = {IEEE Trans. Pattern Anal. Mach. Intell.},
month = {aug},
pages = {1798–1828},
numpages = {31},
keywords = {unsupervised learning, representation learning, neural nets, feature learning, autoencoder, Speech recognition, Neural networks, Manifolds, Machine learning, Learning systems, Feature extraction, Deep learning, Boltzmann machine, Abstracts}
}

@misc{Bengio+2013SSE,
      title={Estimating or Propagating Gradients Through Stochastic Neurons for Conditional Computation}, 
      author={Yoshua Bengio and Nicholas Léonard and Aaron Courville},
      year={2013},
      eprint={1308.3432},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1308.3432}, 
}


@inproceedings{Maddison+2014,
	author = {Maddison, Chris J and Tarlow, Daniel and Minka, Tom},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},
	publisher = {Curran Associates, Inc.},
	title = {A\ast Sampling},
	url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/309fee4e541e51de2e41f21bebb342aa-Paper.pdf},
	volume = {27},
	year = {2014},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2014/file/309fee4e541e51de2e41f21bebb342aa-Paper.pdf}}


@inproceedings{Chen+2020,
    author          = {Mark Chen and Alec Radford and Rewon Child and Jeffrey Wu and Heewoo Jun and David Luan and Ilya Sutskever},
    year            = {2020},
    title           = {Generative Pretraining from Pixels},
    booktitle       = {Proceedings of the 37th International Conference on Machine Learning},
    volume          = {},
    pages           = {},
    url             = {https://proceedings.mlr.press/v119/chen20s.html}
}

@misc{Rakhimov+2020,
      title={Latent Video Transformer}, 
      author={Ruslan Rakhimov and Denis Volkhonskiy and Alexey Artemov and Denis Zorin and Evgeny Burnaev},
      year={2020},
    url          = {https://arxiv.org/abs/2006.10704},
}

@unpublished{Yan+2021,
  author       = {Wilson Yan and
                  Yunzhi Zhang and
                  Pieter Abbeel and
                  Aravind Srinivas},
    year   = {2021},
    title  = {VideoGPT: Video Generation using VQ-VAE and Transformers},
    url    = {https://arxiv.org/abs/2104.10157}
}

@inproceedings{Micheli+2023,
    author          = {Vincent Micheli and Eloi Alonso and François Fleuret},
    year            = {2023},
    title           = {Transformers are Sample-Efficient World Models},
    booktitle       = {International Conference on Learning Representations},
    volume          = {},
    pages           = {},
    url             = {https://openreview.net/forum?id=vhFu1Acb0xb}
}

@inproceedings{Ha-Schmidthuber2018,
    author          = {David Ha and Jürgen Schmidhuber},
    year            = {2018},
    title           = {Recurrent World Models Facilitate Policy Evaluation},
    booktitle       = {Advances in Neural Information Processing Systems},
    volume          = {31},
    pages           = {},
    url             = {https://papers.nips.cc/paper_files/paper/2018/hash/2de5d16682c3c35007e4e92982f1a2ba-Abstract.html}
}

@inproceedings{Racaniere+2017,
author = {Racani\`{e}re, S\'{e}bastien and Weber, Th\'{e}ophane and Reichert, David P. and Buesing, Lars and Guez, Arthur and Rezende, Danilo and Badia, Adria Puigdom\`{e}nech and Vinyals, Oriol and Heess, Nicolas and Li, Yujia and Pascanu, Razvan and Battaglia, Peter and Hassabis, Demis and Silver, David and Wierstra, Daan},
title = {Imagination-augmented agents for deep reinforcement learning},
year = {2017},
isbn = {9781510860964},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We introduce Imagination-Augmented Agents (I2As), a novel architecture for deep reinforcement learning combining model-free and model-based aspects. In contrast to most existing model-based reinforcement learning and planning methods, which prescribe how a model should be used to arrive at a policy, I2As learn to interpret predictions from a learned environment model to construct implicit plans in arbitrary ways, by using the predictions as additional context in deep policy networks. I2As show improved data efficiency, performance, and robustness to model misspecification compared to several baselines.},
booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
pages = {5694–5705},
numpages = {12},
location = {Long Beach, California, USA},
series = {NIPS'17},
url             = {https://dl.acm.org/doi/10.5555/3295222.3295320},
}

@inproceedings{Kaiser+2020,
title={Model Based Reinforcement Learning for Atari},
author={Łukasz Kaiser and Mohammad Babaeizadeh and Piotr Miłos and Błażej Osiński and Roy H Campbell and Konrad Czechowski and Dumitru Erhan and Chelsea Finn and Piotr Kozakowski and Sergey Levine and Afroz Mohiuddin and Ryan Sepassi and George Tucker and Henryk Michalewski},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=S1xCPJHtDB}
}

@inproceedings{Hafner+2021,
title={Mastering Atari with Discrete World Models},
author={Danijar Hafner and Timothy P Lillicrap and Mohammad Norouzi and Jimmy Ba},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=0oabwyZbOu}
}

@article{Raffel+2020,
author = {Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J.},
title = {Exploring the limits of transfer learning with a unified text-to-text transformer},
year = {2020},
issue_date = {January 2020},
publisher = {JMLR.org},
volume = {21},
number = {1},
issn = {1532-4435},
abstract = {Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pretraining objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new "Colossal Clean Crawled Corpus", we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.},
journal = {J. Mach. Learn. Res.},
month = {jan},
articleno = {140},
numpages = {67},
keywords = {deep learning, attention based models, multi-task learning, natural language processing, transfer learning}
}


@inproceedings{Chang+2023,
  title = 	 {Muse: Text-To-Image Generation via Masked Generative Transformers},
  author =       {Chang, Huiwen and Zhang, Han and Barber, Jarred and Maschinot, Aaron and Lezama, Jose and Jiang, Lu and Yang, Ming-Hsuan and Murphy, Kevin Patrick and Freeman, William T. and Rubinstein, Michael and Li, Yuanzhen and Krishnan, Dilip},
  booktitle = 	 {Proceedings of the 40th International Conference on Machine Learning},
  pages = 	 {4055--4075},
  year = 	 {2023},
  editor = 	 {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  volume = 	 {202},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {23--29 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v202/chang23b/chang23b.pdf},
  url = 	 {https://proceedings.mlr.press/v202/chang23b.html},
  abstract = 	 {We present Muse, a text-to-image Transformermodel that achieves state-of-the-art image genera-tion performance while being significantly moreefficient than diffusion or autoregressive models.Muse is trained on a masked modeling task indiscrete token space: given the text embeddingextracted from a pre-trained large language model(LLM), Muse learns to predict randomly maskedimage tokens. Compared to pixel-space diffusionmodels, such as Imagen and DALL-E 2, Muse issignificantly more efficient due to the use of dis-crete tokens and requires fewer sampling itera-tions; compared to autoregressive models such asParti, Muse is more efficient due to the use of par-allel decoding. The use of a pre-trained LLM en-ables fine-grained language understanding, whichtranslates to high-fidelity image generation andthe understanding of visual concepts such as ob-jects, their spatial relationships, pose, cardinalityetc. Our 900M parameter model achieves a newSOTA on CC3M, with an FID score of 6.06. TheMuse 3B parameter model achieves an FID of7.88 on zero-shot COCO evaluation, along with aCLIP score of 0.32. Muse also directly enables anumber of image editing applications without theneed to fine-tune or invert the model: inpainting,outpainting, and mask-free editing. More resultsand videos demonstrating editing are available at https://muse-icml.github.io/}
}


@inproceedings{Ramesh+2021,
  title = 	 {Zero-Shot Text-to-Image Generation},
  author =       {Ramesh, Aditya and Pavlov, Mikhail and Goh, Gabriel and Gray, Scott and Voss, Chelsea and Radford, Alec and Chen, Mark and Sutskever, Ilya},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {8821--8831},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/ramesh21a/ramesh21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/ramesh21a.html},
  abstract = 	 {Text-to-image generation has traditionally focused on finding better modeling assumptions for training on a fixed dataset. These assumptions might involve complex architectures, auxiliary losses, or side information such as object part labels or segmentation masks supplied during training. We describe a simple approach for this task based on a transformer that autoregressively models the text and image tokens as a single stream of data. With sufficient data and scale, our approach is competitive with previous domain-specific models when evaluated in a zero-shot fashion.}
}

@misc{Dhariwal+2020,
      title={Jukebox: A Generative Model for Music}, 
      author={Prafulla Dhariwal and Heewoo Jun and Christine Payne and Jong Wook Kim and Alec Radford and Ilya Sutskever},
      year={2020},
      eprint={2005.00341},
      archivePrefix={arXiv},
      primaryClass={eess.AS},
      url          = {https://arxiv.org/abs/2005.00341},
}

@misc{Child+2019,
      title={Generating Long Sequences with Sparse Transformers}, 
      author={Rewon Child and Scott Gray and Alec Radford and Ilya Sutskever},
      year={2019},
      eprint={1904.10509},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url          = {https://arxiv.org/abs/1904.10509},
}

@inproceedings{Lewis+2020,
    author          = {Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and K\"{u}ttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rockt\"{a}schel, Tim and Riedel, Sebastian and Kiela, Douwe},
    year            = {2020},
    title           = {Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks},
    booktitle       = {Advances in Neural Information Processing Systems},
    volume          = {33},
    pages           = {9459-9474},
    url             = {https://proceedings.neurips.cc/paper/2020/hash/6b493230205f780e1bc26945df7481e5-Abstract.html}
}

@inproceedings{Petroni+2019,
    title = "Language Models as Knowledge Bases?",
    author = {Petroni, Fabio  and
      Rockt{\"a}schel, Tim  and
      Riedel, Sebastian  and
      Lewis, Patrick  and
      Bakhtin, Anton  and
      Wu, Yuxiang  and
      Miller, Alexander},
    editor = "Inui, Kentaro  and
      Jiang, Jing  and
      Ng, Vincent  and
      Wan, Xiaojun",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1250",
    doi = "10.18653/v1/D19-1250",
    pages = "2463--2473",
    abstract = "Recent progress in pretraining language models on large textual corpora led to a surge of improvements for downstream NLP tasks. Whilst learning linguistic knowledge, these models may also be storing relational knowledge present in the training data, and may be able to answer queries structured as {``}fill-in-the-blank{''} cloze statements. Language models have many advantages over structured knowledge bases: they require no schema engineering, allow practitioners to query about an open class of relations, are easy to extend to more data, and require no human supervision to train. We present an in-depth analysis of the relational knowledge already present (without fine-tuning) in a wide range of state-of-the-art pretrained language models. We find that (i) without fine-tuning, BERT contains relational knowledge competitive with traditional NLP methods that have some access to oracle knowledge, (ii) BERT also does remarkably well on open-domain question answering against a supervised baseline, and (iii) certain types of factual knowledge are learned much more readily than others by standard language model pretraining approaches. The surprisingly strong ability of these models to recall factual knowledge without any fine-tuning demonstrates their potential as unsupervised open-domain QA systems. The code to reproduce our analysis is available at \url{https://github.com/facebookresearch/LAMA}.",
}

@inproceedings{Roberts+2020,
    title = "How Much Knowledge Can You Pack Into the Parameters of a Language Model?",
    author = "Roberts, Adam  and
      Raffel, Colin  and
      Shazeer, Noam",
    editor = "Webber, Bonnie  and
      Cohn, Trevor  and
      He, Yulan  and
      Liu, Yang",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.437",
    doi = "10.18653/v1/2020.emnlp-main.437",
    pages = "5418--5426",
    abstract = "It has recently been observed that neural language models trained on unstructured text can implicitly store and retrieve knowledge using natural language queries. In this short paper, we measure the practical utility of this approach by fine-tuning pre-trained models to answer questions without access to any external context or knowledge. We show that this approach scales with model size and performs competitively with open-domain systems that explicitly retrieve answers from an external knowledge source when answering questions. To facilitate reproducibility and future work, we release our code and trained models.",
}

@misc{Marcus2020,
      title={The Next Decade in AI: Four Steps Towards Robust Artificial Intelligence}, 
      author={Gary Marcus},
      year={2020},
      eprint={2002.06177},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url          = {https://arxiv.org/abs/2002.06177},
}

@misc{Aghajanyan+2022,
      title={CM3: A Causal Masked Multimodal Model of the Internet}, 
      author={Armen Aghajanyan and Bernie Huang and Candace Ross and Vladimir Karpukhin and Hu Xu and Naman Goyal and Dmytro Okhonko and Mandar Joshi and Gargi Ghosh and Mike Lewis and Luke Zettlemoyer},
      year={2022},
      eprint={2201.07520},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url          = {https://arxiv.org/abs/2201.07520},
}

@misc{Alayrac+2022,
      title={Flamingo: a Visual Language Model for Few-Shot Learning}, 
      author={Jean-Baptiste Alayrac and Jeff Donahue and Pauline Luc and Antoine Miech and Iain Barr and Yana Hasson and Karel Lenc and Arthur Mensch and Katie Millican and Malcolm Reynolds and Roman Ring and Eliza Rutherford and Serkan Cabi and Tengda Han and Zhitao Gong and Sina Samangooei and Marianne Monteiro and Jacob Menick and Sebastian Borgeaud and Andrew Brock and Aida Nematzadeh and Sahand Sharifzadeh and Mikolaj Binkowski and Ricardo Barreira and Oriol Vinyals and Andrew Zisserman and Karen Simonyan},
      year={2022},
      eprint={2204.14198},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url          = {https://arxiv.org/abs/2204.14198},
}

@misc{Chowdhery+2022,
      title={PaLM: Scaling Language Modeling with Pathways}, 
      author={Aakanksha Chowdhery and Sharan Narang and Jacob Devlin and Maarten Bosma and Gaurav Mishra and Adam Roberts and Paul Barham and Hyung Won Chung and Charles Sutton and Sebastian Gehrmann and Parker Schuh and Kensen Shi and Sasha Tsvyashchenko and Joshua Maynez and Abhishek Rao and Parker Barnes and Yi Tay and Noam Shazeer and Vinodkumar Prabhakaran and Emily Reif and Nan Du and Ben Hutchinson and Reiner Pope and James Bradbury and Jacob Austin and Michael Isard and Guy Gur-Ari and Pengcheng Yin and Toju Duke and Anselm Levskaya and Sanjay Ghemawat and Sunipa Dev and Henryk Michalewski and Xavier Garcia and Vedant Misra and Kevin Robinson and Liam Fedus and Denny Zhou and Daphne Ippolito and David Luan and Hyeontaek Lim and Barret Zoph and Alexander Spiridonov and Ryan Sepassi and David Dohan and Shivani Agrawal and Mark Omernick and Andrew M. Dai and Thanumalayan Sankaranarayana Pillai and Marie Pellat and Aitor Lewkowycz and Erica Moreira and Rewon Child and Oleksandr Polozov and Katherine Lee and Zongwei Zhou and Xuezhi Wang and Brennan Saeta and Mark Diaz and Orhan Firat and Michele Catasta and Jason Wei and Kathy Meier-Hellstern and Douglas Eck and Jeff Dean and Slav Petrov and Noah Fiedel},
      year={2022},
      eprint={2204.02311},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url          = {https://arxiv.org/abs/2204.02311},
}

@article{Yu+2022,
title={Scaling Autoregressive Models for Content-Rich Text-to-Image Generation},
author={Jiahui Yu and Yuanzhong Xu and Jing Yu Koh and Thang Luong and Gunjan Baid and Zirui Wang and Vijay Vasudevan and Alexander Ku and Yinfei Yang and Burcu Karagol Ayan and Ben Hutchinson and Wei Han and Zarana Parekh and Xin Li and Han Zhang and Jason Baldridge and Yonghui Wu},
journal={Transactions on Machine Learning Research},
issn={2835-8856},
year={2022},
url={https://openreview.net/forum?id=AFDcYJKhND},
note={Featured Certification}
}

@inproceedings{Karpukhin+2020,
    title = "Dense Passage Retrieval for Open-Domain Question Answering",
    author = "Karpukhin, Vladimir  and
      Oguz, Barlas  and
      Min, Sewon  and
      Lewis, Patrick  and
      Wu, Ledell  and
      Edunov, Sergey  and
      Chen, Danqi  and
      Yih, Wen-tau",
    editor = "Webber, Bonnie  and
      Cohn, Trevor  and
      He, Yulan  and
      Liu, Yang",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.550",
    doi = "10.18653/v1/2020.emnlp-main.550",
    pages = "6769--6781",
    abstract = "Open-domain question answering relies on efficient passage retrieval to select candidate contexts, where traditional sparse vector space models, such as TF-IDF or BM25, are the de facto method. In this work, we show that retrieval can be practically implemented using dense representations alone, where embeddings are learned from a small number of questions and passages by a simple dual-encoder framework. When evaluated on a wide range of open-domain QA datasets, our dense retriever outperforms a strong Lucene-BM25 system greatly by 9{\%}-19{\%} absolute in terms of top-20 passage retrieval accuracy, and helps our end-to-end QA system establish new state-of-the-art on multiple open-domain QA benchmarks.",
}


@inproceedings{Radford+2021,
  title = 	 {Learning Transferable Visual Models From Natural Language Supervision},
  author =       {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {8748--8763},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/radford21a/radford21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/radford21a.html},
  abstract = 	 {State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on.}
}

@misc{Touvron+2023,
      title={LLaMA: Open and Efficient Foundation Language Models}, 
      author={Hugo Touvron and Thibaut Lavril and Gautier Izacard and Xavier Martinet and Marie-Anne Lachaux and Timothée Lacroix and Baptiste Rozière and Naman Goyal and Eric Hambro and Faisal Azhar and Aurelien Rodriguez and Armand Joulin and Edouard Grave and Guillaume Lample},
      year={2023},
      eprint={2302.13971},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url          = {https://arxiv.org/abs/2302.13971},
}

@inproceedings{Wang+2023-Self-Instruct,
    title = "Self-Instruct: Aligning Language Models with Self-Generated Instructions",
    author = "Wang, Yizhong  and
      Kordi, Yeganeh  and
      Mishra, Swaroop  and
      Liu, Alisa  and
      Smith, Noah A.  and
      Khashabi, Daniel  and
      Hajishirzi, Hannaneh",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.754",
    doi = "10.18653/v1/2023.acl-long.754",
    pages = "13484--13508",
    abstract = "Large {``}instruction-tuned{''} language models (i.e., finetuned to respond to instructions) have demonstrated a remarkable ability to generalize zero-shot to new tasks. Nevertheless, they depend heavily on human-written instruction data that is often limited in quantity, diversity, and creativity, therefore hindering the generality of the tuned model. We introduce Self-Instruct, a framework for improving the instruction-following capabilities of pretrained language models by bootstrapping off their own generations. Our pipeline generates instructions, input, and output samples from a language model, then filters invalid or similar ones before using them to finetune the original model. Applying our method to the vanilla GPT3, we demonstrate a 33{\%} absolute improvement over the original model on Super-NaturalInstructions, on par with the performance of InstructGPT-001, which was trained with private user data and human annotations. For further evaluation, we curate a set of expert-written instructions for novel tasks, and show through human evaluation that tuning GPT3 with Self-Instruct outperforms using existing public instruction datasets by a large margin, leaving only a 5{\%} absolute gap behind InstructGPT-001. Self-Instruct provides an almost annotation-free method for aligning pre-trained language models with instructions, and we release our large synthetic dataset to facilitate future studies on instruction tuning.",
}

@inproceedings{Lewis+2020-BART,
    title = "{BART}: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension",
    author = "Lewis, Mike  and
      Liu, Yinhan  and
      Goyal, Naman  and
      Ghazvininejad, Marjan  and
      Mohamed, Abdelrahman  and
      Levy, Omer  and
      Stoyanov, Veselin  and
      Zettlemoyer, Luke",
    editor = "Jurafsky, Dan  and
      Chai, Joyce  and
      Schluter, Natalie  and
      Tetreault, Joel",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.703",
    doi = "10.18653/v1/2020.acl-main.703",
    pages = "7871--7880",
    abstract = "We present BART, a denoising autoencoder for pretraining sequence-to-sequence models. BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and other recent pretraining schemes. We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of sentences and using a novel in-filling scheme, where spans of text are replaced with a single mask token. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa on GLUE and SQuAD, and achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 3.5 ROUGE. BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining. We also replicate other pretraining schemes within the BART framework, to understand their effect on end-task performance.",
}


@inproceedings{Yasunaga+2023,
  title = 	 {Retrieval-Augmented Multimodal Language Modeling},
  author =       {Yasunaga, Michihiro and Aghajanyan, Armen and Shi, Weijia and James, Richard and Leskovec, Jure and Liang, Percy and Lewis, Mike and Zettlemoyer, Luke and Yih, Wen-Tau},
  booktitle = 	 {Proceedings of the 40th International Conference on Machine Learning},
  pages = 	 {39755--39769},
  year = 	 {2023},
  editor = 	 {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  volume = 	 {202},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {23--29 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v202/yasunaga23a/yasunaga23a.pdf},
  url = 	 {https://proceedings.mlr.press/v202/yasunaga23a.html},
  abstract = 	 {Recent multimodal models such as DALL-E and CM3 have achieved remarkable progress in text-to-image and image-to-text generation. However, these models store all their knowledge (e.g., the appearance of the Eiffel Tower) in the model parameters, requiring increasingly larger models and training data to capture more knowledge. To integrate knowledge in a more scalable and modular way, we propose a retrieval-augmented multimodal model, which enables a base multimodal model (generator) to refer to relevant text and images fetched by a retriever from external memory (e.g., documents on the web). Specifically, for the retriever, we use a pretrained CLIP, and for the generator, we train a CM3 Transformer on the LAION dataset. Our resulting model, named Retrieval-Augmented CM3 (RA-CM3), is the first multimodal model that can retrieve and generate both text and images. We show that RA-CM3 significantly outperforms baseline multimodal models such as DALL-E and CM3 on both image and caption generation tasks (12 FID and 17 CIDEr improvements on MS-COCO), while requiring much less compute for training ($&lt;$30% of DALL-E). Moreover, we show that RA-CM3 exhibits novel capabilities such as faithful image generation and multimodal in-context learning (e.g., image generation from demonstrations).}
}

@misc{Hu+2023,
      title={GAIA-1: A Generative World Model for Autonomous Driving}, 
      author={Anthony Hu and Lloyd Russell and Hudson Yeo and Zak Murez and George Fedoseev and Alex Kendall and Jamie Shotton and Gianluca Corrado},
      year={2023},
      eprint={2309.17080},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url          = {https://arxiv.org/abs/2309.17080},
}

@misc{Wang+2023-VALL-E,
      title={Neural Codec Language Models are Zero-Shot Text to Speech Synthesizers}, 
      author={Chengyi Wang and Sanyuan Chen and Yu Wu and Ziqiang Zhang and Long Zhou and Shujie Liu and Zhuo Chen and Yanqing Liu and Huaming Wang and Jinyu Li and Lei He and Sheng Zhao and Furu Wei},
      year={2023},
      eprint={2301.02111},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url          = {https://arxiv.org/abs/2301.02111},
}

@misc{Yu+2023,
      title={Scaling Autoregressive Multi-Modal Models: Pretraining and Instruction Tuning}, 
      author={Lili Yu and Bowen Shi and Ramakanth Pasunuru and Benjamin Muller and Olga Golovneva and Tianlu Wang and Arun Babu and Binh Tang and Brian Karrer and Shelly Sheynin and Candace Ross and Adam Polyak and Russell Howes and Vasu Sharma and Puxin Xu and Hovhannes Tamoyan and Oron Ashual and Uriel Singer and Shang-Wen Li and Susan Zhang and Richard James and Gargi Ghosh and Yaniv Taigman and Maryam Fazel-Zarandi and Asli Celikyilmaz and Luke Zettlemoyer and Armen Aghajanyan},
      year={2023},
      eprint={2309.02591},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url          = {https://arxiv.org/abs/2309.02591},
}

@misc{Tamkin+2021,
      title={Understanding the Capabilities, Limitations, and Societal Impact of Large Language Models}, 
      author={Alex Tamkin and Miles Brundage and Jack Clark and Deep Ganguli},
      year={2021},
      eprint={2102.02503},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url          = {https://arxiv.org/abs/2102.02503},
}

@misc{Leng-Yuan2023,
      title={Do LLM Agents Exhibit Social Behavior?}, 
      author={Yan Leng and Yuan Yuan},
      year={2023},
      eprint={2312.15198},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url          = {https://arxiv.org/abs/2312.15198},
}

@misc{Nakano+2022,
      title={WebGPT: Browser-assisted question-answering with human feedback}, 
      author={Reiichiro Nakano and Jacob Hilton and Suchir Balaji and Jeff Wu and Long Ouyang and Christina Kim and Christopher Hesse and Shantanu Jain and Vineet Kosaraju and William Saunders and Xu Jiang and Karl Cobbe and Tyna Eloundou and Gretchen Krueger and Kevin Button and Matthew Knight and Benjamin Chess and John Schulman},
      year={2022},
      eprint={2112.09332},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url          = {https://arxiv.org/abs/2112.09332},
}

@inproceedings{Guu+2020,
author = {Guu, Kelvin and Lee, Kenton and Tung, Zora and Pasupat, Panupong and Chang, Ming-Wei},
title = {REALM: retrieval-augmented language model pre-training},
year = {2020},
publisher = {JMLR.org},
abstract = {Language model pre-training has been shown to capture a surprising amount of world knowledge, crucial for NLP tasks such as question answering. However, this knowledge is stored implicitly in the parameters of a neural network, requiring everlarger networks to cover more facts.To capture knowledge in a more modular and interpretable way, we augment language model pretraining with a latent knowledge retriever, which allows the model to retrieve and attend over documents from a large corpus such as Wikipedia, used during pre-training, fine-tuning and inference. For the first time, we show how to pre-train such a knowledge retriever in an unsupervised manner, using masked language modeling as the learning signal and backpropagating through a retrieval step that considers millions of documents.We demonstrate the effectiveness of Retrieval-Augmented Language Model pretraining (REALM) by fine-tuning on the challenging task of Open-domain Question Answering (Open-QA). We compare against state-of-the-art models for both explicit and implicit knowledge storage on three popular Open-QA benchmarks, and find that we outperform all previous methods by a significant margin (4-16\% absolute accuracy), while also providing qualitative benefits such as interpretability and modularity.},
booktitle = {Proceedings of the 37th International Conference on Machine Learning},
articleno = {368},
numpages = {10},
series = {ICML'20}
}

@inproceedings{Ouyang+2022,
	author = {Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and Schulman, John and Hilton, Jacob and Kelton, Fraser and Miller, Luke and Simens, Maddie and Askell, Amanda and Welinder, Peter and Christiano, Paul F and Leike, Jan and Lowe, Ryan},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
	pages = {27730--27744},
	publisher = {Curran Associates, Inc.},
	title = {Training language models to follow instructions with human feedback},
	url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/b1efde53be364a73914f58805a001731-Paper-Conference.pdf},
	volume = {35},
	year = {2022},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2022/file/b1efde53be364a73914f58805a001731-Paper-Conference.pdf}}


@misc{Schulman+2017,
      title={Proximal Policy Optimization Algorithms}, 
      author={John Schulman and Filip Wolski and Prafulla Dhariwal and Alec Radford and Oleg Klimov},
      year={2017},
      eprint={1707.06347},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url          = {https://arxiv.org/abs/1707.06347},
}

@misc{Thoppilan+2022,
      title={LaMDA: Language Models for Dialog Applications}, 
      author={Romal Thoppilan and Daniel De Freitas and Jamie Hall and Noam Shazeer and Apoorv Kulshreshtha and Heng-Tze Cheng and Alicia Jin and Taylor Bos and Leslie Baker and Yu Du and YaGuang Li and Hongrae Lee and Huaixiu Steven Zheng and Amin Ghafouri and Marcelo Menegali and Yanping Huang and Maxim Krikun and Dmitry Lepikhin and James Qin and Dehao Chen and Yuanzhong Xu and Zhifeng Chen and Adam Roberts and Maarten Bosma and Vincent Zhao and Yanqi Zhou and Chung-Ching Chang and Igor Krivokon and Will Rusch and Marc Pickett and Pranesh Srinivasan and Laichee Man and Kathleen Meier-Hellstern and Meredith Ringel Morris and Tulsee Doshi and Renelito Delos Santos and Toju Duke and Johnny Soraker and Ben Zevenbergen and Vinodkumar Prabhakaran and Mark Diaz and Ben Hutchinson and Kristen Olson and Alejandra Molina and Erin Hoffman-John and Josh Lee and Lora Aroyo and Ravi Rajakumar and Alena Butryna and Matthew Lamm and Viktoriya Kuzmina and Joe Fenton and Aaron Cohen and Rachel Bernstein and Ray Kurzweil and Blaise Aguera-Arcas and Claire Cui and Marian Croak and Ed Chi and Quoc Le},
      year={2022},
      eprint={2201.08239},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url          = {https://arxiv.org/abs/2201.08239},
}

@misc{Schulman+2015,
      title={Trust Region Policy Optimization}, 
      author={John Schulman and Sergey Levine and Philipp Moritz and Michael I. Jordan and Pieter Abbeel},
      year={2015},
      eprint={1502.05477},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url          = {https://arxiv.org/abs/1502.05477},
}

@misc{Zheng+2023,
      title={Secrets of RLHF in Large Language Models Part I: PPO}, 
      author={Rui Zheng and Shihan Dou and Songyang Gao and Yuan Hua and Wei Shen and Binghai Wang and Yan Liu and Senjie Jin and Qin Liu and Yuhao Zhou and Limao Xiong and Lu Chen and Zhiheng Xi and Nuo Xu and Wenbin Lai and Minghao Zhu and Cheng Chang and Zhangyue Yin and Rongxiang Weng and Wensen Cheng and Haoran Huang and Tianxiang Sun and Hang Yan and Tao Gui and Qi Zhang and Xipeng Qiu and Xuanjing Huang},
      year={2023},
      eprint={2307.04964},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url          = {https://arxiv.org/abs/2307.04964},
}

@inproceedings{Radford+2023-Whisper,
author = {Radford, Alec and Kim, Jong Wook and Xu, Tao and Brockman, Greg and McLeavey, Christine and Sutskever, Ilya},
title = {Robust speech recognition via large-scale weak supervision},
year = {2023},
publisher = {JMLR.org},
abstract = {We study the capabilities of speech processing systems trained simply to predict large amounts of transcripts of audio on the internet. When scaled to 680,000 hours of multilingual and multitask supervision, the resulting models generalize well to standard benchmarks and are often competitive with prior fully supervised results without the need for any dataset specific fine-tuning. When compared to humans, the models approach their accuracy and robustness. We are releasing models and inference code to serve as a foundation for further work on robust speech processing.},
booktitle = {Proceedings of the 40th International Conference on Machine Learning},
articleno = {1182},
numpages = {27},
location = {Honolulu, Hawaii, USA},
series = {ICML'23},
url             = {https://dl.acm.org/doi/10.5555/3618408.3619590},
}

@misc{Baker+2022,
      title={Video PreTraining (VPT): Learning to Act by Watching Unlabeled Online Videos}, 
      author={Bowen Baker and Ilge Akkaya and Peter Zhokhov and Joost Huizinga and Jie Tang and Adrien Ecoffet and Brandon Houghton and Raul Sampedro and Jeff Clune},
      year={2022},
      eprint={2206.11795},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url          = {https://arxiv.org/abs/2206.11795},
}

@misc{Ramesh+2022,
      title={Hierarchical Text-Conditional Image Generation with CLIP Latents}, 
      author={Aditya Ramesh and Prafulla Dhariwal and Alex Nichol and Casey Chu and Mark Chen},
      year={2022},
      eprint={2204.06125},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url          = {https://arxiv.org/abs/2204.06125},
}

@techreport{Manning+2022,
    author      = {Sam Manning and Pamela Mishkin and Gillian Hadfield and Tyna Eloundou and Emily Eisne},
    institution = {OpenAI},
    title       = {A Research Agenda for Assessing the Economic Impacts of Code Generation Models},
    year        = {2022},
    url         = {https://openai.com/research/economic-impacts},
}


@inproceedings{Dickstein+2015,
  title = 	 {{Deep Unsupervised Learning using Nonequilibrium Thermodynamics}},
  author = 	 {Sohl-Dickstein, Jascha and Weiss, Eric and Maheswaranathan, Niru and Ganguli, Surya},
  booktitle = 	 {Proceedings of the 32nd International Conference on Machine Learning},
  pages = 	 {2256--2265},
  year = 	 {2015},
  editor = 	 {Bach, Francis and Blei, David},
  volume = 	 {37},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Lille, France},
  month = 	 {07--09 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v37/sohl-dickstein15.pdf},
  url = 	 {https://proceedings.mlr.press/v37/sohl-dickstein15.html},
  abstract = 	 {A central problem in machine learning involves modeling complex data-sets using highly flexible families of probability distributions in which learning, sampling, inference, and evaluation are still analytically or computationally tractable. Here, we develop an approach that simultaneously achieves both flexibility and tractability. The essential idea, inspired by non-equilibrium statistical physics, is to systematically and slowly destroy structure in a data distribution through an iterative forward diffusion process. We then learn a reverse diffusion process that restores structure in data, yielding a highly flexible and tractable generative model of the data. This approach allows us to rapidly learn, sample from, and evaluate probabilities in deep generative models with thousands of layers or time steps, as well as to compute conditional and posterior probabilities under the learned model. We additionally release an open source reference implementation of the algorithm.}
}

@inproceedings{Song-Ermon2020,
    author          = {Yang Song and Stefano Ermon},
    year            = {2020},
    title           = {{Improved Techniques for Training Score-Based Generative Models}},
    booktitle       = {Advances in Neural Information Processing Systems},
    volume          = {33},
    pages           = {},
    url             = {https://proceedings.neurips.cc/paper/2020/hash/92c3b916311a5517d9290576e3ea37ad-Abstract.html}
}


@inproceedings{Song-Ermon2019,
	author = {Song, Yang and Ermon, Stefano},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
	publisher = {Curran Associates, Inc.},
	title = {{Generative Modeling by Estimating Gradients of the Data Distribution}},
	url = {https://proceedings.neurips.cc/paper_files/paper/2019/file/3001ef257407d5a371a96dcd947c7d93-Paper.pdf},
	volume = {32},
	year = {2019},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2019/file/3001ef257407d5a371a96dcd947c7d93-Paper.pdf}}


@inproceedings{Ho+2020,
    author          = {Jonathan Ho and Ajay Jain and Pieter Abbeel},
    year            = {2020},
    title           = {{Denoising Diffusion Probabilistic Models}},
    booktitle       = {Advances in Neural Information Processing Systems},
    volume          = {33},
    pages           = {},
    url             = {https://proceedings.neurips.cc/paper/2020/hash/4c5bcfec8584af0d967f1ab10179ca4b-Abstract.html}
}

@inproceedings{Aharoni+2019,
    title = "Massively Multilingual Neural Machine Translation",
    author = "Aharoni, Roee  and
      Johnson, Melvin  and
      Firat, Orhan",
    editor = "Burstein, Jill  and
      Doran, Christy  and
      Solorio, Thamar",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1388",
    doi = "10.18653/v1/N19-1388",
    pages = "3874--3884",
    abstract = "Multilingual Neural Machine Translation enables training a single model that supports translation from multiple source languages into multiple target languages. We perform extensive experiments in training massively multilingual NMT models, involving up to 103 distinct languages and 204 translation directions simultaneously. We explore different setups for training such models and analyze the trade-offs between translation quality and various modeling decisions. We report results on the publicly available TED talks multilingual corpus where we show that massively multilingual many-to-many models are effective in low resource settings, outperforming the previous state-of-the-art while supporting up to 59 languages in 116 translation directions in a single model. Our experiments on a large-scale dataset with 103 languages, 204 trained directions and up to one million examples per direction also show promising results, surpassing strong bilingual baselines and encouraging future work on massively multilingual NMT.",
}

@misc{Geminiteam+2023,
      title={Gemini: A Family of Highly Capable Multimodal Models}, 
      author={Gemini Team and Rohan Anil and Sebastian Borgeaud and Yonghui Wu and Jean-Baptiste Alayrac and Jiahui Yu and Radu Soricut and Johan Schalkwyk and Andrew M. Dai and Anja Hauth and Katie Millican and David Silver and Slav Petrov and Melvin Johnson and Ioannis Antonoglou and Julian Schrittwieser and Amelia Glaese and Jilin Chen and Emily Pitler and Timothy Lillicrap and Angeliki Lazaridou and Orhan Firat and James Molloy and Michael Isard and Paul R. Barham and Tom Hennigan and Benjamin Lee and Fabio Viola and Malcolm Reynolds and Yuanzhong Xu and Ryan Doherty and Eli Collins and Clemens Meyer and Eliza Rutherford and Erica Moreira and Kareem Ayoub and Megha Goel and George Tucker and Enrique Piqueras and Maxim Krikun and Iain Barr and Nikolay Savinov and Ivo Danihelka and Becca Roelofs and Anaïs White and Anders Andreassen and Tamara von Glehn and Lakshman Yagati and Mehran Kazemi and Lucas Gonzalez and Misha Khalman and Jakub Sygnowski and Alexandre Frechette and Charlotte Smith and Laura Culp and Lev Proleev and Yi Luan and Xi Chen and James Lottes and Nathan Schucher and Federico Lebron and Alban Rrustemi and Natalie Clay and Phil Crone and Tomas Kocisky and Jeffrey Zhao and Bartek Perz and Dian Yu and Heidi Howard and Adam Bloniarz and Jack W. Rae and Han Lu and Laurent Sifre and Marcello Maggioni and Fred Alcober and Dan Garrette and Megan Barnes and Shantanu Thakoor and Jacob Austin and Gabriel Barth-Maron and William Wong and Rishabh Joshi and Rahma Chaabouni and Deeni Fatiha and Arun Ahuja and Ruibo Liu and Yunxuan Li and Sarah Cogan and Jeremy Chen and Chao Jia and Chenjie Gu and Qiao Zhang and Jordan Grimstad and Ale Jakse Hartman and Martin Chadwick and Gaurav Singh Tomar and Xavier Garcia and Evan Senter and Emanuel Taropa and Thanumalayan Sankaranarayana Pillai and Jacob Devlin and Michael Laskin and Diego de Las Casas and Dasha Valter and Connie Tao and Lorenzo Blanco and Adrià Puigdomènech Badia and David Reitter and Mianna Chen and Jenny Brennan and Clara Rivera and Sergey Brin and Shariq Iqbal and Gabriela Surita and Jane Labanowski and Abhi Rao and Stephanie Winkler and Emilio Parisotto and Yiming Gu and Kate Olszewska and Yujing Zhang and Ravi Addanki and Antoine Miech and Annie Louis and Laurent El Shafey and Denis Teplyashin and Geoff Brown and Elliot Catt and Nithya Attaluri and Jan Balaguer and Jackie Xiang and Pidong Wang and Zoe Ashwood and Anton Briukhov and Albert Webson and Sanjay Ganapathy and Smit Sanghavi and Ajay Kannan and Ming-Wei Chang and Axel Stjerngren and Josip Djolonga and Yuting Sun and Ankur Bapna and Matthew Aitchison and Pedram Pejman and Henryk Michalewski and Tianhe Yu and Cindy Wang and Juliette Love and Junwhan Ahn and Dawn Bloxwich and Kehang Han and Peter Humphreys and Thibault Sellam and James Bradbury and Varun Godbole and Sina Samangooei and Bogdan Damoc and Alex Kaskasoli and Sébastien M. R. Arnold and Vijay Vasudevan and Shubham Agrawal and Jason Riesa and Dmitry Lepikhin and Richard Tanburn and Srivatsan Srinivasan and Hyeontaek Lim and Sarah Hodkinson and Pranav Shyam and Johan Ferret and Steven Hand and Ankush Garg and Tom Le Paine and Jian Li and Yujia Li and Minh Giang and Alexander Neitz and Zaheer Abbas and Sarah York and Machel Reid and Elizabeth Cole and Aakanksha Chowdhery and Dipanjan Das and Dominika Rogozińska and Vitaly Nikolaev and Pablo Sprechmann and Zachary Nado and Lukas Zilka and Flavien Prost and Luheng He and Marianne Monteiro and Gaurav Mishra and Chris Welty and Josh Newlan and Dawei Jia and Miltiadis Allamanis and Clara Huiyi Hu and Raoul de Liedekerke and Justin Gilmer and Carl Saroufim and Shruti Rijhwani and Shaobo Hou and Disha Shrivastava and Anirudh Baddepudi and Alex Goldin and Adnan Ozturel and Albin Cassirer and Yunhan Xu and Daniel Sohn and Devendra Sachan and Reinald Kim Amplayo and Craig Swanson and Dessie Petrova and Shashi Narayan and Arthur Guez and Siddhartha Brahma and Jessica Landon and Miteyan Patel and Ruizhe Zhao and Kevin Villela and Luyu Wang and Wenhao Jia and Matthew Rahtz and Mai Giménez and Legg Yeung and Hanzhao Lin and James Keeling and Petko Georgiev and Diana Mincu and Boxi Wu and Salem Haykal and Rachel Saputro and Kiran Vodrahalli and James Qin and Zeynep Cankara and Abhanshu Sharma and Nick Fernando and Will Hawkins and Behnam Neyshabur and Solomon Kim and Adrian Hutter and Priyanka Agrawal and Alex Castro-Ros and George van den Driessche and Tao Wang and Fan Yang and Shuo-yiin Chang and Paul Komarek and Ross McIlroy and Mario Lučić and Guodong Zhang and Wael Farhan and Michael Sharman and Paul Natsev and Paul Michel and Yong Cheng and Yamini Bansal and Siyuan Qiao and Kris Cao and Siamak Shakeri and Christina Butterfield and Justin Chung and Paul Kishan Rubenstein and Shivani Agrawal and Arthur Mensch and Kedar Soparkar and Karel Lenc and Timothy Chung and Aedan Pope and Loren Maggiore and Jackie Kay and Priya Jhakra and Shibo Wang and Joshua Maynez and Mary Phuong and Taylor Tobin and Andrea Tacchetti and Maja Trebacz and Kevin Robinson and Yash Katariya and Sebastian Riedel and Paige Bailey and Kefan Xiao and Nimesh Ghelani and Lora Aroyo and Ambrose Slone and Neil Houlsby and Xuehan Xiong and Zhen Yang and Elena Gribovskaya and Jonas Adler and Mateo Wirth and Lisa Lee and Music Li and Thais Kagohara and Jay Pavagadhi and Sophie Bridgers and Anna Bortsova and Sanjay Ghemawat and Zafarali Ahmed and Tianqi Liu and Richard Powell and Vijay Bolina and Mariko Iinuma and Polina Zablotskaia and James Besley and Da-Woon Chung and Timothy Dozat and Ramona Comanescu and Xiance Si and Jeremy Greer and Guolong Su and Martin Polacek and Raphaël Lopez Kaufman and Simon Tokumine and Hexiang Hu and Elena Buchatskaya and Yingjie Miao and Mohamed Elhawaty and Aditya Siddhant and Nenad Tomasev and Jinwei Xing and Christina Greer and Helen Miller and Shereen Ashraf and Aurko Roy and Zizhao Zhang and Ada Ma and Angelos Filos and Milos Besta and Rory Blevins and Ted Klimenko and Chih-Kuan Yeh and Soravit Changpinyo and Jiaqi Mu and Oscar Chang and Mantas Pajarskas and Carrie Muir and Vered Cohen and Charline Le Lan and Krishna Haridasan and Amit Marathe and Steven Hansen and Sholto Douglas and Rajkumar Samuel and Mingqiu Wang and Sophia Austin and Chang Lan and Jiepu Jiang and Justin Chiu and Jaime Alonso Lorenzo and Lars Lowe Sjösund and Sébastien Cevey and Zach Gleicher and Thi Avrahami and Anudhyan Boral and Hansa Srinivasan and Vittorio Selo and Rhys May and Konstantinos Aisopos and Léonard Hussenot and Livio Baldini Soares and Kate Baumli and Michael B. Chang and Adrià Recasens and Ben Caine and Alexander Pritzel and Filip Pavetic and Fabio Pardo and Anita Gergely and Justin Frye and Vinay Ramasesh and Dan Horgan and Kartikeya Badola and Nora Kassner and Subhrajit Roy and Ethan Dyer and Víctor Campos and Alex Tomala and Yunhao Tang and Dalia El Badawy and Elspeth White and Basil Mustafa and Oran Lang and Abhishek Jindal and Sharad Vikram and Zhitao Gong and Sergi Caelles and Ross Hemsley and Gregory Thornton and Fangxiaoyu Feng and Wojciech Stokowiec and Ce Zheng and Phoebe Thacker and Çağlar Ünlü and Zhishuai Zhang and Mohammad Saleh and James Svensson and Max Bileschi and Piyush Patil and Ankesh Anand and Roman Ring and Katerina Tsihlas and Arpi Vezer and Marco Selvi and Toby Shevlane and Mikel Rodriguez and Tom Kwiatkowski and Samira Daruki and Keran Rong and Allan Dafoe and Nicholas FitzGerald and Keren Gu-Lemberg and Mina Khan and Lisa Anne Hendricks and Marie Pellat and Vladimir Feinberg and James Cobon-Kerr and Tara Sainath and Maribeth Rauh and Sayed Hadi Hashemi and Richard Ives and Yana Hasson and YaGuang Li and Eric Noland and Yuan Cao and Nathan Byrd and Le Hou and Qingze Wang and Thibault Sottiaux and Michela Paganini and Jean-Baptiste Lespiau and Alexandre Moufarek and Samer Hassan and Kaushik Shivakumar and Joost van Amersfoort and Amol Mandhane and Pratik Joshi and Anirudh Goyal and Matthew Tung and Andrew Brock and Hannah Sheahan and Vedant Misra and Cheng Li and Nemanja Rakićević and Mostafa Dehghani and Fangyu Liu and Sid Mittal and Junhyuk Oh and Seb Noury and Eren Sezener and Fantine Huot and Matthew Lamm and Nicola De Cao and Charlie Chen and Gamaleldin Elsayed and Ed Chi and Mahdis Mahdieh and Ian Tenney and Nan Hua and Ivan Petrychenko and Patrick Kane and Dylan Scandinaro and Rishub Jain and Jonathan Uesato and Romina Datta and Adam Sadovsky and Oskar Bunyan and Dominik Rabiej and Shimu Wu and John Zhang and Gautam Vasudevan and Edouard Leurent and Mahmoud Alnahlawi and Ionut Georgescu and Nan Wei and Ivy Zheng and Betty Chan and Pam G Rabinovitch and Piotr Stanczyk and Ye Zhang and David Steiner and Subhajit Naskar and Michael Azzam and Matthew Johnson and Adam Paszke and Chung-Cheng Chiu and Jaume Sanchez Elias and Afroz Mohiuddin and Faizan Muhammad and Jin Miao and Andrew Lee and Nino Vieillard and Sahitya Potluri and Jane Park and Elnaz Davoodi and Jiageng Zhang and Jeff Stanway and Drew Garmon and Abhijit Karmarkar and Zhe Dong and Jong Lee and Aviral Kumar and Luowei Zhou and Jonathan Evens and William Isaac and Zhe Chen and Johnson Jia and Anselm Levskaya and Zhenkai Zhu and Chris Gorgolewski and Peter Grabowski and Yu Mao and Alberto Magni and Kaisheng Yao and Javier Snaider and Norman Casagrande and Paul Suganthan and Evan Palmer and Geoffrey Irving and Edward Loper and Manaal Faruqui and Isha Arkatkar and Nanxin Chen and Izhak Shafran and Michael Fink and Alfonso Castaño and Irene Giannoumis and Wooyeol Kim and Mikołaj Rybiński and Ashwin Sreevatsa and Jennifer Prendki and David Soergel and Adrian Goedeckemeyer and Willi Gierke and Mohsen Jafari and Meenu Gaba and Jeremy Wiesner and Diana Gage Wright and Yawen Wei and Harsha Vashisht and Yana Kulizhskaya and Jay Hoover and Maigo Le and Lu Li and Chimezie Iwuanyanwu and Lu Liu and Kevin Ramirez and Andrey Khorlin and Albert Cui and Tian LIN and Marin Georgiev and Marcus Wu and Ricardo Aguilar and Keith Pallo and Abhishek Chakladar and Alena Repina and Xihui Wu and Tom van der Weide and Priya Ponnapalli and Caroline Kaplan and Jiri Simsa and Shuangfeng Li and Olivier Dousse and Fan Yang and Jeff Piper and Nathan Ie and Minnie Lui and Rama Pasumarthi and Nathan Lintz and Anitha Vijayakumar and Lam Nguyen Thiet and Daniel Andor and Pedro Valenzuela and Cosmin Paduraru and Daiyi Peng and Katherine Lee and Shuyuan Zhang and Somer Greene and Duc Dung Nguyen and Paula Kurylowicz and Sarmishta Velury and Sebastian Krause and Cassidy Hardin and Lucas Dixon and Lili Janzer and Kiam Choo and Ziqiang Feng and Biao Zhang and Achintya Singhal and Tejasi Latkar and Mingyang Zhang and Quoc Le and Elena Allica Abellan and Dayou Du and Dan McKinnon and Natasha Antropova and Tolga Bolukbasi and Orgad Keller and David Reid and Daniel Finchelstein and Maria Abi Raad and Remi Crocker and Peter Hawkins and Robert Dadashi and Colin Gaffney and Sid Lall and Ken Franko and Egor Filonov and Anna Bulanova and Rémi Leblond and Vikas Yadav and Shirley Chung and Harry Askham and Luis C. Cobo and Kelvin Xu and Felix Fischer and Jun Xu and Christina Sorokin and Chris Alberti and Chu-Cheng Lin and Colin Evans and Hao Zhou and Alek Dimitriev and Hannah Forbes and Dylan Banarse and Zora Tung and Jeremiah Liu and Mark Omernick and Colton Bishop and Chintu Kumar and Rachel Sterneck and Ryan Foley and Rohan Jain and Swaroop Mishra and Jiawei Xia and Taylor Bos and Geoffrey Cideron and Ehsan Amid and Francesco Piccinno and Xingyu Wang and Praseem Banzal and Petru Gurita and Hila Noga and Premal Shah and Daniel J. Mankowitz and Alex Polozov and Nate Kushman and Victoria Krakovna and Sasha Brown and MohammadHossein Bateni and Dennis Duan and Vlad Firoiu and Meghana Thotakuri and Tom Natan and Anhad Mohananey and Matthieu Geist and Sidharth Mudgal and Sertan Girgin and Hui Li and Jiayu Ye and Ofir Roval and Reiko Tojo and Michael Kwong and James Lee-Thorp and Christopher Yew and Quan Yuan and Sumit Bagri and Danila Sinopalnikov and Sabela Ramos and John Mellor and Abhishek Sharma and Aliaksei Severyn and Jonathan Lai and Kathy Wu and Heng-Tze Cheng and David Miller and Nicolas Sonnerat and Denis Vnukov and Rory Greig and Jennifer Beattie and Emily Caveness and Libin Bai and Julian Eisenschlos and Alex Korchemniy and Tomy Tsai and Mimi Jasarevic and Weize Kong and Phuong Dao and Zeyu Zheng and Frederick Liu and Fan Yang and Rui Zhu and Mark Geller and Tian Huey Teh and Jason Sanmiya and Evgeny Gladchenko and Nejc Trdin and Andrei Sozanschi and Daniel Toyama and Evan Rosen and Sasan Tavakkol and Linting Xue and Chen Elkind and Oliver Woodman and John Carpenter and George Papamakarios and Rupert Kemp and Sushant Kafle and Tanya Grunina and Rishika Sinha and Alice Talbert and Abhimanyu Goyal and Diane Wu and Denese Owusu-Afriyie and Cosmo Du and Chloe Thornton and Jordi Pont-Tuset and Pradyumna Narayana and Jing Li and Sabaer Fatehi and John Wieting and Omar Ajmeri and Benigno Uria and Tao Zhu and Yeongil Ko and Laura Knight and Amélie Héliou and Ning Niu and Shane Gu and Chenxi Pang and Dustin Tran and Yeqing Li and Nir Levine and Ariel Stolovich and Norbert Kalb and Rebeca Santamaria-Fernandez and Sonam Goenka and Wenny Yustalim and Robin Strudel and Ali Elqursh and Balaji Lakshminarayanan and Charlie Deck and Shyam Upadhyay and Hyo Lee and Mike Dusenberry and Zonglin Li and Xuezhi Wang and Kyle Levin and Raphael Hoffmann and Dan Holtmann-Rice and Olivier Bachem and Summer Yue and Sho Arora and Eric Malmi and Daniil Mirylenka and Qijun Tan and Christy Koh and Soheil Hassas Yeganeh and Siim Põder and Steven Zheng and Francesco Pongetti and Mukarram Tariq and Yanhua Sun and Lucian Ionita and Mojtaba Seyedhosseini and Pouya Tafti and Ragha Kotikalapudi and Zhiyu Liu and Anmol Gulati and Jasmine Liu and Xinyu Ye and Bart Chrzaszcz and Lily Wang and Nikhil Sethi and Tianrun Li and Ben Brown and Shreya Singh and Wei Fan and Aaron Parisi and Joe Stanton and Chenkai Kuang and Vinod Koverkathu and Christopher A. Choquette-Choo and Yunjie Li and TJ Lu and Abe Ittycheriah and Prakash Shroff and Pei Sun and Mani Varadarajan and Sanaz Bahargam and Rob Willoughby and David Gaddy and Ishita Dasgupta and Guillaume Desjardins and Marco Cornero and Brona Robenek and Bhavishya Mittal and Ben Albrecht and Ashish Shenoy and Fedor Moiseev and Henrik Jacobsson and Alireza Ghaffarkhah and Morgane Rivière and Alanna Walton and Clément Crepy and Alicia Parrish and Yuan Liu and Zongwei Zhou and Clement Farabet and Carey Radebaugh and Praveen Srinivasan and Claudia van der Salm and Andreas Fidjeland and Salvatore Scellato and Eri Latorre-Chimoto and Hanna Klimczak-Plucińska and David Bridson and Dario de Cesare and Tom Hudson and Piermaria Mendolicchio and Lexi Walker and Alex Morris and Ivo Penchev and Matthew Mauger and Alexey Guseynov and Alison Reid and Seth Odoom and Lucia Loher and Victor Cotruta and Madhavi Yenugula and Dominik Grewe and Anastasia Petrushkina and Tom Duerig and Antonio Sanchez and Steve Yadlowsky and Amy Shen and Amir Globerson and Adam Kurzrok and Lynette Webb and Sahil Dua and Dong Li and Preethi Lahoti and Surya Bhupatiraju and Dan Hurt and Haroon Qureshi and Ananth Agarwal and Tomer Shani and Matan Eyal and Anuj Khare and Shreyas Rammohan Belle and Lei Wang and Chetan Tekur and Mihir Sanjay Kale and Jinliang Wei and Ruoxin Sang and Brennan Saeta and Tyler Liechty and Yi Sun and Yao Zhao and Stephan Lee and Pandu Nayak and Doug Fritz and Manish Reddy Vuyyuru and John Aslanides and Nidhi Vyas and Martin Wicke and Xiao Ma and Taylan Bilal and Evgenii Eltyshev and Daniel Balle and Nina Martin and Hardie Cate and James Manyika and Keyvan Amiri and Yelin Kim and Xi Xiong and Kai Kang and Florian Luisier and Nilesh Tripuraneni and David Madras and Mandy Guo and Austin Waters and Oliver Wang and Joshua Ainslie and Jason Baldridge and Han Zhang and Garima Pruthi and Jakob Bauer and Feng Yang and Riham Mansour and Jason Gelman and Yang Xu and George Polovets and Ji Liu and Honglong Cai and Warren Chen and XiangHai Sheng and Emily Xue and Sherjil Ozair and Adams Yu and Christof Angermueller and Xiaowei Li and Weiren Wang and Julia Wiesinger and Emmanouil Koukoumidis and Yuan Tian and Anand Iyer and Madhu Gurumurthy and Mark Goldenson and Parashar Shah and MK Blake and Hongkun Yu and Anthony Urbanowicz and Jennimaria Palomaki and Chrisantha Fernando and Kevin Brooks and Ken Durden and Harsh Mehta and Nikola Momchev and Elahe Rahimtoroghi and Maria Georgaki and Amit Raul and Sebastian Ruder and Morgan Redshaw and Jinhyuk Lee and Komal Jalan and Dinghua Li and Ginger Perng and Blake Hechtman and Parker Schuh and Milad Nasr and Mia Chen and Kieran Milan and Vladimir Mikulik and Trevor Strohman and Juliana Franco and Tim Green and Demis Hassabis and Koray Kavukcuoglu and Jeffrey Dean and Oriol Vinyals},
      year={2023},
      eprint={2312.11805},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url          = {https://arxiv.org/abs/2312.11805},
}

@inproceedings{Shazeer+2017,
      title={Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer}, 
      author={Noam Shazeer and Azalia Mirhoseini and Krzysztof Maziarz and Andy Davis and Quoc Le and Geoffrey Hinton and Jeff Dean},
      year={2017},
    booktitle       = {International Conference on Learning Representations},
    volume          = {},
    pages           = {},
    url             = {https://openreview.net/forum?id=B1ckMDqlg}
}


@inproceedings{Nichol+2022,
  title = 	 {{GLIDE}: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models},
  author =       {Nichol, Alexander Quinn and Dhariwal, Prafulla and Ramesh, Aditya and Shyam, Pranav and Mishkin, Pamela and Mcgrew, Bob and Sutskever, Ilya and Chen, Mark},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {16784--16804},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/nichol22a/nichol22a.pdf},
  url = 	 {https://proceedings.mlr.press/v162/nichol22a.html},
  abstract = 	 {Diffusion models have recently been shown to generate high-quality synthetic images, especially when paired with a guidance technique to trade off diversity for fidelity. We explore diffusion models for the problem of text-conditional image synthesis and compare two different guidance strategies: CLIP guidance and classifier-free guidance. We find that the latter is preferred by human evaluators for both photorealism and caption similarity, and often produces photorealistic samples. Samples from a 3.5&nbsp;billion parameter text-conditional diffusion model using classifier-free guidance are favored by human evaluators to those from DALL-E, even when the latter uses expensive CLIP reranking. Additionally, we find that our models can be fine-tuned to perform image inpainting, enabling powerful text-driven image editing. We train a smaller model on a filtered dataset and release the code and weights at https://github.com/openai/glide-text2im.}
}

@inproceedings{Lin+2022,
    author          = {Stephanie Lin and Jacob Hilton and Owain Evans},
    year            = {2022},
    title           = {Teaching Models to Express Their Uncertainty in Words},
    booktitle       = {Transactions on Machine Learning Research},
    volume          = {},
    pages           = {},
    url             = {https://openai.com/research/teaching-models-to-express-their-uncertainty-in-words}
}

@techreport{Weng-Brockman2022,
    author      = {Lilian Weng and Greg Brockman},
    institution = {OpenAI},
    title       = {Techniques for Training Large Neural Networks},
    url        = {https://openai.com/research/techniques-for-training-large-neural-networks},
    year       = {2022}
}

@techreport{Nichol2022,
    author      = {Alex Nichol},
    institution = {OpenAI},
    title       = {DALL-E2 Pre-training Mitigations},
    year        = {2022},
    url         = {https://openai.com/research/dall-e-2-pre-training-mitigations},
}

@inbook{Lehman+2024,
    author         = {Joel Lehman and Jonathan Gordon Shawn Jain and Kamal Ndousse and Cathy Yah and Kenneth O. Stanley},
    chapter        = {Evolution through Large Models},
    editor         = {Wolfgang Banzhaf and Penousal Machado and Mengjie Zhang},
    pages          = {331-366},
    publisher      = {Springer Singapore},
    title          = {Handbook of Evolutionary Machine Learning},
    year           = {2024},
    url            = {https://link.springer.com/chapter/10.1007/978-981-99-3814-8_11},
}

@misc{Saunders+2022,
      title={Self-critiquing models for assisting human evaluators}, 
      author={William Saunders and Catherine Yeh and Jeff Wu and Steven Bills and Long Ouyang and Jonathan Ward and Jan Leike},
      year={2022},
      eprint={2206.05802},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url          = {https://openai.com/research/critiques},
}

@misc{Khlaaf+2022,
      title={A Hazard Analysis Framework for Code Synthesis Large Language Models}, 
      author={Heidy Khlaaf and Pamela Mishkin and Joshua Achiam and Gretchen Krueger and Miles Brundage},
      year={2022},
      eprint={2207.14157},
      archivePrefix={arXiv},
      primaryClass={cs.SE},
      url          = {https://openai.com/research/a-hazard-analysis-framework-for-code-synthesis-large-language-models},
}

@misc{Goldstein+2023,
      title={Generative Language Models and Automated Influence Operations: Emerging Threats and Potential Mitigations}, 
      author={Josh A. Goldstein and Girish Sastry and Micah Musser and Renee DiResta and Matthew Gentzel and Katerina Sedova},
      year={2023},
      eprint={2301.04246},
      archivePrefix={arXiv},
      primaryClass={cs.CY},
      url          = {https://openai.com/research/forecasting-misuse},
}

@misc{Eloundou+2023,
      title={GPTs are GPTs: An Early Look at the Labor Market Impact Potential of Large Language Models}, 
      author={Tyna Eloundou and Sam Manning and Pamela Mishkin and Daniel Rock},
      year={2023},
      eprint={2303.10130},
      archivePrefix={arXiv},
      primaryClass={econ.GN},
      url          = {https://openai.com/research/gpts-are-gpts},
}

@misc{Chen+2021,
      title={Evaluating Large Language Models Trained on Code}, 
      author={Mark Chen and Jerry Tworek and Heewoo Jun and Qiming Yuan and Henrique Ponde de Oliveira Pinto and Jared Kaplan and Harri Edwards and Yuri Burda and Nicholas Joseph and Greg Brockman and Alex Ray and Raul Puri and Gretchen Krueger and Michael Petrov and Heidy Khlaaf and Girish Sastry and Pamela Mishkin and Brooke Chan and Scott Gray and Nick Ryder and Mikhail Pavlov and Alethea Power and Lukasz Kaiser and Mohammad Bavarian and Clemens Winter and Philippe Tillet and Felipe Petroski Such and Dave Cummings and Matthias Plappert and Fotios Chantzis and Elizabeth Barnes and Ariel Herbert-Voss and William Hebgen Guss and Alex Nichol and Alex Paino and Nikolas Tezak and Jie Tang and Igor Babuschkin and Suchir Balaji and Shantanu Jain and William Saunders and Christopher Hesse and Andrew N. Carr and Jan Leike and Josh Achiam and Vedant Misra and Evan Morikawa and Alec Radford and Matthew Knight and Miles Brundage and Mira Murati and Katie Mayer and Peter Welinder and Bob McGrew and Dario Amodei and Sam McCandlish and Ilya Sutskever and Wojciech Zaremba},
      year={2021},
      eprint={2107.03374},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url          = {https://arxiv.org/abs/2107.03374},
}

@techreport{Leike+2022,
    author      = {Jan Leike and John Schulman and Jeffrey Wu},
    institution = {OpenAI},
    title       = {Our Approach to Alignment Research},
    year        = {2022},
    url         = {https://openai.com/blog/our-approach-to-alignment-research},
}

@techreport{Leike+2023,
    author      = {Jan Leike and Jeffrey Wu and Steven Bills and William Saunders and Leo Gao and Henk Tillman and Daniel Mossing},
    institution = {OpenAI},
    title       = {Language Models Can Explain Neurons in Language Models},
    year        = {2023},
    url         = {https://openai.com/research/language-models-can-explain-neurons-in-language-models},
}

@misc{Anderljung+2023,
      title={Frontier AI Regulation: Managing Emerging Risks to Public Safety}, 
      author={Markus Anderljung and Joslyn Barnhart and Anton Korinek and Jade Leung and Cullen O'Keefe and Jess Whittlestone and Shahar Avin and Miles Brundage and Justin Bullock and Duncan Cass-Beggs and Ben Chang and Tantum Collins and Tim Fist and Gillian Hadfield and Alan Hayes and Lewis Ho and Sara Hooker and Eric Horvitz and Noam Kolt and Jonas Schuett and Yonadav Shavit and Divya Siddarth and Robert Trager and Kevin Wolf},
      year={2023},
      eprint={2307.03718},
      archivePrefix={arXiv},
      primaryClass={cs.CY},
      institution = {OpenAI},
      url          = {https://openai.com/research/frontier-ai-regulation},
}

@misc{Shoker+2023,
      title={Confidence-Building Measures for Artificial Intelligence: Workshop Proceedings}, 
      author={Sarah Shoker and Andrew Reddie and Sarah Barrington and Ruby Booth and Miles Brundage and Husanjot Chahal and Michael Depp and Bill Drexel and Ritwik Gupta and Marina Favaro and Jake Hecla and Alan Hickey and Margarita Konaev and Kirthi Kumar and Nathan Lambert and Andrew Lohn and Cullen O'Keefe and Nazneen Rajani and Michael Sellitto and Robert Trager and Leah Walker and Alexa Wehsener and Jessica Young},
      year={2023},
      eprint={2308.00862},
      archivePrefix={arXiv},
      primaryClass={cs.CY},
      url          = {https://openai.com/research/confidence-building-measures-for-artificial-intelligence},
}

@techreport{OpenAI2023-GPT4V,
    author      = {OpenAI},
    institution = {OpenAI},
    title       = {GPT-4V(ision) System Card},
    year        = {2023},
    url         = {https://openai.com/research/gpt-4v-system-card},
}

@misc{Yang+2023-GPT-4V,
      title={The Dawn of LMMs: Preliminary Explorations with GPT-4V(ision)}, 
      author={Zhengyuan Yang and Linjie Li and Kevin Lin and Jianfeng Wang and Chung-Ching Lin and Zicheng Liu and Lijuan Wang},
      year={2023},
      eprint={2309.17421},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url          = {https://arxiv.org/abs/2309.17421},
}

@misc{Perot+2023,
      title={LMDX: Language Model-based Document Information Extraction and Localization}, 
      author={Vincent Perot and Kai Kang and Florian Luisier and Guolong Su and Xiaoyu Sun and Ramya Sree Boppana and Zilong Wang and Jiaqi Mu and Hao Zhang and Nan Hua},
      year={2023},
      eprint={2309.10952},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url          = {https://arxiv.org/abs/2309.10952},
}

@techreport{OpenAI2023DallE3,
    author      = {OpenAI},
    institution = {OpenAI},
    title       = {DALL-E3 System Card},
    year        = {2023},
    url         = {https://openai.com/research/dall-e-3-system-card},
}

@techreport{Shavit+2023,
    author      = {Yonadav Shavit and Sandhini Agarwal and Miles Brundage},
    institution = {OpenAI},
    title       = {Practices for Governing Agentic AI Systems},
    year        = {2023},
    url         = {https://openai.com/research/practices-for-governing-agentic-ai-systems},
}

@misc{Burns+2023,
      title={Weak-to-Strong Generalization: Eliciting Strong Capabilities With Weak Supervision}, 
      author={Collin Burns and Pavel Izmailov and Jan Hendrik Kirchner and Bowen Baker and Leo Gao and Leopold Aschenbrenner and Yining Chen and Adrien Ecoffet and Manas Joglekar and Jan Leike and Ilya Sutskever and Jeff Wu},
      year={2023},
      eprint={2312.09390},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url          = {https://openai.com/research/weak-to-strong-generalization},
}

@techreport{Patwardhan+2024,
    author      = {Tejal Patwardhan and Kevin Liu and Todor Markov and Neil Chowdhury and Dillon Leet and Natalie Cone and Caitlin Maltbie and Joost Huizinga and Carroll Wainwright and Shawn (Froggi) Jackson and Steven Adler and Rocco Casagrande and Aleksander Mandry},
    institution = {OpenAI},
    title       = {Building an early warning system for LLM-aided biological threat creation},
    year        = {2024},
    url         = {https://openai.com/research/building-an-early-warning-system-for-llm-aided-biological-threat-creation},
}

@inproceedings{Fu+2023,
title={Hungry Hungry Hippos: Towards Language Modeling with State Space Models},
author={Daniel Y Fu and Tri Dao and Khaled Kamal Saab and Armin W Thomas and Atri Rudra and Christopher Re},
booktitle={The Eleventh International Conference on Learning Representations},
year={2023},
url={https://openreview.net/forum?id=COZDy0WYGg}
}

@inproceedings{Gu+2022,
title={Efficiently Modeling Long Sequences with Structured State Spaces},
author={Albert Gu and Karan Goel and Christopher Re},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=uYLFoz1vlAC}
}

@article{Zhou+2020,
	author = {Jie Zhou and Ganqu Cui and Shengding Hu and Zhengyan Zhang and Cheng Yang and Zhiyuan Liu and Lifeng Wang and Changcheng Li and Maosong Sun},
	journal = {AI Open},
	pages = {57-81},
	title = {Graph neural networks: A review of methods and applications},
	volume = {1},
	year = {2020},
    url             = {https://www.sciencedirect.com/science/article/pii/S2666651021000012},
}

@article{Wu+2021,
   title={A Comprehensive Survey on Graph Neural Networks},
   volume={32},
   ISSN={2162-2388},
   url={http://dx.doi.org/10.1109/TNNLS.2020.2978386},
   DOI={10.1109/tnnls.2020.2978386},
   number={1},
   journal={IEEE Transactions on Neural Networks and Learning Systems},
   publisher={Institute of Electrical and Electronics Engineers (IEEE)},
   author={Wu, Zonghan and Pan, Shirui and Chen, Fengwen and Long, Guodong and Zhang, Chengqi and Yu, Philip S.},
   year={2021},
   month={jan}, pages={4–24} 
}

@article{Velickovic2023,
	author = {Petar Veli{\v c}kovi{\'c}},
	journal = {Current Opinion in Structural Biology},
	pages = {102538},
	title = {Everything is connected: Graph neural networks},
	volume = {79},
	year = {2023},
    url             = {https://www.sciencedirect.com/science/article/pii/S0959440X2300012X},
}

@inproceedings{Dhariwal-Nichol2021,
title={Diffusion Models Beat {GAN}s on Image Synthesis},
author={Prafulla Dhariwal and Alexander Quinn Nichol},
booktitle={Advances in Neural Information Processing Systems},
editor={A. Beygelzimer and Y. Dauphin and P. Liang and J. Wortman Vaughan},
year={2021},
url={https://openreview.net/forum?id=AAWuCvzaVt}
}

@misc{Luo2022,
      title={Understanding Diffusion Models: A Unified Perspective}, 
      author={Calvin Luo},
      year={2022},
      eprint={2208.11970},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url          = {https://arxiv.org/abs/2208.11970},
}

@article{Hyvarinen2005,
    author          = {Aapo Hyvärinen},
    year            = {2005},
    title           = {Estimation of Non-Normalized Statistical Models by Score Matching},
    journal         = {Journal of Machine Learning Research},
    volume          = {6},
    number          = {24},
    pages           = {695-709},
    url             = {https://jmlr.org/papers/v6/hyvarinen05a.html}
}

@inproceedings{Rombach+2022,
    author          = {Robin Rombach and Andreas Blattmann and Dominik Lorenz and Patrick Esser and Björn Ommer},
    year            = {2022},
    title           = {High-Resolution Image Systhesis with Latent Diffusion Models},
    booktitle       = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    volume          = {},
    pages           = {10684-10695},
    url             = {https://openaccess.thecvf.com/content/CVPR2022/html/Rombach_High-Resolution_Image_Synthesis_With_Latent_Diffusion_Models_CVPR_2022_paper.html}
}
@InProceedings{Esser2021,
    author    = {Esser, Patrick and Rombach, Robin and Ommer, Bjorn},
    title     = {Taming Transformers for High-Resolution Image Synthesis},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2021},
    pages     = {12873-12883}
}

@inproceedings{Ronneberger+2015,
	address = {Cham},
	author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
	booktitle = {Medical Image Computing and Computer-Assisted Intervention -- MICCAI 2015},
	editor = {Navab, Nassir and Hornegger, Joachim and Wells, William M. and Frangi, Alejandro F.},
	pages = {234--241},
	publisher = {Springer International Publishing},
	title = {U-Net: Convolutional Networks for Biomedical Image Segmentation},
	year = {2015},
    url             = {https://link.springer.com/chapter/10.1007/978-3-319-24574-4_28},
}


@article{Yang+2023-Diff,
	author = {Yang, Ruihan and Srivastava, Prakhar and Mandt, Stephan},
	journal = {Entropy},
	number = {10},
	title = {Diffusion Probabilistic Modeling for Video Generation},
	volume = {25},
	year = {2023},
    url             = {https://www.mdpi.com/1099-4300/25/10/1469},
}

@article{Kobyzev+2021,
author = {I. Kobyzev and S. D. Prince and M. A. Brubaker},
journal = {IEEE Transactions on Pattern Analysis &amp; Machine Intelligence},
title = {Normalizing Flows: An Introduction and Review of Current Methods},
year = {2021},
volume = {43},
number = {11},
issn = {1939-3539},
pages = {3964-3979},
abstract = {Normalizing Flows are generative models which produce tractable distributions where both sampling and density evaluation can be efficient and exact. The goal of this survey article is to give a coherent and comprehensive review of the literature around the construction and use of Normalizing Flows for distribution learning. We aim to provide context and explanation of the models, review current state-of-the-art literature, and identify open questions and promising future directions.},
keywords = {estimation;jacobian matrices;mathematical model;training;computational modeling;context modeling;random variables},
doi = {10.1109/TPAMI.2020.2992934},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month = {nov},
url             = {https://www.computer.org/csdl/journal/tp/2021/11/09089305/1jDwlyVxAwE},
}

@article{Papamakarios+2021,
author = {Papamakarios, George and Nalisnick, Eric and Rezende, Danilo Jimenez and Mohamed, Shakir and Lakshminarayanan, Balaji},
title = {Normalizing flows for probabilistic modeling and inference},
year = {2021},
issue_date = {January 2021},
publisher = {JMLR.org},
volume = {22},
number = {1},
issn = {1532-4435},
abstract = {Normalizing flows provide a general mechanism for defining expressive probability distributions, only requiring the specification of a (usually simple) base distribution and a series of bijective transformations. There has been much recent work on normalizing flows, ranging from improving their expressive power to expanding their application. We believe the field has now matured and is in need of a unified perspective. In this review, we attempt to provide such a perspective by describing flows through the lens of probabilistic modeling and inference. We place special emphasis on the fundamental principles of flow design, and discuss foundational topics such as expressive power and computational trade-offs. We also broaden the conceptual framing of flows by relating them to more general probability transformations. Lastly, we summarize the use of flows for tasks such as generative modeling, approximate inference, and supervised learning.},
journal = {J. Mach. Learn. Res.},
month = {jan},
articleno = {57},
numpages = {64},
keywords = {generative models, probabilistic inference, probabilistic modeling, invertible neural networks, normalizing flows},
url             = {https://dl.acm.org/doi/abs/10.5555/3546258.3546315},
}

@misc{Brock+2019,
      title={Large Scale GAN Training for High Fidelity Natural Image Synthesis}, 
      author={Andrew Brock and Jeff Donahue and Karen Simonyan},
      year={2019},
      eprint={1809.11096},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url          = {https://arxiv.org/abs/1809.11096},
}

@inproceedings{Saharia+2022,
title={Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding},
author={Chitwan Saharia and William Chan and Saurabh Saxena and Lala Li and Jay Whang and Emily Denton and Seyed Kamyar Seyed Ghasemipour and Raphael Gontijo-Lopes and Burcu Karagol Ayan‎ and Tim Salimans and Jonathan Ho and David J. Fleet and Mohammad Norouzi},
booktitle={Advances in Neural Information Processing Systems},
editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
year={2022},
url={https://openreview.net/forum?id=08Yk-n5l2Al}
}


@inproceedings{Jaegle+2021,
  title = 	 {Perceiver: General Perception with Iterative Attention},
  author =       {Jaegle, Andrew and Gimeno, Felix and Brock, Andy and Vinyals, Oriol and Zisserman, Andrew and Carreira, Joao},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {4651--4664},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/jaegle21a/jaegle21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/jaegle21a.html},
  abstract = 	 {Biological systems understand the world by simultaneously processing high-dimensional inputs from modalities as diverse as vision, audition, touch, proprioception, etc. The perception models used in deep learning on the other hand are designed for individual modalities, often relying on domain-specific assumptions such as the local grid structures exploited by virtually all existing vision models. These priors introduce helpful inductive biases, but also lock models to individual modalities. In this paper we introduce the Perceiver {–} a model that builds upon Transformers and hence makes few architectural assumptions about the relationship between its inputs, but that also scales to hundreds of thousands of inputs, like ConvNets. The model leverages an asymmetric attention mechanism to iteratively distill inputs into a tight latent bottleneck, allowing it to scale to handle very large inputs. We show that this architecture is competitive with or outperforms strong, specialized models on classification tasks across various modalities: images, point clouds, audio, video and video+audio. The Perceiver obtains performance comparable to ResNet-50 and ViT on ImageNet without 2D convolutions by directly attending to 50,000 pixels. It is also competitive in all modalities in AudioSet.}
}

@inproceedings{Huang+2021,
title={{A Variational Perspective on Diffusion-Based Generative Models and Score Matching}},
author={Chin-Wei Huang and Jae Hyun Lim and Aaron Courville},
booktitle={Advances in Neural Information Processing Systems},
editor={A. Beygelzimer and Y. Dauphin and P. Liang and J. Wortman Vaughan},
year={2021},
url={https://openreview.net/forum?id=bXehDYUjjXi}
}

@article{Vincent2011,
    author          = {Pascal Vincent},
    year            = {2011},
    title           = {A Connection between Score Matching and Denoising Autoencoders},
    journal         = {Neural Computation},
    volume          = {23},
    number          = {7},
    pages           = {1661-1674},
    url             = {https://direct.mit.edu/neco/article/23/7/1661/7677/A-Connection-Between-Score-Matching-and-Denoising}
}

@inproceedings{Song+2021NeurIPS,
    author          = {Yang Song and Conor Durkan and Iain Murray and Stefano Ermon},
    year            = {2021},
    title           = {{Maximum Likelihood Training of Score-Based Diffusion Models}},
    booktitle       = {Advances in Neural Information Processing Systems},
    volume          = {34},
    pages           = {},
    url             = {https://proceedings.neurips.cc/paper/2021/hash/0a9fdbb17feb6ccb7ec405cfb85222c4-Abstract.html},
}

@inproceedings{Durkan+2019,
 author = {Durkan, Conor and Bekasov, Artur and Murray, Iain and Papamakarios, George},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Neural Spline Flows},
 url = {https://proceedings.neurips.cc/paper_files/paper/2019/file/7ac71d433f282034e088473244df8c02-Paper.pdf},
 volume = {32},
 year = {2019}
}


@misc{Tzen-Raginsky2019,
      title={{Neural Stochastic Differential Equations: Deep Latent Gaussian Models in the Diffusion Limit}}, 
      author={Belinda Tzen and Maxim Raginsky},
      year={2019},
      eprint={1905.09883},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url          = {https://arxiv.org/abs/1905.09883},
}

@inproceedings{Song+2021ICLR,
title={{Score-Based Generative Modeling through Stochastic Differential Equations}},
author={Yang Song and Jascha Sohl-Dickstein and Diederik P Kingma and Abhishek Kumar and Stefano Ermon and Ben Poole},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=PxTIG12RRHS}
}

@inproceedings{Jang+2017,
title={Categorical Reparameterization with Gumbel-Softmax},
author={Eric Jang and Shixiang Gu and Ben Poole},
booktitle={International Conference on Learning Representations},
year={2017},
url={https://openreview.net/forum?id=rkE3y85ee}
}

@inproceedings{Maddison+2017,
title={The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables},
author={Chris J. Maddison and Andriy Mnih and Yee Whye Teh},
booktitle={International Conference on Learning Representations},
year={2017},
url={https://openreview.net/forum?id=S1jE5L5gl}
}


@inproceedings{Reed+2016,
  title = 	 {Generative Adversarial Text to Image Synthesis},
  author = 	 {Reed, Scott and Akata, Zeynep and Yan, Xinchen and Logeswaran, Lajanugen and Schiele, Bernt and Lee, Honglak},
  booktitle = 	 {Proceedings of The 33rd International Conference on Machine Learning},
  pages = 	 {1060--1069},
  year = 	 {2016},
  editor = 	 {Balcan, Maria Florina and Weinberger, Kilian Q.},
  volume = 	 {48},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {New York, New York, USA},
  month = 	 {20--22 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v48/reed16.pdf},
  url = 	 {https://proceedings.mlr.press/v48/reed16.html},
  abstract = 	 {Automatic synthesis of realistic images from text would be interesting and useful, but current AI systems are still far from this goal. However, in recent years generic and powerful recurrent neural network architectures have been developed to learn discriminative text feature representations. Meanwhile, deep convolutional generative adversarial networks (GANs) have begun to generate highly compelling images of specific categories such as faces, album covers, room interiors and flowers. In this work, we develop a novel deep architecture and GAN formulation to effectively bridge these advances in text and image modeling, translating visual concepts from characters to pixels. We demonstrate the capability of our model to generate plausible images of birds and flowers from detailed text descriptions.}
}

@inproceedings{Peebles-Xie2023,
    author    = {Peebles, William and Xie, Saining},
    title     = {Scalable Diffusion Models with Transformers},
    booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
    month     = {October},
    year      = {2023},
    pages     = {4195-4205},
    url             = {https://openaccess.thecvf.com/content/ICCV2023/html/Peebles_Scalable_Diffusion_Models_with_Transformers_ICCV_2023_paper.html},
}

@inproceedings{Arnab+2021,
  author={Arnab, Anurag and Dehghani, Mostafa and Heigold, Georg and Sun, Chen and Lučić, Mario and Schmid, Cordelia},
  booktitle={2021 IEEE/CVF International Conference on Computer Vision (ICCV)}, 
  title={ViViT: A Video Vision Transformer}, 
  year={2021},
  volume={},
  number={},
  pages={6816-6826},
  keywords={Training;Computer vision;Three-dimensional displays;Benchmark testing;Transformers;Spatiotemporal phenomena;Kinetic theory;Video analysis and understanding;Action and behavior recognition},
  doi={10.1109/ICCV48922.2021.00676},
  url             = {https://ieeexplore.ieee.org/abstract/document/9710415},
}

@inproceedings{Dehghani+2023,
title={Patch n{\textquoteright} Pack: NaViT, a Vision Transformer for any Aspect Ratio and Resolution},
author={Mostafa Dehghani and Basil Mustafa and Josip Djolonga and Jonathan Heek and Matthias Minderer and Mathilde Caron and Andreas Peter Steiner and Joan Puigcerver and Robert Geirhos and Ibrahim Alabdulmohsin and Avital Oliver and Piotr Padlewski and Alexey A. Gritsenko and Mario Lucic and Neil Houlsby},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023},
url={https://openreview.net/forum?id=VpGFHmI7e5}
}

@misc{Ma+2024,
      title={SiT: Exploring Flow and Diffusion-based Generative Models with Scalable Interpolant Transformers}, 
      author={Nanye Ma and Mark Goldstein and Michael S. Albergo and Nicholas M. Boffi and Eric Vanden-Eijnden and Saining Xie},
      year={2024},
      eprint={2401.08740},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url          = {https://arxiv.org/abs/2401.08740},
}

@inproceedings{Albergo-Vanden-Eijnden2023,
title={{Building Normalizing Flows with Stochastic Interpolants}},
author={Michael Samuel Albergo and Eric Vanden-Eijnden},
booktitle={The Eleventh International Conference on Learning Representations},
year={2023},
url={https://openreview.net/forum?id=li7qeBbCR1t}
}

@misc{Albergo+2023,
      title={{Stochastic Interpolants: A Unifying Framework for Flows and Diffusions}}, 
      author={Michael S. Albergo and Nicholas M. Boffi and Eric Vanden-Eijnden},
      year={2023},
      eprint={2303.08797},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url          = {https://arxiv.org/abs/2303.08797},
}

@inproceedings{Lipman+2023,
title={Flow Matching for Generative Modeling},
author={Yaron Lipman and Ricky T. Q. Chen and Heli Ben-Hamu and Maximilian Nickel and Matthew Le},
booktitle={The Eleventh International Conference on Learning Representations},
year={2023},
url={https://openreview.net/forum?id=PqvMRDCJT9t}
}

@inproceedings{Liu+2023-Flow,
title={Flow Straight and Fast: Learning to Generate and Transfer Data with Rectified Flow},
author={Xingchao Liu and Chengyue Gong and qiang liu},
booktitle={The Eleventh International Conference on Learning Representations},
year={2023},
url={https://openreview.net/forum?id=XVjTT1nw5z}
}

@book{伊藤-加藤2005,
    author         = {伊藤正己 and 加藤一郎},
    year           = {2005},
    month          = {3},
    title          = {現代法学入門},
    series         = {有斐閣双書},
    volume         = {},
    edition        = {4},
    url            = {https://www.yuhikaku.co.jp/books/detail/4641112568},
    publisher      = {有斐閣}
}

@book{渡辺洋三1993,
    author         = {渡辺洋三},
    year           = {1993},
    month          = {6},
    title          = {法の常識},
    series         = {有斐閣双書},
    volume         = {},
    edition        = {3},
    url            = {https://www.yuhikaku.co.jp/books/detail/4641111014},
    publisher      = {有斐閣}
}

@inbook{実質的違法論について2004,
    author         = {厚生労働省},
    chapter        = {実質的違法論について},
    editor         = {},
    pages          = {},
    publisher      = {},
    title          = {在宅及び養護学校における日常的な医療の医学的・法律学的整理に関する研究会（第１回）},
    year           = {2004},
    url            = {https://www.mhlw.go.jp/shingi/2004/05/s0531-11b4.html},
}

@article{Bezanson+2017,
    author          = {Jeff Bezanson and Alan Edelman and Stefan Karpinski and Viral B. Shah},
    year            = {2017},
    title           = {Julia: A Fresh Approach to Numerical Computing},
    journal         = {SIAM Review},
    volume          = {59},
    number          = {1},
    pages           = {65-98},
    url             = {https://epubs.siam.org/doi/10.1137/141000671}
}

@inproceedings{Simonyan-Zisserman2015,
    author          = {Simonyan, K., and Zisserman, A.},
    year            = {2015},
    title           = {Very deep convolutional networks for large-scale image recognition},
    booktitle       = {International Conference on Learning Representations},
    volume          = {3},
    pages           = {1-14},
    url             = {https://ora.ox.ac.uk/objects/uuid:60713f18-a6d1-4d97-8f45-b60ad8aebbce}
}


@article{Broderick+2023,
	abstract = {Probabilistic machine learning increasingly informs critical decisions in medicine, economics, politics, and beyond. To aid the development of trust in these decisions, we develop a taxonomy delineating where trust in an analysis can break down: (i) in the translation of real-world goals to goals on a particular set of training data, (ii) in the translation of abstract goals on the training data to a concrete mathematical problem, (iii) in the use of an algorithm to solve the stated mathematical problem, and (iv) in the use of a particular code implementation of the chosen algorithm. We detail how trust can fail at each step and illustrate our taxonomy with two case studies. Finally, we describe a wide variety of methods that can be used to increase trust at each step of our taxonomy. The use of our taxonomy highlights not only steps where existing research work on trust tends to concentrate and but also steps where building trust is particularly challenging. A taxonomy delineates where trust can break down in a probabilistic\&amp;nbsp;machine learning workflow that informs critical decisions.},
	author = {Tamara Broderick and Andrew Gelman and Rachael Meager and Anna L. Smith and Tian Zheng},
	doi = {10.1126/sciadv.abn3999},
	eprint = {https://www.science.org/doi/pdf/10.1126/sciadv.abn3999},
	journal = {Science Advances},
	number = {7},
	pages = {eabn3999},
	title = {Toward a taxonomy of trust for probabilistic machine learning},
	url = {https://www.science.org/doi/abs/10.1126/sciadv.abn3999},
	volume = {9},
	year = {2023},
	bdsk-url-1 = {https://www.science.org/doi/abs/10.1126/sciadv.abn3999},
	bdsk-url-2 = {https://doi.org/10.1126/sciadv.abn3999}}


@inproceedings{Li+2018,
	author = {Li, Hao and Xu, Zheng and Taylor, Gavin and Studer, Christoph and Goldstein, Tom},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
	publisher = {Curran Associates, Inc.},
	title = {Visualizing the Loss Landscape of Neural Nets},
	url = {https://proceedings.neurips.cc/paper_files/paper/2018/file/a41b3bb3e6b050b6c9067c67f663b915-Paper.pdf},
	volume = {31},
	year = {2018},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2018/file/a41b3bb3e6b050b6c9067c67f663b915-Paper.pdf}}


@InProceedings{Balduzzi+2017,
  title = 	 {The Shattered Gradients Problem: If resnets are the answer, then what is the question?},
  author =       {David Balduzzi and Marcus Frean and Lennox Leary and J. P. Lewis and Kurt Wan-Duo Ma and Brian McWilliams},
  booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
  pages = 	 {342--350},
  year = 	 {2017},
  editor = 	 {Precup, Doina and Teh, Yee Whye},
  volume = 	 {70},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {06--11 Aug},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v70/balduzzi17b/balduzzi17b.pdf},
  url = 	 {https://proceedings.mlr.press/v70/balduzzi17b.html},
  abstract = 	 {A long-standing obstacle to progress in deep learning is the problem of vanishing and exploding gradients. Although, the problem has largely been overcome via carefully constructed initializations and batch normalization, architectures incorporating skip-connections such as highway and resnets perform much better than standard feedforward architectures despite well-chosen initialization and batch normalization. In this paper, we identify the shattered gradients problem. Specifically, we show that the correlation between gradients in standard feedforward networks decays exponentially with depth resulting in gradients that resemble white noise whereas, in contrast, the gradients in architectures with skip-connections are far more resistant to shattering, decaying sublinearly. Detailed empirical evidence is presented in support of the analysis, on both fully-connected networks and convnets. Finally, we present a new “looks linear” (LL) initialization that prevents shattering, with preliminary experiments showing the new initialization allows to train very deep networks without the addition of skip-connections.}
}

@article{Wolpert1996,
    author          = {David H. Wolpert},
    year            = {1996},
    title           = {The Lack of a Priori Distinctions between Learning Algorithms},
    journal         = {Neural Computation},
    volume          = {8},
    number          = {7},
    pages           = {1341-130},
    url             = {https://doi.org/10.1162/neco.1996.8.7.1341}
}

@misc{Bronstein+2021,
      title={Geometric Deep Learning: Grids, Groups, Graphs, Geodesics, and Gauges}, 
      author={Michael M. Bronstein and Joan Bruna and Taco Cohen and Petar Veličković},
      year={2021},
      eprint={2104.13478},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url          = {https://arxiv.org/abs/2104.13478},
}


@inproceedings{Simard+1991,
	author = {Simard, Patrice and Victorri, Bernard and LeCun, Yann and Denker, John},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {J. Moody and S. Hanson and R.P. Lippmann},
	publisher = {Morgan-Kaufmann},
	title = {Tangent Prop - A formalism for specifying selected invariances in an adaptive network},
	url = {https://proceedings.neurips.cc/paper_files/paper/1991/file/65658fde58ab3c2b6e5132a39fae7cb9-Paper.pdf},
	volume = {4},
	year = {1991},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/1991/file/65658fde58ab3c2b6e5132a39fae7cb9-Paper.pdf}}

@inproceedings{Zhou-Chellappa1988,
    author          = {Y. Zhou and R. Chellappa},
    year            = {1988},
    title           = {Computation of Optic Flow Using a Neural Network},
    booktitle       = {IEEE 1988 International Conference on Neural Networks},
    volume          = {},
    pages           = {},
    url             = {https://ieeexplore.ieee.org/document/23914}
}

@INPROCEEDINGS{Deng+2009,
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Kai Li and Li Fei-Fei},
  booktitle={2009 IEEE Conference on Computer Vision and Pattern Recognition}, 
  title={ImageNet: A large-scale hierarchical image database}, 
  year={2009},
  volume={},
  number={},
  pages={248-255},
  keywords={Large-scale systems;Image databases;Explosions;Internet;Robustness;Information retrieval;Image retrieval;Multimedia databases;Ontologies;Spine},
  doi={10.1109/CVPR.2009.5206848}}

@misc{Dodge-Karam2017,
      title={A Study and Comparison of Human and Deep Learning Recognition Performance Under Visual Distortions}, 
      author={Samuel Dodge and Lina Karam},
      year={2017},
      eprint={1705.02498},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url          = {https://arxiv.org/abs/1705.02498},
}

@INPROCEEDINGS{Long+2015,
author = {J. Long and E. Shelhamer and T. Darrell},
booktitle = {2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
title = {Fully convolutional networks for semantic segmentation},
year = {2015},
volume = {},
issn = {1063-6919},
pages = {3431-3440},
keywords = {},
doi = {10.1109/CVPR.2015.7298965},
url = {https://doi.ieeecomputersociety.org/10.1109/CVPR.2015.7298965},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month = {jun}
}

@INPROCEEDINGS {Noh+2015,
author = {H. Noh and S. Hong and B. Han},
booktitle = {2015 IEEE International Conference on Computer Vision (ICCV)},
title = {Learning Deconvolution Network for Semantic Segmentation},
year = {2015},
volume = {},
issn = {2380-7504},
pages = {1520-1528},
abstract = {We propose a novel semantic segmentation algorithm by learning a deep deconvolution network. We learn the network on top of the convolutional layers adopted from VGG 16-layer net. The deconvolution network is composed of deconvolution and unpooling layers, which identify pixelwise class labels and predict segmentation masks. We apply the trained network to each proposal in an input image, and construct the final semantic segmentation map by combining the results from all proposals in a simple manner. The proposed algorithm mitigates the limitations of the existing methods based on fully convolutional networks by integrating deep deconvolution network and proposal-wise prediction, our segmentation method typically identifies detailed structures and handles objects in multiple scales naturally. Our network demonstrates outstanding performance in PASCAL VOC 2012 dataset, and we achieve the best accuracy (72.5%) among the methods trained without using Microsoft COCO dataset through ensemble with the fully convolutional network.},
keywords = {deconvolution;semantics;image segmentation;visualization;feature extraction;shape;image reconstruction},
doi = {10.1109/ICCV.2015.178},
url = {https://doi.ieeecomputersociety.org/10.1109/ICCV.2015.178},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month = {dec}
}

@ARTICLE{Badrinarayanan+2017,
  author={Badrinarayanan, Vijay and Kendall, Alex and Cipolla, Roberto},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation}, 
  year={2017},
  volume={39},
  number={12},
  pages={2481-2495},
  keywords={Decoding;Neural networks;Training;Computer architecture;Image segmentation;Semantics;Convolutional codes;Deep convolutional neural networks;semantic pixel-wise segmentation;indoor scenes;road scenes;encoder;decoder;pooling;upsampling},
  doi={10.1109/TPAMI.2016.2644615},
  url             = {https://ieeexplore.ieee.org/document/7803544},}

@book{Clark2018,
    author         = {Michael Clark},
    year           = {2018},
    title          = {Graphical \& Latent Variable Modeling},
    series         = {},
    volume         = {},
    edition        = {},
    url            = {https://m-clark.github.io/sem/},
    publisher      = {}
}

@article{Joreskog70,
    author          = {Jöreskog, Karl Gustav},
    title           = {A general method for analysis of covariance structures},
    year            = {1970},
    journal         = {Biometrika},
    volume          = {57},
    number          = {2},
    pages           = {239-251},
    url             = {https://www.jstor.org/stable/2334833},
}


@article{Joreskog1969,
	abstract = {We describe a general procedure by which any number of parameters of the factor analytic model can be held fixed at any values and the remaining free parameters estimated by the maximum likelihood method. The generality of the approach makes it possible to deal with all kinds of solutions: orthogonal, oblique and various mixtures of these. By choosing the fixed parameters appropriately, factors can be defined to have desired properties and make subsequent rotation unnecessary. The goodness of fit of the maximum likelihood solution under the hypothesis represented by the fixed parameters is tested by a large samplex2 test based on the likelihood ratio technique. A by-product of the procedure is an estimate of the variance-covariance matrix of the estimated parameters. From this, approximate confidence intervals for the parameters can be obtained. Several examples illustrating the usefulness of the procedure are given.},
	author = {J{\"o}reskog, K.  G. },
	date = {1969/06/01},
	date-added = {2024-08-13 11:49:03 +0900},
	date-modified = {2024-08-13 11:49:03 +0900},
	doi = {10.1007/BF02289343},
	id = {J{\"o}reskog1969},
	isbn = {1860-0980},
	journal = {Psychometrika},
	number = {2},
	pages = {183--202},
	title = {A general approach to confirmatory maximum likelihood factor analysis},
	url = {https://doi.org/10.1007/BF02289343},
	volume = {34},
	year = {1969},
	bdsk-url-1 = {https://doi.org/10.1007/BF02289343}}


@article{Glivenko1933,
    author          = {Valery Ivanovich Glivenko},
    year            = {1933},
    title           = {Sulla determinazione empirica della leggi di probabilità},
    journal         = {Giornale dell'Instituo Italiano degli Attuari},
    volume          = {4},
    number          = {1},
    pages           = {92-99},
    url             = {}
}

@article{Cantelli1933,
    author          = {F. P. Cantelli},
    year            = {1933},
    title           = {Sulla determinazione empirica della leggi di probabilità},
    journal         = {Giornale dell'Instituo Italiano degli Attuari},
    volume          = {4},
    number          = {1},
    pages           = {421-424},
    url             = {}
}


@article{Shalev-Shwartz2010,
	author = {Shai Shalev-Shwartz and Ohad Shamir and Nathan Srebro and Karthik Sridharan},
	journal = {Journal of Machine Learning Research},
	number = {90},
	pages = {2635--2670},
	title = {Learnability, Stability and Uniform Convergence},
	url = {http://jmlr.org/papers/v11/shalev-shwartz10a.html},
	volume = {11},
	year = {2010},
	bdsk-url-1 = {http://jmlr.org/papers/v11/shalev-shwartz10a.html}}

@misc{Andreas2022,
      title={Language Models as Agent Models}, 
      author={Jacob Andreas},
      year={2022},
      eprint={2212.01681},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url          = {https://arxiv.org/abs/2212.01681},
}

@misc{Hashimoto2024,
    author       = {Tatsunori Hashimoto},
    howpublished = {Lecture at MLSS2024},
    title        = {Large Language Models},
    year         = {2024}
}

@misc{Kingma-Ba2017,
      title={Adam: A Method for Stochastic Optimization}, 
      author={Diederik P. Kingma and Jimmy Ba},
      year={2017},
      eprint={1412.6980},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url          = {https://arxiv.org/abs/1412.6980},
}


@article{Ackley+1985,
	abstract = {The computational power of massively parallel networks of simple processing elements resides in the communication bandwidth provided by the hardware connections between elements. These connections can allow a significant fraction of the knowledge of the system to be applied to an instance of a problem in a very short time. One kind of computation for which massively parallel networks appear to be well suited is large constraint satisfaction searches, but to use the connections efficiently two conditions must be met: First, a search technique that is suitable for parallel networks must be found. Second, there must be some way of choosing internal representations which allow the preexisting hardware connections to be used efficiently for encoding the constraints in the domain being searched. We describe a general parallel search method, based on statistical mechanics, and we show how it leads to a general learning rule for modifying the connection strengths so as to incorporate knowledge about a task domain in an efficient way. We describe some simple examples in which the learning algorithm creates internal representations that are demonstrably the most efficient way of using the preexisting connectivity structure.},
	author = {David H. Ackley and Geoffrey E. Hinton and Terrence J. Sejnowski},
	doi = {https://doi.org/10.1016/S0364-0213(85)80012-4},
	issn = {0364-0213},
	journal = {Cognitive Science},
	number = {1},
	pages = {147-169},
	title = {A learning algorithm for boltzmann machines},
	url = {https://www.sciencedirect.com/science/article/pii/S0364021385800124},
	volume = {9},
	year = {1985},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0364021385800124},
	bdsk-url-2 = {https://doi.org/10.1016/S0364-0213(85)80012-4}}

@misc{Baykal+2023,
      title={EdVAE: Mitigating Codebook Collapse with Evidential Discrete Variational Autoencoders}, 
      author={Gulcin Baykal and Melih Kandemir and Gozde Unal},
      year={2023},
      eprint={2310.05718},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url          = {https://arxiv.org/abs/2310.05718},
}


@inproceedings{Sensoy+2018,
	author = {Sensoy, Murat and Kaplan, Lance and Kandemir, Melih},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
	publisher = {Curran Associates, Inc.},
	title = {Evidential Deep Learning to Quantify Classification Uncertainty},
	url = {https://proceedings.neurips.cc/paper_files/paper/2018/file/a981f2b708044d6fb4a71a1463242520-Paper.pdf},
	volume = {31},
	year = {2018},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2018/file/a981f2b708044d6fb4a71a1463242520-Paper.pdf}}

@misc{Amini+2020,
      title={Deep Evidential Regression}, 
      author={Alexander Amini and Wilko Schwarting and Ava Soleimany and Daniela Rus},
      year={2020},
      eprint={1910.02600},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url          = {https://arxiv.org/abs/1910.02600},
}

@misc{Hestness+2017,
      title={Deep Learning Scaling is Predictable, Empirically}, 
      author={Joel Hestness and Sharan Narang and Newsha Ardalani and Gregory Diamos and Heewoo Jun and Hassan Kianinejad and Md. Mostofa Ali Patwary and Yang Yang and Yanqi Zhou},
      year={2017},
      eprint={1712.00409},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url          = {https://arxiv.org/abs/1712.00409},
}

@misc{Dai-Le2015,
      title={Semi-supervised Sequence Learning}, 
      author={Andrew M. Dai and Quoc V. Le},
      year={2015},
      eprint={1511.01432},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url          = {https://arxiv.org/abs/1511.01432},
}

@misc{Hoffmann+2022,
      title={Training Compute-Optimal Large Language Models}, 
      author={Jordan Hoffmann and Sebastian Borgeaud and Arthur Mensch and Elena Buchatskaya and Trevor Cai and Eliza Rutherford and Diego de Las Casas and Lisa Anne Hendricks and Johannes Welbl and Aidan Clark and Tom Hennigan and Eric Noland and Katie Millican and George van den Driessche and Bogdan Damoc and Aurelia Guy and Simon Osindero and Karen Simonyan and Erich Elsen and Jack W. Rae and Oriol Vinyals and Laurent Sifre},
      year={2022},
      eprint={2203.15556},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url          = {https://arxiv.org/abs/2203.15556},
}

@misc{Lieber+2021,
    author       = {Opher Lieber and Or Sharir and Barak Lenz and Yoav Shoham},
    note         = {White Paper},
    title        = {Jurrassic-1: Technical Details and Evaluation},
    year         = {2021},
    url          = {https://www.ai21.com/blog/announcing-ai21-studio-and-jurassic-1},
    institution = {AI21},
}

@misc{Mohri-Hashimoto2024,
      title={Language Models with Conformal Factuality Guarantees}, 
      author={Christopher Mohri and Tatsunori Hashimoto},
      year={2024},
      eprint={2402.10978},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url          = {https://arxiv.org/abs/2402.10978},
}

@misc{Kuditipudi+2023,
      title={Robust Distortion-free Watermarks for Language Models}, 
      author={Rohith Kuditipudi and John Thickstun and Tatsunori Hashimoto and Percy Liang},
      year={2023},
      eprint={2307.15593},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url          = {https://arxiv.org/abs/2402.10978},
}

@misc{Gu-Dao2024,
      title={Mamba: Linear-Time Sequence Modeling with Selective State Spaces}, 
      author={Albert Gu and Tri Dao},
      year={2024},
      eprint={2312.00752},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2312.00752}, 
}

@inproceedings{Smith+2023,
title={Simplified State Space Layers for Sequence Modeling},
author={Jimmy T.H. Smith and Andrew Warrington and Scott Linderman},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=Ai8Hw3AXqks}
}

@inproceedings{Garg+2022,
title={What Can Transformers Learn In-Context? A Case Study of Simple Function Classes},
author={Shivam Garg and Dimitris Tsipras and Percy Liang and Gregory Valiant},
booktitle={Advances in Neural Information Processing Systems},
editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
year={2022},
url={https://openreview.net/forum?id=flNZJ2eOet}
}

@inproceedings{vanOswald+2023,
title	= {Transformers learn in-context by gradient descent},author	= {Johannes von{\ }Oswald and Eyvind Niklasson and Ettore Randazzo and João Sacramento and Alexander Mordvintsev and Andrey Zhmoginov and Max Vladymyrov},year	= {2023},URL	= {https://research.google/pubs/transformers-learn-in-context-by-gradient-descent/},pages	= {35151--35174}}

@inproceedings{Akyurek+2023,
title={​​What learning algorithm is in-context learning? Investigations with linear models},
author={Ekin Aky{\"u}rek and Dale Schuurmans and Jacob Andreas and Tengyu Ma and Denny Zhou},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=0g0X4H8yN4I}
}

@inproceedings{Bai+2023,
title={Transformers as Statisticians: Provable In-Context Learning with In-Context Algorithm Selection},
author={Yu Bai and Fan Chen and Huan Wang and Caiming Xiong and Song Mei},
booktitle={Workshop on Efficient Systems for Foundation Models @ ICML2023},
year={2023},
url={https://openreview.net/forum?id=vlCG5HKEkI}
}

@inproceedings{Guo+2024,
title={How Do Transformers Learn In-Context Beyond Simple Functions? A Case Study on Learning with Representations},
author={Tianyu Guo and Wei Hu and Song Mei and Huan Wang and Caiming Xiong and Silvio Savarese and Yu Bai},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=ikwEDva1JZ}
}

@misc{Kim-Suzuki2024,
      title={Transformers Learn Nonlinear Features In Context: Nonconvex Mean-field Dynamics on the Attention Landscape}, 
      author={Juno Kim and Taiji Suzuki},
      year={2024},
      eprint={2402.01258},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url          = {https://arxiv.org/abs/2402.01258},
}


@InProceedings{Daniely+2011,
  title = 	 {Multiclass Learnability and the ERM principle},
  author = 	 {Daniely, Amit and Sabato, Sivan and Ben-David, Shai and Shalev-Shwartz, Shai},
  booktitle = 	 {Proceedings of the 24th Annual Conference on Learning Theory},
  pages = 	 {207--232},
  year = 	 {2011},
  editor = 	 {Kakade, Sham M. and von Luxburg, Ulrike},
  volume = 	 {19},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Budapest, Hungary},
  month = 	 {09--11 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v19/daniely11a/daniely11a.pdf},
  url = 	 {https://proceedings.mlr.press/v19/daniely11a.html},
  abstract = 	 {Multiclass learning is an area of growing practical relevance, for which the currently available theory is still far from providing satisfactory understanding.  We study the learnability of multiclass prediction, and derive upper and lower bounds on the sample complexity of multiclass hypothesis classes in different learning models: batch/online, realizable/unrealizable,full information/bandit feedback.  Our analysis reveals a surprising phenomenon: In the multiclass setting, in sharp contrast to binary classification, not all Empirical Risk Minimization (ERM) algorithms are equally successful. We show that there exist hypotheses classes for which some ERM learners have lower sample complexity than others. Furthermore, there are classes that are learnable by some ERM learners, while other ERM learner will fail to learn them. We propose a principle for designing good ERM learners, and use this principle to prove tight bounds on the sample complexity of learning symmetric multiclass hypothesis classes (that is, classes that are invariant under any permutation of label names). We demonstrate the relevance of the theory by analyzing the sample complexity of two widely used hypothesis classes: generalized linear multiclass models and reduction trees. We also obtain some practically relevant conclusions.}
}

@misc{Clerico+2023,
      title={Generalisation under gradient descent via deterministic PAC-Bayes}, 
      author={Eugenio Clerico and Tyler Farghly and George Deligiannidis and Benjamin Guedj and Arnaud Doucet},
      year={2023},
      eprint={2209.02525},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url          = {https://arxiv.org/abs/2209.02525},
}


@article{岡島義憲2020,
	author = {岡島義憲},
	doi = {10.11517/jsaisigtwo.2020.AGI-015_08},
	journal = {人工知能学会第二種研究会資料},
	number = {AGI-015},
	pages = {08},
	title = {Spiking Neural Network 技術の現状と課題に関する考察},
	volume = {2020},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.11517/jsaisigtwo.2020.AGI-015_08}}


@misc{Wang+2023-BitNet,
	abstract = {The increasing size of large language models has posed challenges for deployment and raised concerns about environmental impact due to high energy consumption. In this work, we introduce BitNet, a scalable and stable 1-bit Transformer architecture designed for large language models. Specifically, we introduce BitLinear as a drop-in replacement of the nn.Linear layer in order to train 1-bit weights from scratch. Experimental results on language modeling show that BitNet achieves competitive performance while substantially reducing memory footprint and energy consumption, compared to state-of-the-art 8-bit quantization methods and FP16 Transformer baselines. Furthermore, BitNet exhibits a scaling law akin to full-precision Transformers, suggesting its potential for effective scaling to even larger language models while maintaining efficiency and performance benefits.},
	author = {Wang, Hongyu and Ma, Shuming and Dong, Li and Huang, Shaohan and Wang, Huaijie and Ma, Lingxiao and Yang, Fan and Wang, Ruiping and Wu, Yi and Wei, Furu},
	howpublished = {arXiv},
	month = {October},
	title = {BitNet: Scaling 1-bit Transformers for Large Language Models},
	url = {https://www.microsoft.com/en-us/research/publication/bitnet-scaling-1-bit-transformers-for-large-language-models/},
	year = {2023},
	bdsk-url-1 = {https://www.microsoft.com/en-us/research/publication/bitnet-scaling-1-bit-transformers-for-large-language-models/}}


@article{Maass1997,
	abstract = {The computational power of formal models for networks of spiking neurons is compared with that of other neural network models based on McCulloch Pitts neurons (i.e., threshold gates), respectively, sigmoidal gates. In particular it is shown that networks of spiking neurons are, with regard to the number of neurons that are needed, computationally more powerful than these other neural network models. A concrete biologically relevant function is exhibited which can be computed by a single spiking neuron (for biologically reasonable values of its parameters), but which requires hundreds of hidden units on a sigmoidal neural net. On the other hand, it is known that any function that can be computed by a small sigmoidal neural net can also be computed by a small network of spiking neurons. This article does not assume prior knowledge about spiking neurons, and it contains an extensive list of references to the currently available literature on computations in networks of spiking neurons and relevant results from neurobiology.},
	author = {Wolfgang Maass},
	doi = {https://doi.org/10.1016/S0893-6080(97)00011-7},
	issn = {0893-6080},
	journal = {Neural Networks},
	keywords = {Spiking neuron, Integrate-and-fire neutron, Computational complexity, Sigmoidal neural nets, Lower bounds},
	number = {9},
	pages = {1659-1671},
	title = {Networks of spiking neurons: The third generation of neural network models},
	url = {https://www.sciencedirect.com/science/article/pii/S0893608097000117},
	volume = {10},
	year = {1997},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0893608097000117},
	bdsk-url-2 = {https://doi.org/10.1016/S0893-6080(97)00011-7}}


@article{Tavanaei+2019,
	abstract = {In recent years, deep learning has revolutionized the field of machine learning, for computer vision in particular. In this approach, a deep (multilayer) artificial neural network (ANN) is trained, most often in a supervised manner using backpropagation. Vast amounts of labeled training examples are required, but the resulting classification accuracy is truly impressive, sometimes outperforming humans. Neurons in an ANN are characterized by a single, static, continuous-valued activation. Yet biological neurons use discrete spikes to compute and transmit information, and the spike times, in addition to the spike rates, matter. Spiking neural networks (SNNs) are thus more biologically realistic than ANNs, and are arguably the only viable option if one wants to understand how the brain computes at the neuronal description level. The spikes of biological neurons are sparse in time and space, and event-driven. Combined with bio-plausible local learning rules, this makes it easier to build low-power, neuromorphic hardware for SNNs. However, training deep SNNs remains a challenge. Spiking neurons' transfer function is usually non-differentiable, which prevents using backpropagation. Here we review recent supervised and unsupervised methods to train deep SNNs, and compare them in terms of accuracy and computational cost. The emerging picture is that SNNs still lag behind ANNs in terms of accuracy, but the gap is decreasing, and can even vanish on some tasks, while SNNs typically require many fewer operations and are the better candidates to process spatio-temporal data.},
	author = {Amirhossein Tavanaei and Masoud Ghodrati and Saeed Reza Kheradpisheh and Timoth{\'e}e Masquelier and Anthony Maida},
	doi = {https://doi.org/10.1016/j.neunet.2018.12.002},
	issn = {0893-6080},
	journal = {Neural Networks},
	keywords = {Deep learning, Spiking neural network, Biological plausibility, Machine learning, Power-efficient architecture},
	pages = {47-63},
	title = {Deep learning in spiking neural networks},
	url = {https://www.sciencedirect.com/science/article/pii/S0893608018303332},
	volume = {111},
	year = {2019},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0893608018303332},
	bdsk-url-2 = {https://doi.org/10.1016/j.neunet.2018.12.002}}

@inproceedings{Ujvary+2023,
title={Estimating optimal {PAC}-Bayes bounds with Hamiltonian Monte Carlo},
author={Szilvia Ujv{\'a}ry and Gergely Flamich and Vincent Fortuin and Jos{\'e} Miguel Hern{\'a}ndez-Lobato},
booktitle={NeurIPS 2023 Workshop on Mathematics of Modern Machine Learning},
year={2023},
url={https://openreview.net/forum?id=6ZUH4KRM1o}
}

@misc{Dziugaite-Roy2017,
      title={Computing Nonvacuous Generalization Bounds for Deep (Stochastic) Neural Networks with Many More Parameters than Training Data}, 
      author={Gintare Karolina Dziugaite and Daniel M. Roy},
      year={2017},
      eprint={1703.11008},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url          = {https://arxiv.org/abs/1703.11008},
}

@misc{Jiang+2024,
      title={MegaScale: Scaling Large Language Model Training to More Than 10,000 GPUs}, 
      author={Ziheng Jiang and Haibin Lin and Yinmin Zhong and Qi Huang and Yangrui Chen and Zhi Zhang and Yanghua Peng and Xiang Li and Cong Xie and Shibiao Nong and Yulu Jia and Sun He and Hongmin Chen and Zhihao Bai and Qi Hou and Shipeng Yan and Ding Zhou and Yiyao Sheng and Zhuo Jiang and Haohan Xu and Haoran Wei and Zhang Zhang and Pengfei Nie and Leqi Zou and Sida Zhao and Liang Xiang and Zherui Liu and Zhe Li and Xiaoying Jia and Jianxi Ye and Xin Jin and Xin Liu},
      year={2024},
      eprint={2402.15627},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{gal2016bayesian,

      eprint={1506.02158},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@inproceedings{Gal-Ghahramani2016,
      title={Bayesian Convolutional Neural Networks with Bernoulli Approximate Variational Inference}, 
      author={Yarin Gal and Zoubin Ghahramani},
      year={2016},
    booktitle       = {International Conference on Learning Representations},
    volume          = {},
    pages           = {},
    url             = {https://openreview.net/forum?id=3QxqXoJEyfp7y9wltP11}
}


@InProceedings{Gal-Ghahramani2016-Dropout,
  title = 	 {Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning},
  author = 	 {Gal, Yarin and Ghahramani, Zoubin},
  booktitle = 	 {Proceedings of The 33rd International Conference on Machine Learning},
  pages = 	 {1050--1059},
  year = 	 {2016},
  editor = 	 {Balcan, Maria Florina and Weinberger, Kilian Q.},
  volume = 	 {48},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {New York, New York, USA},
  month = 	 {20--22 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v48/gal16.pdf},
  url = 	 {https://proceedings.mlr.press/v48/gal16.html},
  abstract = 	 {Deep learning tools have gained tremendous attention in applied machine learning. However such tools for regression and classification do not capture model uncertainty. In comparison, Bayesian models offer a mathematically grounded framework to reason about model uncertainty, but usually come with a prohibitive computational cost. In this paper we develop a new theoretical framework casting dropout training in deep neural networks (NNs) as approximate Bayesian inference in deep Gaussian processes. A direct result of this theory gives us tools to model uncertainty with dropout NNs – extracting information from existing models that has been thrown away so far. This mitigates the problem of representing uncertainty in deep learning without sacrificing either computational complexity or test accuracy. We perform an extensive study of the properties of dropout’s uncertainty. Various network architectures and non-linearities are assessed on tasks of regression and classification, using MNIST as an example. We show a considerable improvement in predictive log-likelihood and RMSE compared to existing state-of-the-art methods, and finish by using dropout’s uncertainty in deep reinforcement learning.}
}


@article{Krzywinski-Altman2013,
	abstract = {Statistics does not tell us whether we are right. It tells us the chances of being wrong.},
	author = {Krzywinski, Martin and Altman, Naomi},
	date = {2013/09/01},
	date-added = {2024-03-06 15:09:25 +0900},
	date-modified = {2024-03-06 15:09:25 +0900},
	doi = {10.1038/nmeth.2613},
	id = {Krzywinski2013},
	isbn = {1548-7105},
	journal = {Nature Methods},
	number = {9},
	pages = {809--810},
	title = {Importance of being uncertain},
	url = {https://doi.org/10.1038/nmeth.2613},
	volume = {10},
	year = {2013},
	bdsk-url-1 = {https://doi.org/10.1038/nmeth.2613}}

@book{Feynman1998,
    author         = {Richard P. Feynman},
    year           = {1998},
    title          = {The Meaning of It All: Thoughts of a Citizen Scientist},
    series         = {},
    volume         = {},
    edition        = {},
    url            = {},
    publisher      = {Addison-Wesley}
}

@unpublished{Feynman1964,
    author = {Richard P. Feynman},
    year   = {1964},
    title  = {Seeking New Laws},
    url    = {https://www.feynmanlectures.caltech.edu/messenger.html}
}

@article{Herzog-Ostwald2013,
	author = {Herzog, Stefan and Ostwald, Dirk},
	date = {2013/02/01},
	date-added = {2024-03-06 16:10:57 +0900},
	date-modified = {2024-03-06 16:10:57 +0900},
	doi = {10.1038/494035b},
	id = {Herzog2013},
	isbn = {1476-4687},
	journal = {Nature},
	number = {7435},
	pages = {35--35},
	title = {Sometimes Bayesian statistics are better},
	url = {https://doi.org/10.1038/494035b},
	volume = {494},
	year = {2013},
	bdsk-url-1 = {https://doi.org/10.1038/494035b}}


@article{Trafimow-Marks2015,
	author = {David Trafimow and Michael Marks},
	doi = {10.1080/01973533.2015.1012991},
	eprint = {https://doi.org/10.1080/01973533.2015.1012991},
	journal = {Basic and Applied Social Psychology},
	number = {1},
	pages = {1-2},
	publisher = {Routledge},
	title = {Editorial},
	url = {https://doi.org/10.1080/01973533.2015.1012991},
	volume = {37},
	year = {2015},
	bdsk-url-1 = {https://doi.org/10.1080/01973533.2015.1012991}}


@article{Nuzzo2014,
	abstract = {P values, the 'gold standard' of statistical validity, are not as reliable as many scientists assume.},
	author = {Nuzzo, Regina},
	date = {2014/02/01},
	date-added = {2024-03-06 16:40:33 +0900},
	date-modified = {2024-03-06 16:40:33 +0900},
	doi = {10.1038/506150a},
	id = {Nuzzo2014},
	isbn = {1476-4687},
	journal = {Nature},
	number = {7487},
	pages = {150--152},
	title = {Scientific method: Statistical errors},
	url = {https://doi.org/10.1038/506150a},
	volume = {506},
	year = {2014},
	bdsk-url-1 = {https://doi.org/10.1038/506150a}}

@article{平石-中村2022,
    author          = {平石界 and 中村大輝},
    year            = {2022},
    title           = {心理学における再現性危機の10年―危機は克服されたのか，克服され得るのか―},
    journal         = {科学哲学},
    volume          = {54},
    number          = {2},
    pages           = {27-50},
    url             = {https://www.jstage.jst.go.jp/article/jpssj/54/2/54_27/_article/-char/ja}
}

@article{Angrist-Pischke2010,
 ISSN = {08953309},
 URL = {http://www.jstor.org/stable/25703496},
 author = {Joshua D. Angrist and Jörn-Steffen Pischke},
 journal = {The Journal of Economic Perspectives},
 number = {2},
 pages = {3--30},
 publisher = {American Economic Association},
 title = {The Credibility Revolution in Empirical Economics: How Better Research Design is Taking the Con out of Econometrics},
 urldate = {2024-03-06},
 volume = {24},
 year = {2010}
}

@misc{Arjovsky+2020,
      title={Invariant Risk Minimization}, 
      author={Martin Arjovsky and Léon Bottou and Ishaan Gulrajani and David Lopez-Paz},
      year={2020},
      eprint={1907.02893},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
url          = {https://arxiv.org/abs/1907.02893},
}

@ARTICLE {Wang+2023-Domain,
author = {J. Wang and C. Lan and C. Liu and Y. Ouyang and T. Qin and W. Lu and Y. Chen and W. Zeng and P. S. Yu},
journal = {IEEE Transactions on Knowledge &amp; Data Engineering},
title = {Generalizing to Unseen Domains: A Survey on Domain Generalization},
year = {2023},
volume = {35},
number = {08},
issn = {1558-2191},
pages = {8052-8072},
abstract = {Machine learning systems generally assume that the training and testing distributions are the same. To this end, a key requirement is to develop models that can generalize to unseen distributions. Domain generalization (DG), i.e., out-of-distribution generalization, has attracted increasing interests in recent years. Domain generalization deals with a challenging setting where one or several different but related domain(s) are given, and the goal is to learn a model that can generalize to an unseen test domain. Great progress has been made in the area of domain generalization for years. This paper presents the first review of recent advances in this area. First, we provide a formal definition of domain generalization and discuss several related fields. We then thoroughly review the theories related to domain generalization and carefully analyze the theory behind generalization. We categorize recent algorithms into three classes: data manipulation, representation learning, and learning strategy, and present several popular algorithms in detail for each category. Third, we introduce the commonly used datasets, applications, and our open-sourced codebase for fair evaluation. Finally, we summarize existing literature and present some potential research topics for the future.},
keywords = {training;task analysis;data models;predictive models;multitasking;computational modeling;adaptation models},
doi = {10.1109/TKDE.2022.3178128},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month = {aug}
}

@book{Wang-Chen2023,
    author         = {Jindong Wang and Yiqiang Chen},
    year           = {2023},
    title          = {Introduction to Transfer Learning: Algorithms and Practice},
    series         = {Machine Learning: Foundations, Methodologies, and Applications},
    volume         = {},
    edition        = {},
    url            = {https://link.springer.com/book/10.1007/978-981-19-7584-4},
    publisher      = {Springer Singapore}
}


@article{Caruana1997,
	abstract = {Multitask Learning is an approach to inductive transfer that improves generalization by using the domain information contained in the training signals of related tasks as an inductive bias. It does this by learning tasks in parallel while using a shared representation; what is learned for each task can help other tasks be learned better. This paper reviews prior work on MTL, presents new evidence that MTL in backprop nets discovers task relatedness without the need of supervisory signals, and presents new results for MTL with k-nearest neighbor and kernel regression. In this paper we demonstrate multitask learning in three domains. We explain how multitask learning works, and show that there are many opportunities for multitask learning in real domains. We present an algorithm and results for multitask learning with case-based methods like k-nearest neighbor and kernel regression, and sketch an algorithm for multitask learning in decision trees. Because multitask learning works, can be applied to many different kinds of domains, and can be used with different learning algorithms, we conjecture there will be many opportunities for its use on real-world problems.},
	author = {Caruana, Rich},
	date = {1997/07/01},
	date-added = {2024-03-10 14:19:51 +0900},
	date-modified = {2024-03-10 14:19:51 +0900},
	doi = {10.1023/A:1007379606734},
	id = {Caruana1997},
	isbn = {1573-0565},
	journal = {Machine Learning},
	number = {1},
	pages = {41--75},
	title = {Multitask Learning},
	url = {https://doi.org/10.1023/A:1007379606734},
	volume = {28},
	year = {1997},
	bdsk-url-1 = {https://doi.org/10.1023/A:1007379606734}}

@ARTICLE{Zhuang+2021,
  author={Zhuang, Fuzhen and Qi, Zhiyuan and Duan, Keyu and Xi, Dongbo and Zhu, Yongchun and Zhu, Hengshu and Xiong, Hui and He, Qing},
  journal={Proceedings of the IEEE}, 
  title={A Comprehensive Survey on Transfer Learning}, 
  year={2021},
  volume={109},
  number={1},
  pages={43-76},
  keywords={Transfer learning;Semisupervised learning;Data models;Covariance matrices;Machine learning;Adaptation models;Domain adaptation;interpretation;machine learning;transfer learning},
  doi={10.1109/JPROC.2020.3004555}}


@article{Wang-Deng2018,
	abstract = {Deep domain adaptation has emerged as a new learning technique to address the lack of massive amounts of labeled data. Compared to conventional methods, which learn shared feature subspaces or reuse important source instances with shallow representations, deep domain adaptation methods leverage deep networks to learn more transferable representations by embedding domain adaptation in the pipeline of deep learning. There have been comprehensive surveys for shallow domain adaptation, but few timely reviews the emerging deep learning based methods. In this paper, we provide a comprehensive survey of deep domain adaptation methods for computer vision applications with four major contributions. First, we present a taxonomy of different deep domain adaptation scenarios according to the properties of data that define how two domains are diverged. Second, we summarize deep domain adaptation approaches into several categories based on training loss, and analyze and compare briefly the state-of-the-art methods under these categories. Third, we overview the computer vision applications that go beyond image classification, such as face recognition, semantic segmentation and object detection. Fourth, some potential deficiencies of current methods and several future directions are highlighted.},
	author = {Mei Wang and Weihong Deng},
	doi = {https://doi.org/10.1016/j.neucom.2018.05.083},
	issn = {0925-2312},
	journal = {Neurocomputing},
	keywords = {Deep domain adaptation, Deep networks, Transfer learning, Computer vision applications},
	pages = {135-153},
	title = {Deep visual domain adaptation: A survey},
	url = {https://www.sciencedirect.com/science/article/pii/S0925231218306684},
	volume = {312},
	year = {2018},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0925231218306684},
	bdsk-url-2 = {https://doi.org/10.1016/j.neucom.2018.05.083}}

@ARTICLE {Hospedales+2022,
author = {T. Hospedales and A. Antoniou and P. Micaelli and A. Storkey},
journal = {IEEE Transactions on Pattern Analysis &amp; Machine Intelligence},
title = {Meta-Learning in Neural Networks: A Survey},
year = {2022},
volume = {44},
number = {09},
issn = {1939-3539},
pages = {5149-5169},
abstract = {The field of meta-learning, or learning-to-learn, has seen a dramatic rise in interest in recent years. Contrary to conventional approaches to AI where tasks are solved from scratch using a fixed learning algorithm, meta-learning aims to improve the learning algorithm itself, given the experience of multiple learning episodes. This paradigm provides an opportunity to tackle many conventional challenges of deep learning, including data and computation bottlenecks, as well as generalization. This survey describes the contemporary meta-learning landscape. We first discuss definitions of meta-learning and position it with respect to related fields, such as transfer learning and hyperparameter optimization. We then propose a new taxonomy that provides a more comprehensive breakdown of the space of meta-learning methods today. We survey promising applications and successes of meta-learning such as few-shot learning and reinforcement learning. Finally, we discuss outstanding challenges and promising areas for future research.},
keywords = {task analysis;optimization;training;machine learning algorithms;predictive models;neural networks;deep learning},
doi = {10.1109/TPAMI.2021.3079209},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month = {sep},
url             = {https://www.computer.org/csdl/journal/tp/2022/09/09428530/1twaJR3AcJW},
}

@misc{Vanschoren2018,
      title={Meta-Learning: A Survey}, 
      author={Joaquin Vanschoren},
      year={2018},
      eprint={1810.03548},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url          = {https://arxiv.org/abs/1810.03548},
}

@inproceedings{Biesialska+2020,
    title = "Continual Lifelong Learning in Natural Language Processing: A Survey",
    author = "Biesialska, Magdalena  and
      Biesialska, Katarzyna  and
      Costa-juss{\`a}, Marta R.",
    editor = "Scott, Donia  and
      Bel, Nuria  and
      Zong, Chengqing",
    booktitle = "Proceedings of the 28th International Conference on Computational Linguistics",
    month = dec,
    year = "2020",
    address = "Barcelona, Spain (Online)",
    publisher = "International Committee on Computational Linguistics",
    url = "https://aclanthology.org/2020.coling-main.574",
    doi = "10.18653/v1/2020.coling-main.574",
    pages = "6523--6541",
    abstract = "Continual learning (CL) aims to enable information systems to learn from a continuous data stream across time. However, it is difficult for existing deep learning architectures to learn a new task without largely forgetting previously acquired knowledge. Furthermore, CL is particularly challenging for language learning, as natural language is ambiguous: it is discrete, compositional, and its meaning is context-dependent. In this work, we look at the problem of CL through the lens of various NLP tasks. Our survey discusses major challenges in CL and current methods applied in neural network models. We also provide a critical review of the existing CL evaluation methods and datasets in NLP. Finally, we present our outlook on future research directions.",
}

@article{Toyota-Fukumizu2024,
title={Out-of-Distribution Optimality of Invariant Risk Minimization},
author={Shoji Toyota and Kenji Fukumizu},
journal={Transactions on Machine Learning Research},
issn={2835-8856},
year={2024},
url={https://openreview.net/forum?id=pWsfWDnJDa},
note={}
}


@InProceedings{Zhao+2019,
  title = 	 {On Learning Invariant Representations for Domain Adaptation},
  author =       {Zhao, Han and Combes, Remi Tachet Des and Zhang, Kun and Gordon, Geoffrey},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {7523--7532},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/zhao19a/zhao19a.pdf},
  url = 	 {https://proceedings.mlr.press/v97/zhao19a.html},
  abstract = 	 {Due to the ability of deep neural nets to learn rich representations, recent advances in unsupervised domain adaptation have focused on learning domain-invariant features that achieve a small error on the source domain. The hope is that the learnt representation, together with the hypothesis learnt from the source domain, can generalize to the target domain. In this paper, we first construct a simple counterexample showing that, contrary to common belief, the above conditions are not sufficient to guarantee successful domain adaptation. In particular, the counterexample exhibits <em>conditional shift</em>: the class-conditional distributions of input features change between source and target domains. To give a sufficient condition for domain adaptation, we propose a natural and interpretable generalization upper bound that explicitly takes into account the aforementioned shift. Moreover, we shed new light on the problem by proving an information-theoretic lower bound on the joint error of <em>any</em> domain adaptation method that attempts to learn invariant representations. Our result characterizes a fundamental tradeoff between learning invariant representations and achieving small joint error on both domains when the marginal label distributions differ from source to target. Finally, we conduct experiments on real-world datasets that corroborate our theoretical findings. We believe these insights are helpful in guiding the future design of domain adaptation and representation learning algorithms.}
}


@article{Ganin+2016,
	author = {Yaroslav Ganin and Evgeniya Ustinova and Hana Ajakan and Pascal Germain and Hugo Larochelle and Fran{\c{c}}ois Laviolette and Mario March and Victor Lempitsky},
	journal = {Journal of Machine Learning Research},
	number = {59},
	pages = {1--35},
	title = {Domain-Adversarial Training of Neural Networks},
	url = {http://jmlr.org/papers/v17/15-239.html},
	volume = {17},
	year = {2016},
	bdsk-url-1 = {http://jmlr.org/papers/v17/15-239.html}}

@misc{Gatys+2015,
      title={A Neural Algorithm of Artistic Style}, 
      author={Leon A. Gatys and Alexander S. Ecker and Matthias Bethge},
      year={2015},
      eprint={1508.06576},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url          = {https://arxiv.org/abs/1508.06576},
}

@INPROCEEDINGS{Gatys+2016,
  author={Gatys, Leon A. and Ecker, Alexander S. and Bethge, Matthias},
  booktitle={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Image Style Transfer Using Convolutional Neural Networks}, 
  year={2016},
  volume={},
  number={},
  pages={2414-2423},
  keywords={Image reconstruction;Neural networks;Image representation;Semantics;Neuroscience;Feature extraction;Visualization},
  doi={10.1109/CVPR.2016.265}}

@article{McAllister+2017, title={Concrete Problems for Autonomous Vehicle Safety: Advantages of Bayesian Deep Learning}, url={https://www.repository.cam.ac.uk/handle/1810/266683}, DOI={10.17863/CAM.12760}, publisher={International Joint Conferences on Artificial Intelligence, Inc.}, author={McAllister, RT and Gal, Y and Kendall, A and Van Der Wilk, M and Shah, A and Cipolla, R and Weller, A}, year={2017}, keywords={technical: techniques, technical: models, social: challenges, social: human-machine interaction} }


@article{Bensal+2019,
	abstractnote = {&lt;p&gt;AI systems are being deployed to support human decision making in high-stakes domains such as healthcare and criminal justice. In many cases, the human and AI form a team, in which the human makes decisions after reviewing the AI's inferences. A successful partnership requires that the human develops insights into the performance of the AI system, including its failures. We study the influence of &lt;em&gt;updates&lt;/em&gt; to an AI system in this setting. While updates can increase the AI's predictive performance, they may also lead to behavioral changes that are at odds with the user's prior experiences and confidence in the AI's inferences. We show that updates that increase AI performance may actually hurt &lt;em&gt;team&lt;/em&gt; performance. We introduce the notion of the &lt;em&gt;compatibility&lt;/em&gt; of an AI update with prior user experience and present methods for studying the role of compatibility in human-AI teams. Empirical results on three high-stakes classification tasks show that current machine learning algorithms do not produce compatible updates. We propose a re-training objective to improve the compatibility of an update by penalizing new errors. The objective offers full leverage of the performance/compatibility tradeoff across different datasets, enabling more compatible yet accurate updates.&lt;/p&gt;},
	author = {Bansal, Gagan and Nushi, Besmira and Kamar, Ece and Weld, Daniel S. and Lasecki, Walter S. and Horvitz, Eric},
	doi = {10.1609/aaai.v33i01.33012429},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	month = {Jul.},
	number = {01},
	pages = {2429-2437},
	title = {Updates in Human-AI Teams: Understanding and Addressing the Performance/Compatibility Tradeoff},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/4087},
	volume = {33},
	year = {2019},
	bdsk-url-1 = {https://ojs.aaai.org/index.php/AAAI/article/view/4087},
	bdsk-url-2 = {https://doi.org/10.1609/aaai.v33i01.33012429}}

@inproceedings{Amershi+2019,
author = {Amershi, Saleema and Weld, Dan and Vorvoreanu, Mihaela and Fourney, Adam and Nushi, Besmira and Collisson, Penny and Suh, Jina and Iqbal, Shamsi and Bennett, Paul N. and Inkpen, Kori and Teevan, Jaime and Kikin-Gil, Ruth and Horvitz, Eric},
title = {Guidelines for Human-AI Interaction},
year = {2019},
isbn = {9781450359702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3290605.3300233},
doi = {10.1145/3290605.3300233},
abstract = {Advances in artificial intelligence (AI) frame opportunities and challenges for user interface design. Principles for human-AI interaction have been discussed in the human-computer interaction community for over two decades, but more study and innovation are needed in light of advances in AI and the growing uses of AI technologies in human-facing applications. We propose 18 generally applicable design guidelines for human-AI interaction. These guidelines are validated through multiple rounds of evaluation including a user study with 49 design practitioners who tested the guidelines against 20 popular AI-infused products. The results verify the relevance of the guidelines over a spectrum of interaction scenarios and reveal gaps in our knowledge, highlighting opportunities for further research. Based on the evaluations, we believe the set of design guidelines can serve as a resource to practitioners working on the design of applications and features that harness AI technologies, and to researchers interested in the further development of human-AI interaction design principles.},
booktitle = {Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems},
pages = {1–13},
numpages = {13},
keywords = {ai-infused systems, design guidelines, human-ai interaction},
location = {Glasgow, Scotland Uk},
series = {CHI '19}
}

@misc{Horita+2023,
      title={A Structure-Guided Diffusion Model for Large-Hole Image Completion}, 
      author={Daichi Horita and Jiaolong Yang and Dong Chen and Yuki Koyama and Kiyoharu Aizawa and Nicu Sebe},
      year={2023},
      eprint={2211.10437},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url          = {https://arxiv.org/abs/2211.10437},
}

@inproceedings{Higgins+2017,
title={beta-{VAE}: Learning Basic Visual Concepts with a Constrained Variational Framework},
author={Irina Higgins and Loic Matthey and Arka Pal and Christopher Burgess and Xavier Glorot and Matthew Botvinick and Shakir Mohamed and Alexander Lerchner},
booktitle={International Conference on Learning Representations},
year={2017},
url={https://openreview.net/forum?id=Sy2fzU9gl}
}

@inproceedings{Burns-Fukai2023,
title={Simplicial Hopfield networks},
author={Thomas F Burns and Tomoki Fukai},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=_QLsH8gatwx}
}

@article{山本-尾崎2018,
    author          = {山本龍彦 and 尾崎愛美},
    year            = {2018},
    title           = {アルゴリズムと公正：State v. Loomis判決を素材に},
    journal         = {科学技術社会論研究},
    volume          = {16},
    number          = {},
    pages           = {96-107},
    url             = {https://www.jstage.jst.go.jp/article/jnlsts/16/0/16_96/_article/-char/ja}
}

@unpublished{深谷賢治1997,
    author = {深谷賢治},
    year   = {1997},
    title  = {「位相的場の理論」 集中講義ノート},
    url    = {https://www.math.kyoto-u.ac.jp/~fukaya/},
    note   = {静岡大学}
}


@inbook{Gromov2001,
	abstract = {Here are a few brief remarks on possible trends in mathematics for the coming decades.},
	address = {Berlin, Heidelberg},
	author = {Gromov, Mikhael},
	booktitle = {Mathematics Unlimited --- 2001 and Beyond},
	doi = {10.1007/978-3-642-56478-9_26},
	editor = {Engquist, Bj{\"o}rn and Schmid, Wilfried},
	isbn = {978-3-642-56478-9},
	pages = {525--527},
	publisher = {Springer Berlin Heidelberg},
	title = {Possible Trends in Mathematics in the Coming Decades},
	url = {https://doi.org/10.1007/978-3-642-56478-9_26},
	year = {2001},
	bdsk-url-1 = {https://doi.org/10.1007/978-3-642-56478-9_26}}

@unpublished{福水健次2024,
    author = {福水健次},
    year   = {2024},
    title  = {Machine Learning with Group Theory},
    url    = {},
    note   = {MLSS2024 Lecture}
}


@article{Azulay-Weiss2019,
	author = {Aharon Azulay and Yair Weiss},
	journal = {Journal of Machine Learning Research},
	number = {184},
	pages = {1--25},
	title = {Why do deep convolutional networks generalize so poorly to small image transformations?},
	url = {http://jmlr.org/papers/v20/19-519.html},
	volume = {20},
	year = {2019},
	bdsk-url-1 = {http://jmlr.org/papers/v20/19-519.html}}


@InProceedings{Cohen-Welling2016,
  title = 	 {Group Equivariant Convolutional Networks},
  author = 	 {Cohen, Taco and Welling, Max},
  booktitle = 	 {Proceedings of The 33rd International Conference on Machine Learning},
  pages = 	 {2990--2999},
  year = 	 {2016},
  editor = 	 {Balcan, Maria Florina and Weinberger, Kilian Q.},
  volume = 	 {48},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {New York, New York, USA},
  month = 	 {20--22 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v48/cohenc16.pdf},
  url = 	 {https://proceedings.mlr.press/v48/cohenc16.html},
  abstract = 	 {We introduce Group equivariant Convolutional Neural Networks (G-CNNs), a natural generalization of convolutional neural networks that reduces sample complexity by exploiting symmetries. G-CNNs use G-convolutions, a new type of layer that enjoys a substantially higher degree of weight sharing than regular convolution layers. G-convolutions increase the expressive capacity of the network without increasing the number of parameters. Group convolution layers are easy to use and can be implemented with negligible computational overhead for discrete groups generated by translations, reflections and rotations. G-CNNs achieve state of the art results on CIFAR10 and rotated MNIST.}
}

@article{Lafarge+2021,
	abstract = {Rotation-invariance is a desired property of machine-learning models for medical image analysis and in particular for computational pathology applications. We propose a framework to encode the geometric structure of the special Euclidean motion group SE(2) in convolutional networks to yield translation and rotation equivariance via the introduction of SE(2)-group convolution layers. This structure enables models to learn feature representations with a discretized orientation dimension that guarantees that their outputs are invariant under a discrete set of rotations. Conventional approaches for rotation invariance rely mostly on data augmentation, but this does not guarantee the robustness of the output when the input is rotated. At that, trained conventional CNNs may require test-time rotation augmentation to reach their full capability. This study is focused on histopathology image analysis applications for which it is desirable that the arbitrary global orientation information of the imaged tissues is not captured by the machine learning models. The proposed framework is evaluated on three different histopathology image analysis tasks (mitosis detection, nuclei segmentation and tumor detection). We present a comparative analysis for each problem and show that consistent increase of performances can be achieved when using the proposed framework.},
	author = {Maxime W. Lafarge and Erik J. Bekkers and Josien P.W. Pluim and Remco Duits and Mitko Veta},
	doi = {https://doi.org/10.1016/j.media.2020.101849},
	issn = {1361-8415},
	journal = {Medical Image Analysis},
	keywords = {Group convolutional neural network, Roto-translation equivariance, Computational pathology, Mitosis detection, Tumor detection, Nuclei segmentation},
	pages = {101849},
	title = {Roto-translation equivariant convolutional networks: Application to histopathology image analysis},
	url = {https://www.sciencedirect.com/science/article/pii/S1361841520302139},
	volume = {68},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S1361841520302139},
	bdsk-url-2 = {https://doi.org/10.1016/j.media.2020.101849}}

@inproceedings{Cohen-Welling2017,
title={Steerable {CNN}s},
author={Taco S. Cohen and Max Welling},
booktitle={International Conference on Learning Representations},
year={2017},
url={https://openreview.net/forum?id=rJQKYt5ll}
}

@InProceedings{Weiler+2018,
author = {Weiler, Maurice and Hamprecht, Fred A. and Storath, Martin},
title = {Learning Steerable Filters for Rotation Equivariant CNNs},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2018}
} 

@misc{Levine2018,
      title={Reinforcement Learning and Control as Probabilistic Inference: Tutorial and Review}, 
      author={Sergey Levine},
      year={2018},
      eprint={1805.00909},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url          = {https://arxiv.org/abs/1805.00909},
}


@InProceedings{Bellemare+2017,
  title = 	 {A Distributional Perspective on Reinforcement Learning},
  author =       {Marc G. Bellemare and Will Dabney and R{\'e}mi Munos},
  booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
  pages = 	 {449--458},
  year = 	 {2017},
  editor = 	 {Precup, Doina and Teh, Yee Whye},
  volume = 	 {70},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {06--11 Aug},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v70/bellemare17a/bellemare17a.pdf},
  url = 	 {https://proceedings.mlr.press/v70/bellemare17a.html},
  abstract = 	 {In this paper we argue for the fundamental importance of the value distribution: the distribution of the random return received by a reinforcement learning agent. This is in contrast to the common approach to reinforcement learning which models the expectation of this return, or value. Although there is an established body of literature studying the value distribution, thus far it has always been used for a specific purpose such as implementing risk-aware behaviour. We begin with theoretical results in both the policy evaluation and control settings, exposing a significant distributional instability in the latter. We then use the distributional perspective to design a new algorithm which applies Bellman’s equation to the learning of approximate value distributions. We evaluate our algorithm using the suite of games from the Arcade Learning Environment. We obtain both state-of-the-art results and anecdotal evidence demonstrating the importance of the value distribution in approximate reinforcement learning. Finally, we combine theoretical and empirical evidence to highlight the ways in which the value distribution impacts learning in the approximate setting.}
}

@book{Bellemare+2023,
    author         = {Marc G. Bellemare and Will Dabney and Mark Rowland},
    year           = {2023},
    title          = {Distributional Reinforcement Learning},
    series         = {Adaptive Computation and Machine Learning series},
    volume         = {},
    edition        = {},
    url            = {https://doi.org/10.7551/mitpress/14207.001.0001},
    publisher      = {The MIT Press}
}


@article{Silver+2016,
	abstract = {The game of Go has long been viewed as the most challenging of classic games for artificial intelligence owing to its enormous search space and the difficulty of evaluating board positions and moves. Here we introduce a new approach to computer Go that uses `value networks'to evaluate board positions and `policy networks'to select moves. These deep neural networks are trained by a novel combination of supervised learning from human expert games, and reinforcement learning from games of self-play. Without any lookahead search, the neural networks play Go at the level of state-of-the-art Monte Carlo tree search programs that simulate thousands of random games of self-play. We also introduce a new search algorithm that combines Monte Carlo simulation with value and policy networks. Using this search algorithm, our program AlphaGo achieved a 99.8{\%} winning rate against other Go programs, and defeated the human European Go champion by 5 games to 0. This is the first time that a computer program has defeated a human professional player in the full-sized game of Go, a feat previously thought to be at least a decade away.},
	author = {Silver, David and Huang, Aja and Maddison, Chris J. and Guez, Arthur and Sifre, Laurent and van den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik and Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray and Graepel, Thore and Hassabis, Demis},
	date = {2016/01/01},
	date-added = {2024-03-12 12:07:42 +0900},
	date-modified = {2024-03-12 12:07:42 +0900},
	doi = {10.1038/nature16961},
	id = {Silver2016},
	isbn = {1476-4687},
	journal = {Nature},
	number = {7587},
	pages = {484--489},
	title = {Mastering the game of Go with deep neural networks and tree search},
	url = {https://doi.org/10.1038/nature16961},
	volume = {529},
	year = {2016},
	bdsk-url-1 = {https://doi.org/10.1038/nature16961}}


@article{Silver+2017,
	abstract = {A long-standing goal of artificial intelligence is an algorithm that learns, tabula rasa, superhuman proficiency in challenging domains. Recently, AlphaGo became the first program to defeat a world champion in the game of Go. The tree search in AlphaGo evaluated positions and selected moves using deep neural networks. These neural networks were trained by supervised learning from human expert moves, and by reinforcement learning from self-play. Here we introduce an algorithm based solely on reinforcement learning, without human data, guidance or domain knowledge beyond game rules. AlphaGo becomes its own teacher: a neural network is trained to predict AlphaGo's own move selections and also the winner of AlphaGo's games. This neural network improves the strength of the tree search, resulting in higher quality move selection and stronger self-play in the next iteration. Starting tabula rasa, our new program AlphaGo Zero achieved superhuman performance, winning 100--0 against the previously published, champion-defeating AlphaGo.},
	author = {Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and Chen, Yutian and Lillicrap, Timothy and Hui, Fan and Sifre, Laurent and van den Driessche, George and Graepel, Thore and Hassabis, Demis},
	date = {2017/10/01},
	date-added = {2024-03-12 12:09:45 +0900},
	date-modified = {2024-03-12 12:09:45 +0900},
	doi = {10.1038/nature24270},
	id = {Silver2017},
	isbn = {1476-4687},
	journal = {Nature},
	number = {7676},
	pages = {354--359},
	title = {Mastering the game of Go without human knowledge},
	url = {https://doi.org/10.1038/nature24270},
	volume = {550},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.1038/nature24270}}


@InProceedings{Chizat-Bach2020,
  title = 	 {Implicit Bias of Gradient Descent for Wide Two-layer Neural Networks Trained with the Logistic Loss},
  author =       {Chizat, L\'ena\"ic  and Bach, Francis},
  booktitle = 	 {Proceedings of Thirty Third Conference on Learning Theory},
  pages = 	 {1305--1338},
  year = 	 {2020},
  editor = 	 {Abernethy, Jacob and Agarwal, Shivani},
  volume = 	 {125},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--12 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v125/chizat20a/chizat20a.pdf},
  url = 	 {https://proceedings.mlr.press/v125/chizat20a.html},
  abstract = 	 { Neural networks trained to minimize the logistic (a.k.a. cross-entropy) loss with gradient-based methods are observed to perform well in many supervised classification tasks. Towards understanding this phenomenon, we analyze the training and generalization behavior of infinitely wide two-layer neural networks with homogeneous activations. We show that the limits of the gradient flow on exponentially tailed losses can be fully characterized as a max-margin classifier in a certain non-Hilbertian space of functions. In presence of hidden low-dimensional structures, the resulting margin is independent of the ambiant dimension, which leads to strong generalization bounds. In contrast, training only the output layer implicitly solves a kernel support vector machine, which a priori does not enjoy such an adaptivity. Our analysis of training is non-quantitative in terms of running time but we prove computational guarantees in simplified settings by showing equivalences with online mirror descent. Finally, numerical experiments suggest that our analysis describes well the practical behavior of two-layer neural networks with ReLU activation and confirm the statistical benefits of this implicit bias.}
}


@inproceedings{Moroshko+2020,
	author = {Moroshko, Edward and Woodworth, Blake E and Gunasekar, Suriya and Lee, Jason D and Srebro, Nati and Soudry, Daniel},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
	pages = {22182--22193},
	publisher = {Curran Associates, Inc.},
	title = {Implicit Bias in Deep Linear Classification: Initialization Scale vs Training Accuracy},
	url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/fc2022c89b61c76bbef978f1370660bf-Paper.pdf},
	volume = {33},
	year = {2020},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2020/file/fc2022c89b61c76bbef978f1370660bf-Paper.pdf}}

@ARTICLE{Song+2013,
  author={Song, Le and Fukumizu, Kenji and Gretton, Arthur},
  journal={IEEE Signal Processing Magazine}, 
  title={{Kernel Embeddings of Conditional Distributions: A Unified Kernel Framework for Nonparametric Inference in Graphical Models}}, 
  year={2013},
  volume={30},
  number={4},
  pages={98-111},
  keywords={Machine learning;Learning systems;Kernel;Computer vision;Computational biology;Parametric statistics},
  doi={10.1109/MSP.2013.2252713}}


@inproceedings{Li+2022,
	author = {Li, Zhu and Meunier, Dimitri and Mollenhauer, Mattes and Gretton, Arthur},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
	pages = {4433--4445},
	publisher = {Curran Associates, Inc.},
	title = {Optimal Rates for Regularized Conditional Mean Embedding Learning},
	url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/1c71cd4032da425409d8ada8727bad42-Paper-Conference.pdf},
	volume = {35},
	year = {2022},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2022/file/1c71cd4032da425409d8ada8727bad42-Paper-Conference.pdf}}


@inproceedings{Oark-Muandet2020,
	author = {Park, Junhyung and Muandet, Krikamol},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
	pages = {21247--21259},
	publisher = {Curran Associates, Inc.},
	title = {A Measure-Theoretic Approach to Kernel Conditional Mean Embeddings},
	url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/f340f1b1f65b6df5b5e3f94d95b11daf-Paper.pdf},
	volume = {33},
	year = {2020},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2020/file/f340f1b1f65b6df5b5e3f94d95b11daf-Paper.pdf}}

@inproceedings{Song+2009,
author = {Song, Le and Huang, Jonathan and Smola, Alex and Fukumizu, Kenji},
title = {Hilbert space embeddings of conditional distributions with applications to dynamical systems},
year = {2009},
isbn = {9781605585161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1553374.1553497},
doi = {10.1145/1553374.1553497},
abstract = {In this paper, we extend the Hilbert space embedding approach to handle conditional distributions. We derive a kernel estimate for the conditional embedding, and show its connection to ordinary embeddings. Conditional embeddings largely extend our ability to manipulate distributions in Hilbert spaces, and as an example, we derive a nonparametric method for modeling dynamical systems where the belief state of the system is maintained as a conditional embedding. Our method is very general in terms of both the domains and the types of distributions that it can handle, and we demonstrate the effectiveness of our method in various dynamical systems. We expect that conditional embeddings will have wider applications beyond modeling dynamical systems.},
booktitle = {Proceedings of the 26th Annual International Conference on Machine Learning},
pages = {961–968},
numpages = {8},
location = {Montreal, Quebec, Canada},
series = {ICML '09}
}

@ARTICLE{Wamg+2024,
  author={Wang, Liyuan and Zhang, Xingxing and Su, Hang and Zhu, Jun},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={A Comprehensive Survey of Continual Learning: Theory, Method and Application}, 
  year={2024},
  volume={},
  number={},
  pages={1-20},
  keywords={Task analysis;Training;Surveys;Testing;Complexity theory;Stability analysis;Visualization;Continual learning;incremental learning;lifelong learning;catastrophic forgetting},
  doi={10.1109/TPAMI.2024.3367329}}

@misc{Novello+2024,
      title={Out-of-Distribution Detection Should Use Conformal Prediction (and Vice-versa?)}, 
      author={Paul Novello and Joseba Dalmau and Léo Andeol},
      year={2024},
      eprint={2403.11532},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url          = {https://arxiv.org/abs/2403.11532},
}

@misc{Farquhar-Gal2019,
      title={A Unifying Bayesian View of Continual Learning}, 
      author={Sebastian Farquhar and Yarin Gal},
      year={2019},
      eprint={1902.06494},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url          = {https://arxiv.org/abs/1902.06494},
}

@article{Efron1986,
 ISSN = {00031305},
 URL = {http://www.jstor.org/stable/2683105},
 abstract = {Originally a talk delivered at a conference on Bayesian statistics, this article attempts to answer the following question: why is most scientific data analysis carried out in a non-Bayesian framework? The argument consists mainly of some practical examples of data analysis, in which the Bayesian approach is difficult but Fisherian/frequentist solutions are relatively easy. There is a brief discussion of objectivity in statistical analyses and of the difficulties of achieving objectivity within a Bayesian framework. The article ends with a list of practical advantages of Fisherian/frequentist methods, which so far seem to have outweighed the philosophical superiority of Bayesianism.},
 author = {B. Efron},
 journal = {The American Statistician},
 number = {1},
 pages = {1--5},
 publisher = {[American Statistical Association, Taylor & Francis, Ltd.]},
 title = {Why Isn't Everyone a Bayesian?},
 urldate = {2024-03-19},
 volume = {40},
 year = {1986}
}


@article{Rubin1984,
	author = {Donald B. Rubin},
	doi = {10.1214/aos/1176346785},
	journal = {The Annals of Statistics},
	keywords = {62-07, Calibration, Empirical Bayes, inference, model monitoring, operating characteristics, posterior predictive checks, Stopping rules},
	number = {4},
	pages = {1151 -- 1172},
	publisher = {Institute of Mathematical Statistics},
	title = {{Bayesianly Justifiable and Relevant Frequency Calculations for the Applied Statistician}},
	url = {https://doi.org/10.1214/aos/1176346785},
	volume = {12},
	year = {1984},
	bdsk-url-1 = {https://doi.org/10.1214/aos/1176346785}}

@book{Frey1998,
    author         = {Brendan J. Frey},
    year           = {1998},
    title          = {Graphical Models for Machine Learning and Digital Communication},
    series         = {Adaptive Computation and Machine Learning Series},
    volume         = {},
    edition        = {},
    url            = {https://mitpress.mit.edu/9780262062022/graphical-models-for-machine-learning-and-digital-communication/},
    publisher      = {The MIT Press}
}

@inproceedings{Deisenroth-Rasmussen2011,
author = {Deisenroth, Marc Peter and Rasmussen, Carl Edward},
title = {PILCO: a model-based and data-efficient approach to policy search},
year = {2011},
isbn = {9781450306195},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {In this paper, we introduce PILCO, a practical, data-efficient model-based policy search method. PILCO reduces model bias, one of the key problems of model-based reinforcement learning, in a principled way. By learning a probabilistic dynamics model and explicitly incorporating model uncertainty into long-term planning, PILCO can cope with very little data and facilitates learning from scratch in only a few trials. Policy evaluation is performed in closed form using state-of-the-art approximate inference. Furthermore, policy gradients are computed analytically for policy improvement. We report unprecedented learning efficiency on challenging and high-dimensional control tasks.},
booktitle = {Proceedings of the 28th International Conference on International Conference on Machine Learning},
pages = {465–472},
numpages = {8},
location = {Bellevue, Washington, USA},
series = {ICML'11}
}


@article{Hagen1991,
	abstract = {Bayesian quadrature treats the problem of numerical integration as one of statistical inference. A prior Gaussian process distribution is assumed for the integrand, observations arise from evaluating the integrand at selected points, and a posterior distribution is derived for the integrand and the integral. Methods are developed for quadrature in Rp. A particular application is integrating the posterior density arising from some other Bayesian analysis. Simulation results are presented, to show that the resulting Bayes--Hermite quadrature rules may perform better than the conventional Gauss--Hermite rules for this application. A key result is derived for product designs, which makes Bayesian quadrature practically useful for integrating in several dimensions. Although the method does not at present provide a solution to the more difficult problem of quadrature in high dimensions, it does seem to offer real improvements over existing methods in relatively low dimensions.},
	author = {A. O'Hagan},
	doi = {https://doi.org/10.1016/0378-3758(91)90002-V},
	issn = {0378-3758},
	journal = {Journal of Statistical Planning and Inference},
	keywords = {Bayesian quadrature, numerical integration, Gaussian process, product rule, Gaussian quadrature},
	number = {3},
	pages = {245-260},
	title = {Bayes--Hermite quadrature},
	url = {https://www.sciencedirect.com/science/article/pii/037837589190002V},
	volume = {29},
	year = {1991},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/037837589190002V},
	bdsk-url-2 = {https://doi.org/10.1016/0378-3758(91)90002-V}}

@INPROCEEDINGS{Steinruecken+2015,
  author={Steinruecken, Christian and Ghahramani, Zoubin and MacKay, David},
  booktitle={2015 Data Compression Conference}, 
  title={Improving PPM with Dynamic Parameter Updates}, 
  year={2015},
  volume={},
  number={},
  pages={193-202},
  keywords={Context;Prediction algorithms;Mathematical model;Probabilistic logic;Heuristic algorithms;Predictive models;Probability distribution;PPM;blending;escape mechanism;gradients;dynamic updates;data compression},
  doi={10.1109/DCC.2015.77}}

@inproceedings{Lloyd+2014,
author = {Lloyd, James Robert and Duvenaud, David and Grosse, Roger and Tenenbaum, Joshua B. and Ghahramani, Zoubin},
title = {Automatic construction and natural-language description of nonparametric regression models},
year = {2014},
publisher = {AAAI Press},
abstract = {This paper presents the beginnings of an automatic statistician, focusing on regression problems. Our system explores an open-ended space of statistical models to discover a good explanation of a data set, and then produces a detailed report with figures and natural language text.Our approach treats unknown regression functions nonparametrically using Gaussian processes, which has two important consequences. First, Gaussian processes can model functions in terms of high-level properties (e.g. smoothness, trends, periodicity, changepoints). Taken together with the compositional structure of our language of models this allows us to automatically describe functions in simple terms. Second, the use of flexible nonparametric models and a rich language for composing them in an open-ended manner also results in state-of-the-art extrapolation performance evaluated over 13 real time series data sets from various domains.},
booktitle = {Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence},
pages = {1242–1250},
numpages = {9},
location = {Qu\'{e}bec City, Qu\'{e}bec, Canada},
series = {AAAI'14}
}


@article{Khan-Rue2023,
	author = {Mohammad Emtiyaz Khan and H{\aa}vard Rue},
	journal = {Journal of Machine Learning Research},
	number = {281},
	pages = {1--46},
	title = {The Bayesian Learning Rule},
	url = {http://jmlr.org/papers/v24/22-0291.html},
	volume = {24},
	year = {2023},
	bdsk-url-1 = {http://jmlr.org/papers/v24/22-0291.html}}


@inproceedings{Kim+2022,
	author = {Kim, Kyurae and Oh, Jisu and Gardner, Jacob and Dieng, Adji Bousso and Kim, Hongseok},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
	pages = {34802--34816},
	publisher = {Curran Associates, Inc.},
	title = {Markov Chain Score Ascent: A Unifying Framework of Variational Inference with Markovian Gradients},
	url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/e0fbc0f2e35e58aeffe5524a69ba90e5-Paper-Conference.pdf},
	volume = {35},
	year = {2022},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2022/file/e0fbc0f2e35e58aeffe5524a69ba90e5-Paper-Conference.pdf}}


@inproceedings{Naesseth+2020,
	author = {Naesseth, Christian and Lindsten, Fredrik and Blei, David},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
	pages = {15499--15510},
	publisher = {Curran Associates, Inc.},
	title = {Markovian Score Climbing: Variational Inference with KL(p\vert \vert q)},
	url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/b20706935de35bbe643733f856d9e5d6-Paper.pdf},
	volume = {33},
	year = {2020},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2020/file/b20706935de35bbe643733f856d9e5d6-Paper.pdf}}


@InProceedings{Ou-Song2020,
  title = 	 {Joint Stochastic Approximation and Its Application to Learning Discrete Latent Variable Models},
  author =       {Ou, Zhijian and Song, Yunfu},
  booktitle = 	 {Proceedings of the 36th Conference on Uncertainty in Artificial Intelligence (UAI)},
  pages = 	 {929--938},
  year = 	 {2020},
  editor = 	 {Peters, Jonas and Sontag, David},
  volume = 	 {124},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {03--06 Aug},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v124/ou20a/ou20a.pdf},
  url = 	 {https://proceedings.mlr.press/v124/ou20a.html},
  abstract = 	 {Although with progress in introducing auxiliary amortized inference models, learning discrete latent variable models is still challenging. In this paper, we show that the annoying difficulty of obtaining reliable stochastic gradients for the inference model and the drawback of indirectly optimizing the target log-likelihood can be gracefully addressed in a new method based on stochastic approximation (SA) theory of the Robbins-Monro type. Specifically, we propose to directly maximize the target log-likelihood and simultaneously minimize the inclusive divergence between the posterior and the inference model. The resulting learning algorithm is called joint SA (JSA). To the best of our knowledge, JSA represents the first method that couples an SA version of the EM (expectation-maximization) algorithm (SAEM) with an adaptive MCMC procedure. Experiments on several benchmark generative modeling and structured prediction tasks show that JSA consistently outperforms recent competitive algorithms, with faster convergence, better final likelihoods, and lower variance of gradient estimates.}
}
@misc{Papamarkou+2024,
      title={Position Paper: Bayesian Deep Learning in the Age of Large-Scale AI}, 
      author={Theodore Papamarkou and Maria Skoularidou and Konstantina Palla and Laurence Aitchison and Julyan Arbel and David Dunson and Maurizio Filippone and Vincent Fortuin and Philipp Hennig and Jose Miguel Hernandez Lobato and Aliaksandr Hubin and Alexander Immer and Theofanis Karaletsos and Mohammad Emtiyaz Khan and Agustinus Kristiadi and Yingzhen Li and Stephan Mandt and Christopher Nemeth and Michael A. Osborne and Tim G. J. Rudner and David Rügamer and Yee Whye Teh and Max Welling and Andrew Gordon Wilson and Ruqi Zhang},
      year={2024},
      eprint={2402.00809},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url             = {https://arxiv.org/abs/2402.00809},
}

@misc{Chen+2023,
      title={Probabilistic Uncertainty Quantification of Prediction Models with Application to Visual Localization}, 
      author={Junan Chen and Josephine Monica and Wei-Lun Chao and Mark Campbell},
      year={2023},
      eprint={2305.20044},
      archivePrefix={arXiv},
      primaryClass={cs.RO},
      url          = {https://arxiv.org/abs/2305.20044},
}

@techreport{内山貴之2015,
    author      = {内山貴之},
    institution = {東芝},
    title       = {半導体向けEUVリソグラフィの現状と展望},
    year        = {2015},
    url         = {https://www.heas.jp/lecture/files/uchiyama.pdf},
}

@inbook{斎藤毅2010,
    author         = {斎藤毅},
    chapter        = {グロタンディーク},
    editor         = {},
    pages          = {8-13},
    publisher      = {日本評論社},
    title          = {数学セミナー},
    year           = {2010},
    month          = {5},
    url            = {https://www.ms.u-tokyo.ac.jp/~t-saito/jd/gr.pdf},
}

@article{Kulik-Scheutzow2015,
    author          = {Alexei Kulik and Michael Scheutzow},
    year            = {2015},
    title           = {A Coupling Approach to Doob's Theorem},
    journal         = {Rendiconti Lincei Matematica e Applicazioni},
    volume          = {26},
    number          = {1},
    pages           = {83-92},
    url             = {https://ems.press/journals/rlm/articles/12955}
}

@inproceedings{Hairer-Mattingly2011,
	abstract = {The aim of this note is to present an elementary proof of a variation of Harris' ergodic theorem of Markov chains.},
	address = {Basel},
	author = {Hairer, Martin and Mattingly, Jonathan C.},
	booktitle = {Seminar on Stochastic Analysis, Random Fields and Applications VI},
	editor = {Dalang, Robert and Dozzi, Marco and Russo, Francesco},
	isbn = {978-3-0348-0021-1},
	pages = {109--117},
	publisher = {Springer Basel},
	title = {Yet Another Look at Harris' Ergodic Theorem for Markov Chains},
	year = {2011}}

@unpublished{Hairer2021-Convergence,
    author = {Martin Hairer},
    note   = {Lecture Note},
    title  = {Convergence of Markov Processes},
    year   = {2021},
    url    = {https://www.hairer.org/notes/Convergence.pdf}
}


@InProceedings{Gao+2020,
author = {Gao, Ruiqi and Nijkamp, Erik and Kingma, Diederik P. and Xu, Zhen and Dai, Andrew M. and Wu, Ying Nian},
title = {Flow Contrastive Estimation of Energy-Based Models},
booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2020},
url             = {https://openaccess.thecvf.com/content_CVPR_2020/html/Gao_Flow_Contrastive_Estimation_of_Energy-Based_Models_CVPR_2020_paper},
}

@inbook{LeCun+2007,
    author         = {Yann LeCun and Sumit Chopra and Raia Hadsell and Marc' Aurelio Ranzato and Fu Jie Huang},
    chapter        = {Energy-Based Models},
    editor         = {Gökhan Baklr and Thomas Hofmann and Bernhard Schölkopf and Alexander J. Smola and Ben Taskar and S. V. N. Vishwanathan},
    pages          = {191-246},
    publisher      = {The MIT Press},
    title          = {Predicting Structured Data},
    year           = {2007},
    url            = {https://ieeexplore.ieee.org/document/6270201},
}

@inproceedings{Kim-Bengio2016,
    author          = {Taesup Kim and Yoshua Bengio},
    year            = {2016},
    title           = {Deep Directed Generative Models with Energy-Based Probability Estimation},
    booktitle       = {International Conference on Learning Representations},
    volume          = {},
    pages           = {},
    url             = {https://openreview.net/forum?id=BNYAGZZj5S7PwR1riXzA}
}

@book{Mezard-Montanari2009,
    author         = {Marc Mézard and Andrea Montanari},
    year           = {2009},
    title          = {Information, Physics, and Computation},
    series         = {Oxford Graduate Texts},
    volume         = {},
    edition        = {},
    url            = {https://doi.org/10.1093/acprof:oso/9780198570837.001.0001},
    publisher      = {Oxford University Press}
}

@unpublished{Chewi2024,
    author = {Sinho Chewi},
    year   = {2024},
    title  = {Log-Concave Sampling},
    url    = {https://chewisinho.github.io/}
}

@misc{Fearnhead+2017,
      title={Continious-time Importance Sampling: Monte Carlo Methods which Avoid Time-discretisation Error}, 
      author={Paul Fearnhead and Krzystof Latuszynski and Gareth O. Roberts and Giorgos Sermaidis},
      year={2017},
      eprint={1712.06201},
      archivePrefix={arXiv},
      primaryClass={stat.ME},
      url          = {https://arxiv.org/abs/1712.06201},
}

@article{Dai+2019,
 ISSN = {00219002},
 URL = {http://www.jstor.org/stable/45277571},
 author = {Hongsheng Dai and Murray Pollock and Gareth Roberts},
 journal = {Journal of Applied Probability},
 number = {1},
 pages = {174--191},
 publisher = {Applied Probability Trust},
 title = {{Monte Carlo Fusion}},
 urldate = {2024-04-01},
 volume = {56},
 year = {2019}
}


@article{Durmus+2016,
	author = {Alain Durmus and Gersende Fort and {\'E}ric Moulines},
	doi = {10.1214/15-AIHP699},
	journal = {Annales de l'Institut Henri Poincar{\'e}, Probabilit{\'e}s et Statistiques},
	keywords = {Markov chain Monte Carlo in infinite dimension, Markov chains, Subgeometric ergodicity, Wasserstein distance},
	number = {4},
	pages = {1799 -- 1822},
	publisher = {Institut Henri Poincar{\'e}},
	title = {{Subgeometric rates of convergence in Wasserstein distance for Markov chains}},
	url = {https://doi.org/10.1214/15-AIHP699},
	volume = {52},
	year = {2016},
	bdsk-url-1 = {https://doi.org/10.1214/15-AIHP699}}


@article{Butkovsky-Veretennikov2013,
	abstract = {We prove that strong ergodicity of a Markov process is linked with a spectral radius of a certain ``associated'' semigroup operator, although, not a ``natural'' one. We also give sufficient conditions for weak ergodicity and provide explicit estimates of the convergence rate. To establish these results we construct a modification of the Vaserstein coupling. Some applications including mixing properties are also discussed.},
	author = {O.A. Butkovsky and A.Yu. Veretennikov},
	doi = {10.1016/j.spa.2013.04.016},
	issn = {0304-4149},
	journal = {Stochastic Processes and their Applications},
	keywords = {Markov process, Exponential convergence, Polynomial convergence, Vaserstein coupling, Mixing, Strong ergodicity},
	number = {9},
	pages = {3518-3541},
	title = {On asymptotics for Vaserstein coupling of Markov chains},
	url = {https://www.sciencedirect.com/science/article/pii/S0304414913001129},
	volume = {123},
	year = {2013},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0304414913001129},
	bdsk-url-2 = {https://doi.org/10.1016/j.spa.2013.04.016}}


@article{Diaconis-Stroock1991,
	author = {Persi Diaconis and Daniel Stroock},
	doi = {10.1214/aoap/1177005980},
	journal = {The Annals of Applied Probability},
	keywords = {Eigenvalues, Markov chains, Random walk},
	number = {1},
	pages = {36 -- 61},
	publisher = {Institute of Mathematical Statistics},
	title = {{Geometric Bounds for Eigenvalues of Markov Chains}},
	url = {https://doi.org/10.1214/aoap/1177005980},
	volume = {1},
	year = {1991},
	bdsk-url-1 = {https://doi.org/10.1214/aoap/1177005980}}

@article{Doeblin1938,
    author          = {Wolfgang Doeblin},
    year            = {1938},
    title           = {Exposé de la Théorie des Chaînes Simple Constantes de Markov á un Nombre Fini d'États},
    journal         = {Revue Mathematique de l'Union Interbalkanique},
    volume          = {2},
    number          = {},
    pages           = {77-105},
    url             = {}
}

@article{Vaserstein1969,
    author          = {L. N. Vaserstein},
    year            = {1969},
    title           = {Markov processes on countable product spaces describing large systems of automata},
    journal         = {Problemy Peredachi Informatsii},
    volume          = {5},
    number          = {3},
    pages           = {64-72},
    url             = {https://www.mathnet.ru/php/archive.phtml?wshow=paper&jrnid=ppi&paperid=1811&option_lang=eng}
}

@article{Pitman1976,
	author = {Pitman, J.  W. },
	date = {1976/12/01},
	date-added = {2024-04-04 17:37:01 +0900},
	date-modified = {2024-04-04 17:37:01 +0900},
	doi = {10.1007/BF00532957},
	id = {Pitman1976},
	isbn = {1432-2064},
	journal = {Zeitschrift f{\"u}r Wahrscheinlichkeitstheorie und Verwandte Gebiete},
	number = {4},
	pages = {315--322},
	title = {On coupling of Markov chains},
	url = {https://doi.org/10.1007/BF00532957},
	volume = {35},
	year = {1976},
	bdsk-url-1 = {https://doi.org/10.1007/BF00532957}}

@book{Nummelin1984,
    author         = {Esa Nummelin},
    year           = {1984},
    title          = {{General Irreducible Markov Chains and Non-Negative Operators}},
    series         = {Cambridge Tracts in Mathematics},
    volume         = {83},
    edition        = {},
    url            = {https://doi.org/10.1017/CBO9780511526237},
    publisher      = {Cambridge University Press}
}

@article{Griffeath1975a,
	author = {Griffeath, David},
	date = {1975/06/01},
	date-added = {2024-04-04 17:40:09 +0900},
	date-modified = {2024-04-04 17:40:09 +0900},
	doi = {10.1007/BF00539434},
	id = {Griffeath1975},
	isbn = {1432-2064},
	journal = {Zeitschrift f{\"u}r Wahrscheinlichkeitstheorie und Verwandte Gebiete},
	number = {2},
	pages = {95--106},
	title = {A maximal coupling for Markov chains},
	url = {https://doi.org/10.1007/BF00539434},
	volume = {31},
	year = {1975},
	bdsk-url-1 = {https://doi.org/10.1007/BF00539434}}

@book{Lindvall1992,
    author         = {Torgny Lindvall},
    year           = {1992},
    title          = {Lectures on the Coupling method},
    series         = {Wiley Series in Probability and Statistics},
    volume         = {},
    edition        = {},
    url            = {},
    publisher      = {John Wiley \& Sons}
}

@book{Breiman1969,
    author         = {Leo Breiman},
    year           = {1969},
    title          = {Probability and Stochastic Processes: with a View Toward Applications},
    series         = {},
    volume         = {},
    edition        = {},
    url            = {},
    publisher      = {Houghton Miffin}
}

@book{Hoel+1986,
    author         = {Paul G. Hoel and Sidney C. Port and Charles J. Stone},
    year           = {1986},
    title          = {Introduction to Stochastic Processes},
    series         = {},
    volume         = {},
    edition        = {},
    url            = {},
    publisher      = {Waveland Press}
}

@article{Rosenthal2002,
	author = {Jeffrey Rosenthal},
	doi = {10.1214/ECP.v7-1054},
	journal = {Electronic Communications in Probability},
	keywords = {convergence rate, drift condition, Markov chain, minorisation condition, mixing time, total variation distance},
	number = {none},
	pages = {123 -- 128},
	publisher = {Institute of Mathematical Statistics and Bernoulli Society},
	title = {{Quantitative Convergence Rates of Markov Chains: A Simple Account}},
	url = {https://doi.org/10.1214/ECP.v7-1054},
	volume = {7},
	year = {2002},
	bdsk-url-1 = {https://doi.org/10.1214/ECP.v7-1054}}
@article{Tweedie1981,
 ISSN = {00219002},
 URL = {http://www.jstor.org/stable/3213172},
 abstract = {For regular Markov processes on a countable space, we provide criteria for the forms of ergodicity in the title in terms of the existence of solutions to inequalities involving the Q-matrix of the process. An application to birth-death processes is given.},
 author = {R. L. Tweedie},
 journal = {Journal of Applied Probability},
 number = {1},
 pages = {122--130},
 publisher = {Applied Probability Trust},
 title = {Criteria for Ergodicity, Exponential Ergodicity and Strong Ergodicity of Markov Processes},
 urldate = {2024-04-04},
 volume = {18},
 year = {1981}
}

@article{Tweedie1994,
	abstract = {This paper describes the role of continuous components in linking the topological and measuretheoretic (or regenerative) analysis of Markov chains and processes. Under Condition{\$}{\$}{$\backslash$}mathcal{\{}T{\}}{\$}{\$}below we show the following parallel results for both discrete and continuous time models:(i)when the model is open set irreducible it is ϕ-irreducible;(ii)under (i), the measure-theoretic classification of the model as Harris recurrent or positive Harris recurrent is equivalent to a topological classification in terms of not leaving compact sets or of tightness of transition kernels;(iii)under (i), the `global'classification of the model as transient, recurrent or positive recurrent is given by a ``local'classification of any individual reachable point;(iv)under (i), every compact set is a small set, so that through the Nummelin splitting there is pseudo-regeneration within compact sets, and compact sets are `test sets'for stability;(v)even without irreducibility, there is always a Doeblin decomposition into a countable disjoint collection of Harris sets and a transient set. We conclude with a guide to verifying Condition{\$}{\$}{$\backslash$}mathcal{\{}T{\}}{\$}{\$}and indicate that it holds under very mild constraints for a wide range of specific models: in particular a ϕ-irreducible Feller chain satisfies Condition{\$}{\$}{$\backslash$}mathcal{\{}T{\}}{\$}{\$}provided only that the support of ϕhas nonempty interior.},
	author = {Tweedie, R.  L. },
	date = {1994/02/01},
	date-added = {2024-06-05 11:15:28 +0900},
	date-modified = {2024-06-05 11:15:28 +0900},
	doi = {10.1007/BF00994264},
	id = {Tweedie1994},
	isbn = {1572-9036},
	journal = {Acta Applicandae Mathematica},
	number = {1},
	pages = {175--188},
	title = {Topological conditions enabling use of harris methods in discrete and continuous time},
	url = {https://doi.org/10.1007/BF00994264},
	volume = {34},
	year = {1994},
	bdsk-url-1 = {https://doi.org/10.1007/BF00994264}}


@article{Douc+2004,
	author = {R. Douc and E. Moulines and Jeffrey S. Rosenthal},
	doi = {10.1214/105051604000000620},
	journal = {The Annals of Applied Probability},
	keywords = {convergence rate, coupling, f-total variation, Markov chain Monte Carlo, simulated annealing},
	number = {4},
	pages = {1643 -- 1665},
	publisher = {Institute of Mathematical Statistics},
	title = {{Quantitative bounds on convergence of time-inhomogeneous Markov chains}},
	url = {https://doi.org/10.1214/105051604000000620},
	volume = {14},
	year = {2004},
	bdsk-url-1 = {https://doi.org/10.1214/105051604000000620}}

@article{Kalashinikov1973,
    author          = {V. V. Kalashinikov},
    year            = {1973},
    title           = {The Property of $\gamma$-reflexivity for Markov Sequences},
    journal         = {Soviet Mathematics Doklady},
    volume          = {14},
    number          = {},
    pages           = {1869-1873},
    url             = {}
}

@article{Lamperti1960,
	author = {John Lamperti},
	doi = {https://doi.org/10.1016/0022-247X(60)90005-6},
	issn = {0022-247X},
	journal = {Journal of Mathematical Analysis and Applications},
	number = {3},
	pages = {314-330},
	title = {Criteria for the recurrence or transience of stochastic process. I},
	url = {https://www.sciencedirect.com/science/article/pii/0022247X60900056},
	volume = {1},
	year = {1960},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/0022247X60900056},
	bdsk-url-2 = {https://doi.org/10.1016/0022-247X(60)90005-6}}

@misc{Johnston+2024,
      title={Taming the Interacting Particle Langevin Algorithm -- the superlinear case}, 
      author={Tim Johnston and Nikolaos Makras and Sotirios Sabanis},
      year={2024},
      eprint={2403.19587},
      archivePrefix={arXiv},
      primaryClass={math.PR},
      url          = {https://arxiv.org/abs/2403.19587},
}

@article{Chau+2021,
	abstract = { We consider the problem of sampling from a target distribution, which is not necessarily log-concave, in the context of empirical risk minimization and stochastic optimization as presented in [M. Raginsky, A. Rakhlin, and M. Telgarsky, Proc. Mach. Learn. Res., 65 (2017), pp. 1674--1703]. Non-asymptotic results are established in the \$L^1\$-Wasserstein distance for the behavior of stochastic gradient Langevin dynamics algorithms. We allow gradient estimates based on dependent data streams. Our convergence estimates are sharper and uniform in the number of iterations, in contrast to those in previous studies. },
	author = {Chau, Ngoc Huy and Moulines, \'{E}ric and R\'{a}sonyi, Mikl\'{o}s and Sabanis, Sotirios and Zhang, Ying},
	doi = {10.1137/20M1355392},
	eprint = {https://doi.org/10.1137/20M1355392},
	journal = {SIAM Journal on Mathematics of Data Science},
	number = {3},
	pages = {959-986},
	title = {On Stochastic Gradient Langevin Dynamics with Dependent Data Streams: The Fully Nonconvex Case},
	url = {https://doi.org/10.1137/20M1355392},
	volume = {3},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1137/20M1355392}}

@book{Rapaport2004,
    author         = {Dennis C Rapaport},
    year           = {2004},
    title          = {The Art of Molecular Dynamics Simulation},
    series         = {},
    volume         = {},
    edition        = {2},
    url            = {https://www.cambridge.org/core/books/art-of-molecular-dynamics-simulation/57D40C5ECE9B7EA17C0E77E7754F5874},
    publisher      = {Cambridge University Press},
    doi            = {10.1017/CBO9780511816581},
}
@article{Alder-Wainwright1957,
    author = {Alder, B. J. and Wainwright, T. E.},
    title = "{Phase Transition for a Hard Sphere System}",
    journal = {The Journal of Chemical Physics},
    volume = {27},
    number = {5},
    pages = {1208-1209},
    year = {1957},
    month = {11},
    issn = {0021-9606},
    doi = {10.1063/1.1743957},
    url = {https://doi.org/10.1063/1.1743957},
    eprint = {https://pubs.aip.org/aip/jcp/article-pdf/27/5/1208/18812897/1208\_1\_online.pdf},
}

@inproceedings{Alder-Wainwright1958,
    author = {Alder, B. J. and Wainwright, T. E.},
    year            = {1958},
    title           = {Molecular Dynamics by Electronic Computers},
    booktitle       = {Proceedings of the International Symposium on Transport Processes in Statistical Mechanics. Held in Brussels, August 27-31, 1956},
    volume          = {},
    pages           = {97},
    url             = {},
    editor          = {I. Prigogine},
}

@article{Alder-Wainwright1959,
    author = {Alder, B. J. and Wainwright, T. E.},
    title = "{Studies in Molecular Dynamics. I. General Method}",
    journal = {The Journal of Chemical Physics},
    volume = {31},
    number = {2},
    pages = {459-466},
    year = {1959},
    month = {08},
    abstract = "{A method is outlined by which it is possible to calculate exactly the behavior of several hundred interacting classical particles. The study of this many‐body problem is carried out by an electronic computer which solves numerically the simultaneous equations of motion. The limitations of this numerical scheme are enumerated and the important steps in making the program efficient on the computers are indicated. The applicability of this method to the solution of many problems in both equilibrium and nonequilibrium statistical mechanics is discussed.}",
    issn = {0021-9606},
    doi = {10.1063/1.1730376},
    url = {https://doi.org/10.1063/1.1730376},
    eprint = {https://pubs.aip.org/aip/jcp/article-pdf/31/2/459/18817177/459\_1\_online.pdf},
}

@book{Griebel+2007,
    author         = {Michael Griebel and Gerhard Zumbusch and Stephan Knapek},
    year           = {2007},
    title          = {Numerical Simulation in Molecular Dynamics: Numerics, Algorithms, Parallelization, Applications},
    series         = {Texts in Computational Science and Engineering},
    volume         = {5},
    edition        = {},
    url            = {https://link.springer.com/book/10.1007/978-3-540-68095-6},
    publisher      = {Springer Belin, Heidelberg}
}

@book{Liu2004,
    author         = {Jun S. Liu},
    year           = {2004},
    title          = {Monte Carlo Strategies in Scientific Computing},
    series         = {Springer Series in Statistics},
    volume         = {},
    edition        = {},
    url            = {},
    publisher      = {Springer New York},
    doi            = {10.1007/978-0-387-76371-2},
}

@article{Hukushima-Nemoto1996,
author = {Hukushima ,Koji and Nemoto ,Koji},
title = {Exchange Monte Carlo Method and  Application to Spin Glass Simulations},
journal = {Journal of the Physical Society of Japan},
volume = {65},
number = {6},
pages = {1604-1608},
year = {1996},
doi = {10.1143/JPSJ.65.1604},
URL = {https://doi.org/10.1143/JPSJ.65.1604},
}

@article{Duane+1987,
title = {Hybrid Monte Carlo},
journal = {Physics Letters B},
volume = {195},
number = {2},
pages = {216-222},
year = {1987},
issn = {0370-2693},
doi = {10.1016/0370-2693(87)91197-X},
url = {https://www.sciencedirect.com/science/article/pii/037026938791197X},
author = {Simon Duane and A.D. Kennedy and Brian J. Pendleton and Duncan Roweth},
abstract = {We present a new method for the numerical simulation of lattice field theory. A hybrid (molecular dynamics/Langevin) algorithm is used to guide a Monte Carlo simulation. There are no discretization errors even for large step sizes. The method is especially efficient for systems such as quantum chromodynamics which contain fermionic degrees of freedom. Detailed results are presented for four-dimensional compact quantum electrodynamics including the dynamical effects of electrons.}
}
@article{用語解説2022,
  title={用語解説},
  author={栗﨑 and 田中},
  journal={生物物理},
  volume={62},
  number={4},
  pages={250-250},
  year={2022},
  doi={10.2142/biophys.62.250}
}
@article{Torrie-Valleau1977,
title = {Nonphysical sampling distributions in Monte Carlo free-energy estimation: Umbrella sampling},
journal = {Journal of Computational Physics},
volume = {23},
number = {2},
pages = {187-199},
year = {1977},
issn = {0021-9991},
doi = {10.1016/0021-9991(77)90121-8},
url = {https://www.sciencedirect.com/science/article/pii/0021999177901218},
author = {G.M. Torrie and J.P. Valleau},
abstract = {The free energy difference between a model system and some reference system can easily be written as an ensemble average, but the conventional Monte Carlo methods of obtaining such averages are inadequate for the free-energy case. That is because the Boltzmann-weighted sampling distribution ordinarily used is extremely inefficient for the purpose. This paper describes the use of arbitrary sampling distributions chosen to facilitate such estimates. The methods have been tested successfully on the Lennard-Jones system over a wide range of temperature and density, including the gas-liquid coexistence region, and are found to be extremely powerful and economical.}
}


@article{Iba2001b,
	abstract = { "Extended Ensemble Monte Carlo" is a generic term that indicates a set of algorithms, which are now popular in a variety of fields in physics and statistical information processing. Exchange Monte Carlo (Metropolis-Coupled Chain, Parallel Tempering), Simulated Tempering (Expanded Ensemble Monte Carlo) and Multicanonical Monte Carlo (Adaptive Umbrella Sampling) are typical members of this family. Here, we give a cross-disciplinary survey of these algorithms with special emphasis on the great flexibility of the underlying idea. In Sec. 2, we discuss the background of Extended Ensemble Monte Carlo. In Secs. 3, 4 and 5, three types of the algorithms, i.e., Exchange Monte Carlo, Simulated Tempering, Multicanonical Monte Carlo, are introduced. In Sec. 6, we give an introduction to Replica Monte Carlo algorithm by Swendsen and Wang. Strategies for the construction of special-purpose extended ensembles are discussed in Sec. 7. We stress that an extension is not necessary restricted to the space of energy or temperature. Even unphysical (unrealizable) configurations can be included in the ensemble, if the resultant fast mixing of the Markov chain offsets the increasing cost of the sampling procedure. Multivariate (multicomponent) extensions are also useful in many examples. In Sec. 8, we give a survey on extended ensembles with a state space whose dimensionality is dynamically varying. In the appendix, we discuss advantages and disadvantages of three types of extended ensemble algorithms. },
	author = {Iba, Yukito},
	doi = {10.1142/S0129183101001912},
	eprint = {https://doi.org/10.1142/S0129183101001912},
	journal = {International Journal of Modern Physics C},
	number = {05},
	pages = {623-656},
	title = {Extended Ensemble Monte Carlo},
	url = {https://doi.org/10.1142/S0129183101001912},
	volume = {12},
	year = {2001},
	bdsk-url-1 = {https://doi.org/10.1142/S0129183101001912}}

@article{Iba1999,
doi = {10.1088/0305-4470/32/21/302},
url = {https://dx.doi.org/10.1088/0305-4470/32/21/302},
year = {1999},
month = {may},
publisher = {},
volume = {32},
number = {21},
pages = {3875},
author = {Yukito Iba},
title = {{The Nishimori line and Bayesian statistics}},
journal = {Journal of Physics A: Mathematical and General},
abstract = {The `Nishimori line' is a line or hypersurface in the parameter space of systems with quenched disorder, where simple expressions of the averages of physical quantities over the quenched random variables are obtained. It has been playing an important role in the theoretical studies of the random frustrated systems since its discovery in around 1980. In this paper, an interpretation of the Nishimori line from the viewpoint of statistical information processing is developed. Our main aim is the reconstruction of the whole theory of the Nishimori line from the viewpoint of Bayesian statistics, or, almost equivalently, from the viewpoint of the theory of error-correcting codes. As a byproduct of the interpretation, counterparts of the Nishimori line in models without gauge invariance are given. We also discussed the issues on the `finite-temperature decoding' of error-correcting codes and clarify the role of gauge invariance in this topic.}
}

@article{Gilks+1994,
 ISSN = {00390526, 14679884},
 URL = {http://www.jstor.org/stable/2348942},
 abstract = {Markov chain Monte Carlo (MCMC) techniques, such as the Gibbs sampler, are increasingly being used for Bayesian inference. We propose a new MCMC method: adaptive direction sampling (ADS) which, unlike the Gibbs sampler, involves sampling in directions which adapt to the target density. We present non-technically the essence of ADS, but with sufficient detail to allow the practitioner to apply the method. We demonstrate irreducibility of the snooker algorithm, a special case of ADS. We compare the performance of special cases of ADS, including the snooker algorithm and the Gibbs sampler, in a simple test example.},
 author = {W. R. Gilks and G. O. Roberts and E. I. George},
 journal = {Journal of the Royal Statistical Society. Series D (The Statistician)},
 number = {1},
 pages = {179--189},
 publisher = {[Royal Statistical Society, Wiley]},
 title = {Adaptive Direction Sampling},
 urldate = {2024-04-07},
 volume = {43},
 year = {1994}
}
@article{Liang-Wong2000,
 ISSN = {10170405, 19968507},
 URL = {http://www.jstor.org/stable/24306722},
 abstract = {Motivated by the success of genetic algorithms and simulated annealing in hard optimization problems, the authors propose a new Markov chain Monte Carlo (MCMC) algorithm called an evolutionary Monte Carlo algorithm. This algorithm has incorporated several attractive features of genetic algorithms and simulated annealing into the framework of MCMC. It works by simulating a population of Markov chains in parallel, where a different temperature is attached to each chain. The population is updated by mutation (Metropolis update), crossover (partial state swapping) and exchange operators (full state swapping). The algorithm is illustrated through examples of Cp-based model selection and change-point identification. The numerical results and the extensive comparisons show that evolutionary Monte Carlo is a promising approach for simulation and optimization.},
 author = {Faming Liang and Wing Hung Wong},
 journal = {Statistica Sinica},
 number = {2},
 pages = {317--342},
 publisher = {Institute of Statistical Science, Academia Sinica},
 title = {EVOLUTIONARY MONTE CARLO: APPLICATIONS TO C
          p
          MODEL SAMPLING AND CHANGE POINT PROBLEM},
 urldate = {2024-04-07},
 volume = {10},
 year = {2000}
}

@article{Liang-Wong2001,
 ISSN = {01621459},
 URL = {http://www.jstor.org/stable/2670304},
 abstract = {We propose an evolutionary Monte Carlo algorithm to sample from a target distribution with real-valued parameters. The attractive features of the algorithm include the ability to learn from the samples obtained in previous steps and the ability to improve the mixing of a system by sampling along a temperature ladder. The effectiveness of the algorithm is examined through three multimodal examples and Bayesian neural networks. The numerical results confirm that the real-coded evolutionary algorithm is a promising general approach for simulation and optimization.},
 author = {Faming Liang and Wing Hung Wong},
 journal = {Journal of the American Statistical Association},
 number = {454},
 pages = {653--666},
 publisher = {[American Statistical Association, Taylor & Francis, Ltd.]},
 title = {Real-Parameter Evolutionary Monte Carlo with Applications to Bayesian Mixture Models},
 urldate = {2024-04-07},
 volume = {96},
 year = {2001}
}

@article{Berg-Neuhaus1991,
	abstract = {Monte Carlo simulations are discussed for systems of volume V = Ld which undergo a first order phase transition in the finite volume limit. Conventional canonical, local Monte Carlo algorithms suffer from exponentially fast slowing down ≈V2 exp (cLd−1). Here we present a class of multicanonical Monte Carlo algorithms which can reduce the slowing down to a quadratic power law ≈V2.},
	author = {Bernd A. Berg and Thomas Neuhaus},
	doi = {10.1016/0370-2693(91)91256-U},
	issn = {0370-2693},
	journal = {Physics Letters B},
	number = {2},
	pages = {249-253},
	title = {Multicanonical algorithms for first order phase transitions},
	url = {https://www.sciencedirect.com/science/article/pii/037026939191256U},
	volume = {267},
	year = {1991},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/037026939191256U},
	bdsk-url-2 = {https://doi.org/10.1016/0370-2693(91)91256-U}}
@article{Fichthorn-Weinberg1991,
    author = {Fichthorn, Kristen A. and Weinberg, W. H.},
    title = "{Theoretical foundations of dynamical Monte Carlo simulations}",
    journal = {The Journal of Chemical Physics},
    volume = {95},
    number = {2},
    pages = {1090-1096},
    year = {1991},
    month = {07},
    abstract = "{Monte Carlo methods are utilized as computational tools in many areas of chemical physics. In this paper, we present the theoretical basis for a dynamical Monte Carlo method in terms of the theory of Poisson processes. We show that if: (1) a ‘‘dynamical hierarchy’’ of transition probabilities is created which also satisfy the detailed‐balance criterion; (2) time increments upon successful events are calculated appropriately; and (3) the effective independence of various events comprising the system can be achieved, then Monte Carlo methods may be utilized to simulate the Poisson process and both static and dynamic properties of model Hamiltonian systems may be obtained and interpreted consistently.}",
    issn = {0021-9606},
    doi = {10.1063/1.461138},
    url = {https://doi.org/10.1063/1.461138},
    eprint = {https://pubs.aip.org/aip/jcp/article-pdf/95/2/1090/18992969/1090\_1\_online.pdf},
}

@article{Bernard+2009,
  title = {Event-chain Monte Carlo algorithms for hard-sphere systems},
  author = {Bernard, Etienne P. and Krauth, Werner and Wilson, David B.},
  journal = {Phys. Rev. E},
  volume = {80},
  issue = {5},
  pages = {056704},
  numpages = {5},
  year = {2009},
  month = {Nov},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevE.80.056704},
  url = {https://link.aps.org/doi/10.1103/PhysRevE.80.056704}
}
@article{Bernard-Krauth2011,
  title = {Two-Step Melting in Two Dimensions: First-Order Liquid-Hexatic Transition},
  author = {Bernard, Etienne P. and Krauth, Werner},
  journal = {Phys. Rev. Lett.},
  volume = {107},
  issue = {15},
  pages = {155704},
  numpages = {4},
  year = {2011},
  month = {Oct},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevLett.107.155704},
  url = {https://link.aps.org/doi/10.1103/PhysRevLett.107.155704}
}

@book{西森秀稔2003,
    author         = {西森秀稔},
    year           = {2003},
    title          = {スピングラスと連想記憶},
    series         = {岩波講座物理の世界},
    volume         = {},
    edition        = {},
    url            = {https://www.iwanami.co.jp/book/b476276.html},
    publisher      = {岩波書店}
}

@book{Minlos2000,
    author         = {R. A. Minlos},
    year           = {2000},
    title          = {Introduction to Mathematical Statistical Physics},
    series         = {University Lecture Series},
    volume         = {19},
    edition        = {},
    url            = {},
    doi            = {10.1090/ulect/019},
    publisher      = {American Mathematical Society}
}
@inbook{Altieri-Baity-Jesi2024,
   title={An introduction to the theory of spin glasses},
   ISBN={9780323914086},
   url={http://dx.doi.org/10.1016/B978-0-323-90800-9.00249-3},
   DOI={10.1016/b978-0-323-90800-9.00249-3},
   booktitle={Encyclopedia of Condensed Matter Physics},
   publisher={Elsevier},
   author={Altieri, Ada and Baity-Jesi, Marco},
   year={2024},
   pages={361–370} }
@article{Edwards-Anderson1975,
doi = {10.1088/0305-4608/5/5/017},
url = {https://dx.doi.org/10.1088/0305-4608/5/5/017},
year = {1975},
month = {may},
publisher = {},
volume = {5},
number = {5},
pages = {965},
author = {S F Edwards and  P W Anderson},
title = {Theory of spin glasses},
journal = {Journal of Physics F: Metal Physics},
abstract = {A new theory of the class of dilute magnetic alloys, called the spin glasses, is proposed which offers a simple explanation of the cusp found experimentally in the susceptibility. The argument is that because the interaction between the spins dissolved in the matrix oscillates in sign according to distance, there will be no mean ferro- or antiferromagnetism, but there will be a ground state with the spins aligned in definite directions, even if these directions appear to be at random. At the critical temperature the existence of these preferred directions affects the orientation of the spins, leading to a cusp in the susceptibility. This cusp is smoothed by an external field. Although the behaviour at low t needs a quantum mechanical treatment, it is interesting to complete the classical calculations down to t=0. Classically the susceptibility tends to a constant value at t=0, and the specific heat to a constant value.}
}
@misc{Chatterjee2023,
      title={Spin glass phase at zero temperature in the Edwards-Anderson model}, 
      author={Sourav Chatterjee},
      year={2023},
      eprint={2301.04112},
      archivePrefix={arXiv},
      primaryClass={math-ph},
      url            = {https://arxiv.org/abs/2301.04112},
}
@article{Sherrington-Kirkpatrick1975,
  title = {Solvable Model of a Spin-Glass},
  author = {Sherrington, David and Kirkpatrick, Scott},
  journal = {Phys. Rev. Lett.},
  volume = {35},
  issue = {26},
  pages = {1792--1796},
  numpages = {0},
  year = {1975},
  month = {Dec},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevLett.35.1792},
  url = {https://link.aps.org/doi/10.1103/PhysRevLett.35.1792}
}

@article{田中利幸2007,
    author          = {田中利幸},
    year            = {2007},
    title           = {レプリカ法における解析接続について(情報物理学の数学的構造)},
    journal         = {数理解析研究所講究録},
    volume          = {1532},
    number          = {},
    pages           = {118-129},
    url             = {http://hdl.handle.net/2433/58952}
}
@article{Cannella-Mydosh1972,
  title = {Magnetic Ordering in Gold-Iron Alloys},
  author = {Cannella, V. and Mydosh, J. A.},
  journal = {Phys. Rev. B},
  volume = {6},
  issue = {11},
  pages = {4220--4237},
  numpages = {0},
  year = {1972},
  month = {Dec},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevB.6.4220},
  url = {https://link.aps.org/doi/10.1103/PhysRevB.6.4220}
}

@article{Panchenko2012,
   title={The Sherrington-Kirkpatrick Model: An Overview},
   volume={149},
   ISSN={1572-9613},
   url={http://dx.doi.org/10.1007/s10955-012-0586-7},
   DOI={10.1007/s10955-012-0586-7},
   number={2},
   journal={Journal of Statistical Physics},
   publisher={Springer Science and Business Media LLC},
   author={Panchenko, Dmitry},
   year={2012},
   month=sep, pages={362–383} }

@article{Parisi1980,
doi = {10.1088/0305-4470/13/4/009},
url = {https://dx.doi.org/10.1088/0305-4470/13/4/009},
year = {1980},
month = {apr},
publisher = {},
volume = {13},
number = {4},
pages = {L115},
author = {G Parisi},
title = {A sequence of approximated solutions to the S-K model for spin glasses},
journal = {Journal of Physics A: Mathematical and General},
abstract = {In the framework of the new version of the replica theory, a sequence of approximated solutions is computed for the Sherrington-Kirkpatrick model (see Phys. Rev. Lett., vol.35, p.1972, 1975) of spin glasses.}
}


@article{Parisi1981,
	abstract = {If the equilibrium properties of a statistical system are obtained by solving numerically the associated Langevin equation describing the approach to equilibrium, the connected correlation functions can be computed directly with small effort and high precision.},
	author = {G. Parisi},
	doi = {https://doi.org/10.1016/0550-3213(81)90056-0},
	issn = {0550-3213},
	journal = {Nuclear Physics B},
	number = {3},
	pages = {378-384},
	title = {Correlation functions and computer simulations},
	url = {https://www.sciencedirect.com/science/article/pii/0550321381900560},
	volume = {180},
	year = {1981},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/0550321381900560},
	bdsk-url-2 = {https://doi.org/10.1016/0550-3213(81)90056-0}}


@article{都福仁1977,
    author          = {都福仁},
    year            = {1977},
    title           = {スピングラス},
    journal         = {日本物理学会誌},
    volume          = {32},
    number          = {6},
    pages           = {463-473},
    url             = {https://doi.org/10.11316/butsuri1946.32.463}
}

@book{久保亮五2003,
    author         = {久保亮五},
    year           = {2003},
    title          = {新装版統計力学},
    series         = {},
    volume         = {},
    edition        = {},
    url            = {https://www.kyoritsu-pub.co.jp/book/b10011230.html},
    publisher      = {共立出版}
}

@book{田中宏和2019,
    author         = {田中宏和},
    year           = {2019},
    title          = {計算論的神経科学：脳の運動制御・感覚処理機構の理論的理解へ},
    series         = {},
    volume         = {},
    edition        = {},
    url            = {https://www.morikita.co.jp/books/mid/085161},
    publisher      = {森北出版}
}

@article{Bolthausen2014,
	abstract = {We propose an iterative scheme for the solutions of the TAP-equations in the Sherrington--Kirkpatrick model which is shown to converge up to and including the de Almeida--Thouless line. The main tool is a representation of the iterations which reveals an interesting structure of them. This representation does not depend on the temperature parameter, but for temperatures below the de Almeida--Thouless line, it contains a part which does not converge to zero in the limit.},
	author = {Bolthausen, Erwin},
	date = {2014/01/01},
	date-added = {2024-04-08 16:31:48 +0900},
	date-modified = {2024-04-08 16:31:48 +0900},
	doi = {10.1007/s00220-013-1862-3},
	id = {Bolthausen2014},
	isbn = {1432-0916},
	journal = {Communications in Mathematical Physics},
	number = {1},
	pages = {333--366},
	title = {An Iterative Construction of Solutions of the TAP Equations for the Sherrington--Kirkpatrick Model},
	url = {https://doi.org/10.1007/s00220-013-1862-3},
	volume = {325},
	year = {2014},
	bdsk-url-1 = {https://doi.org/10.1007/s00220-013-1862-3}}

@article{Thouless+1977,
	author = {D. J. Thouless and P. W. Anderson and R. G. Palmer},
	doi = {10.1080/14786437708235992},
	journal = {Philosophical Magazine},
	number = {3},
	pages = {593--601},
	title = {Solution of 'Solvable Model of a Spin Glass'},
	volume = {35},
	year = {1977},
    url  = {https://doi.org/10.1080/14786437708235992}
}

@book{Talagrand2003,
    author         = {Michael Talagrand},
    year           = {2003},
    title          = {Spin Glasses: A Challenge for Mathematicians: Cavity and Mean Field Models},
    series         = {},
    volume         = {},
    edition        = {},
    url            = {https://link.springer.com/book/9783540003564},
    publisher      = {Springer Berlin, Heidelberg}
}

@article{Chatterjee2010,
	abstract = {We introduce some applications of Stein's method in the high temperature analysis of spin glasses. Stein's method allows the direct analysis of the Gibbs measure without having to eate a cavity. Another advantage is that it gives limit theorems with total variation error bounds, although the bounds can be suboptimal. A surprising byproduct of our analysis is a relatively transparent explanation of the Thouless--Anderson--Palmer system of equations. Along the way, we develop Stein's method for mixtures of two Gaussian densities.},
	author = {Chatterjee, Sourav},
	date = {2010/11/01},
	date-added = {2024-04-08 16:55:40 +0900},
	date-modified = {2024-04-08 16:55:40 +0900},
	doi = {10.1007/s00440-009-0240-8},
	id = {Chatterjee2010},
	isbn = {1432-2064},
	journal = {Probability Theory and Related Fields},
	number = {3},
	pages = {567--600},
	title = {Spin glasses and Stein's method},
	url = {https://doi.org/10.1007/s00440-009-0240-8},
	volume = {148},
	year = {2010},
	bdsk-url-1 = {https://doi.org/10.1007/s00440-009-0240-8}}


@article{Donoho-Montanari2016,
	abstract = {In a recent article, El Karoui et al. (Proc Natl Acad Sci 110(36):14557--14562, 2013) study the distribution of robust regression estimators in the regime in which the number of parameters p is of the same order as the number of samples n. Using numerical simulations and `highly plausible'heuristic arguments, they unveil a striking new phenomenon. Namely, the regression coefficients contain an extra Gaussian noise component that is not explained by classical concepts such as the Fisher information matrix. We show here that that this phenomenon can be characterized rigorously using techniques that were developed by the authors for analyzing the Lasso estimator under high-dimensional asymptotics. We introduce an approximate message passing (AMP) algorithm to compute M-estimators and deploy state evolution to evaluate the operating characteristics of AMP and so also M-estimates. Our analysis clarifies that the `extra Gaussian noise'encountered in this problem is fundamentally similar to phenomena already studied for regularized least squares in the setting {\$}{\$}n<p{\$}{\$}.},
	author = {David Donoho and Andrea Montanari},
	date = {2016/12/01},
	date-added = {2024-04-08 17:07:35 +0900},
	date-modified = {2024-04-08 17:07:35 +0900},
	doi = {10.1007/s00440-015-0675-z},
	id = {Donoho2016},
	isbn = {1432-2064},
	journal = {Probability Theory and Related Fields},
	number = {3},
	pages = {935--969},
	title = {High dimensional robust M-estimation: asymptotic variance via approximate message passing},
	url = {https://doi.org/10.1007/s00440-015-0675-z},
	volume = {166},
	year = {2016},
	bdsk-url-1 = {https://doi.org/10.1007/s00440-015-0675-z}}

@article{Griffeath1975b,
 ISSN = {00219002},
 URL = {http://www.jstor.org/stable/3212726},
 abstract = {The Markov-Dobrush in condition for (weak) ergodicity of non-homogeneous discrete-time Markov chains, and an analogous criterion for continuous chains, are derived by means of coupling techniques.},
 author = {David Griffeath},
 journal = {Journal of Applied Probability},
 number = {4},
 pages = {753--762},
 publisher = {Applied Probability Trust},
 title = {Uniform Coupling of Non-Homogeneous Markov Chains},
 urldate = {2024-04-11},
 volume = {12},
 year = {1975}
}

@article{Metropolis1987,
    author          = {N. Metropolis},
    year            = {1987},
    title           = {The Beginning of the Monte Carlo Method},
    journal         = {Los Alamos Science Special Issue},
    volume          = {15},
    number          = {},
    pages           = {125-130},
    url             = {https://library.lanl.gov/cgi-bin/getfile?00326866.pdf}
}

@article{Eckhardt1987,
    author          = {Roger Eckhardt},
    year            = {1987},
    title           = {{Stan Ulam, John von Neumann, and the Monte Carlo Method}},
    journal         = {Los Alamos Science Special Issue},
    volume          = {15},
    number          = {},
    pages           = {131-143},
    url             = {http://www-star.st-and.ac.uk/~kw25/teaching/mcrt/MC_history_3.pdf}
}

@misc{Tartero-Krauth2023,
      title={Concepts in Monte Carlo sampling}, 
      author={Gabriele Tartero and Werner Krauth},
      year={2023},
      eprint={2309.03136},
      archivePrefix={arXiv},
      primaryClass={cond-mat.stat-mech},
      url          = {https://arxiv.org/abs/2309.03136},
}

@article{Robert-Casella2011,
 ISSN = {08834237},
 URL = {http://www.jstor.org/stable/23059158},
 abstract = {We attempt to trace the history and development of Markov chain Monte Carlo (MCMC) from its early inception in the late 1940s through its use today. We see how the earlier stages of Monte Carlo (MC, not MCMC) research have led to the algorithms currently in use. More importantly, we see how the development of this methodology has not only changed our solutions to problems, but has changed the way we think about problems.},
 author = {Christian Robert and George Casella},
 journal = {Statistical Science},
 number = {1},
 pages = {102--115},
 publisher = {Institute of Mathematical Statistics},
 title = {A Short History of Markov Chain Monte Carlo: Subjective Recollections from Incomplete Data},
 urldate = {2024-04-18},
 volume = {26},
 year = {2011}
}

@book{砂田利一2004,
    author         = {砂田利一},
    year           = {2004},
    title          = {数学から見た統計学と熱力学},
    series         = {岩波講座物理の世界},
    volume         = {4},
    edition        = {},
    url            = {https://www.iwanami.co.jp/book/b476285.html},
    publisher      = {岩波書店}
}

@book{Khinchin1949,
    author         = {A. Khinchin},
    year           = {1949},
    title          = {Mathematical Foundations of Statistical Mechanics},
    series         = {},
    volume         = {},
    edition        = {},
    url            = {},
    publisher      = {Courier Corporation}
}

@article{Diaconis+2000,
 ISSN = {10505164},
 URL = {http://www.jstor.org/stable/2667319},
 abstract = {We analyze the convergence to stationarity of a simple nonreversible Markov chain that serves as a model for several nonreversible Markov chain sampling methods that are used in practice. Our theoretical and numerical results show that nonreversibility can indeed lead to improvements over the diffusive behavior of simple Markov chain sampling schemes. The analysis uses both probabilistic techniques and an explicit diagonalization.},
 author = {Persi Diaconis and Susan Holmes and Radford M. Neal},
 journal = {The Annals of Applied Probability},
 number = {3},
 pages = {726--752},
 publisher = {Institute of Mathematical Statistics},
 title = {Analysis of a Nonreversible Markov Chain Sampler},
 urldate = {2024-04-19},
 volume = {10},
 year = {2000}
}
@inproceedings{Chen+1999,
author = {Chen, Fang and Lov\'{a}sz, L\'{a}szl\'{o} and Pak, Igor},
title = {Lifting Markov chains to speed up mixing},
year = {1999},
isbn = {1581130678},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/301250.301315},
doi = {10.1145/301250.301315},
booktitle = {Proceedings of the Thirty-First Annual ACM Symposium on Theory of Computing},
pages = {275–281},
numpages = {7},
location = {Atlanta, Georgia, USA},
series = {STOC '99}
}


@article{Lewis-Shedler1979,
	abstract = {Abstract A simple and relatively efficient method for simulating one-dimensional and two-dimensional nonhomogeneous Poisson processes is presented The method is applicable for any rate function and is based on controlled deletion of points in a Poisson process whose rate function dominates the given rate function In its simplest implementation, the method obviates the need for numerical integration of the rate function, for ordering of points, and for generation of Poisson variates.},
	author = {Lewis, P. A. W and Shedler, G. S.},
	doi = {10.1002/nav.3800260304},
	eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/nav.3800260304},
	journal = {Naval Research Logistics Quarterly},
	number = {3},
	pages = {403-413},
	title = {Simulation of nonhomogeneous poisson processes by thinning},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/nav.3800260304},
	volume = {26},
	year = {1979},
	bdsk-url-1 = {https://onlinelibrary.wiley.com/doi/abs/10.1002/nav.3800260304},
	bdsk-url-2 = {https://doi.org/10.1002/nav.3800260304}}


@article{Duane-Kogut1986,
	abstract = {The theory of hybrid stochastic algorithms is developed. A generalized Fokker-Planck equation is derived and is used to prove that the correct equilibrium distribution is generated by the algorithm. Systematic errors following from the discrete time-step used the numerical implementation of the scheme are computed. Hybrid algorithms which simulate lattice gauge theory with dynamical fermions are presented. They are optimized in computer simulations and their systematic errors and efficiencies are studied.},
	author = {S. Duane and J.B. Kogut},
	doi = {10.1016/0550-3213(86)90606-1},
	issn = {0550-3213},
	journal = {Nuclear Physics B},
	number = {3},
	pages = {398-420},
	title = {The theory of hybrid stochastic algorithms},
	url = {https://www.sciencedirect.com/science/article/pii/0550321386906061},
	volume = {275},
	year = {1986},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/0550321386906061},
	bdsk-url-2 = {https://doi.org/10.1016/0550-3213(86)90606-1}}


@article{Duane1985,
	abstract = {The aim of this paper is to shed further light on the relation between two non-standard formulations of field theory: stochastic quantization and the microcanonical ensemble. One involves a first-order (Langevin) differential equation in a fictitious ``time'', and the other a second-order ordinary differential equation. I analyze a scheme which is a particular example of a canonical ensemble, and which reduces to the old schemes in different limits. For a gaussian degree of freedom it turns out that the autocorrelation function in the new scheme undergoes damped harmonic motion, and the scheme is optimized (for numerical simulation) at critical damping. This is a clear improvement over the old limits, which correspond to maximal and zero damping. For non-gaussian systems I argue that the new proposal always represents a significant improvement over a Langevin simulation, and may even improve over the microcanonical method, in which case only a trivial code modification is required. A useful by-product of the discussion is a better estimate of systematic errors in microcanonical simulations.},
	author = {Simon Duane},
	doi = {10.1016/0550-3213(85)90369-4},
	issn = {0550-3213},
	journal = {Nuclear Physics B},
	pages = {652-662},
	title = {Stochastic quantization versus the microcanonical ensemble: Getting the best of both worlds},
	url = {https://www.sciencedirect.com/science/article/pii/0550321385903694},
	volume = {257},
	year = {1985},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/0550321385903694},
	bdsk-url-2 = {https://doi.org/10.1016/0550-3213(85)90369-4}}

@article{Parisi-Wu1981,
  author = "G. Parisi and Yongshi Wu",
  title = {PERTURBATION THEORY WITHOUT GAUGE FIXING},
  journal = "Scientia Sinica",
  year = "1981",
  volume = "24",
  number = "4",
  pages = "483-",
  url = {https://www.sciengine.com/Math%20A0/doi/10.1360/ya1981-24-4-483},
}

@phdthesis{酒井佑士2017,
    author      = {酒井佑士},
    school      = {東京大学},
    title       = {マルコフ連鎖モンテカルロ法における詳細つり合い条件の破れの効果と応用},
    year        = {2017},
    url         = {https://repository.dl.itc.u-tokyo.ac.jp/records/50422},
}

@article{Turitsyn+2011,
title = "{Irreversible Monte Carlo algorithms for Efficient Sampling}",
abstract = "Equilibrium systems evolve according to Detailed Balance (DB). This principle guided the development of Monte Carlo sampling techniques, of which the Metropolis-Hastings (MH) algorithm is the famous representative. It is also known that DB is sufficient but not necessary. We construct irreversible deformation of a given reversible algorithm capable of dramatic improvement of sampling from known distribution. Our transformation modifies transition rates keeping the structure of transitions intact. To illustrate the general scheme we design an Irreversible version of Metropolis-Hastings (IMH) and test it on an example of a spin cluster. Standard MH for the model suffers from critical slowdown, while IMH is free from critical slowdown. Published by Elsevier B.V.",
author = "Turitsyn, {Konstantin S.} and Michael Chertkov and Marija Vucelja",
note = "US Department of Energy at Los Alamos National Laboratory [DE-AC52-06NA25396]The authors are grateful to V. Chernyak, F. Krzakala, J. Machta, D. Shah and T. Witten for inspiring discussions and useful remarks. The work at LANL was carried out under the auspices of the National Nuclear Security Administration of the US Department of Energy at Los Alamos National Laboratory under Contract No. DE-AC52-06NA25396. MC also acknowledges the Weston Visiting Professorship Program supporting his stay at the Weizmann Institute, where part of this work was done.",
year = "2011",
month = feb,
doi = "10.1016/j.physd.2010.10.003",
language = "English",
volume = "240",
pages = "410--414",
journal = "Physica D-Nonlinear Phenomena",
issn = "0167-2789",
publisher = "Elsevier Science",
number = "5-Apr",}

@article{Sugita-Okamoto1999,
	abstract = {We have developed a formulation for molecular dynamics algorithm for the replica-exchange method. The effectiveness of the method for the protein-folding problem is tested with the penta-peptide Met-enkephalin. The method can overcome the multiple-minima problem by exchanging non-interacting replicas of the system at several temperatures. From only one simulation run, one can obtain probability distributions in canonical ensemble for a wide temperature range using multiple-histogram reweighting techniques, which allows the calculation of any thermodynamic quantity as a function of temperature in that range.},
	author = {Yuji Sugita and Yuko Okamoto},
	doi = {https://doi.org/10.1016/S0009-2614(99)01123-9},
	issn = {0009-2614},
	journal = {Chemical Physics Letters},
	number = {1},
	pages = {141-151},
	title = {Replica-exchange molecular dynamics method for protein folding},
	url = {https://www.sciencedirect.com/science/article/pii/S0009261499011239},
	volume = {314},
	year = {1999},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0009261499011239},
	bdsk-url-2 = {https://doi.org/10.1016/S0009-2614(99)01123-9}}

@article{岡本祐幸2010,
	author = {岡本祐幸},
	doi = {10.11436/mssj.12.2_6},
	journal = {アンサンブル},
	number = {2},
	pages = {2_6-2_7},
	title = {「拡張アンサンブル法」の特集にあたって},
	volume = {12},
	year = {2010},
	bdsk-url-1 = {https://doi.org/10.11436/mssj.12.2_6}}


@article{Lyubartsev+1992,
    author = {Lyubartsev, A. P. and Martsinovski, A. A. and Shevkunov, S. V. and Vorontsov‐Velyaminov, P. N.},
    title = "{New approach to Monte Carlo calculation of the free energy: Method of expanded ensembles}",
    journal = {The Journal of Chemical Physics},
    volume = {96},
    number = {3},
    pages = {1776-1783},
    year = {1992},
    month = {02},
    abstract = "{We propose a new effective Monte Carlo (MC) procedure for direct calculation of the free energy in a single MC run. The partition function of the expanded ensemble is introduced including a sum of canonical partition functions with a set of temperatures and additive factors (modification). Random walk in the space of both particle coordinates and temperatures provides calculation of free energy in a wide range of T. The method was applied to a primitive model of electrolyte including the region of low temperatures. In similar way other variants of expanded ensembles are constructed (e.g., over the number of particles N or volume V). Its facilities in quantum statistics (path integral Monte Carlo) and some other applications are also discussed.}",
    issn = {0021-9606},
    doi = {10.1063/1.462133},
    url = {https://doi.org/10.1063/1.462133},
    eprint = {https://pubs.aip.org/aip/jcp/article-pdf/96/3/1776/18997202/1776\_1\_online.pdf},
}

@article{Roberts-Rosenthal1998,
 ISSN = {13697412, 14679868},
 URL = {http://www.jstor.org/stable/2985986},
 abstract = {We consider the optimal scaling problem for proposal distributions in Hastings--Metropolis algorithms derived from Langevin diffusions. We propose an asymptotic diffusion limit theorem and show that the relative efficiency of the algorithm can be characterized by its overall acceptance rate, independently of the target distribution. The asymptotically optimal acceptance rate is 0.574. We show that, as a function of dimension n, the complexity of the algorithm is O(n1/3), which compares favourably with the O(n) complexity of random walk Metropolis algorithms. We illustrate this comparison with some example simulations.},
 author = {Gareth O. Roberts and Jeffrey S. Rosenthal},
 journal = {Journal of the Royal Statistical Society. Series B (Statistical Methodology)},
 number = {1},
 pages = {255--268},
 publisher = {[Royal Statistical Society, Wiley]},
 title = {Optimal Scaling of Discrete Approximations to Langevin Diffusions},
 urldate = {2024-04-22},
 volume = {60},
 year = {1998}
}

@article{Roberts-Rosenthal2016,
 ISSN = {00219002},
 URL = {http://www.jstor.org/stable/43860930},
 abstract = {We connect known results about diffusion limits of Markov chain Monte Carlo (MCMC) algorithms to the computer science notion of algorithm complexity. Our main result states that any weak limit of a Markov process implies a corresponding complexity bound (in an appropriate metric). We then combine this result with previously-known MCMC diffusion limit results to prove that under appropriate assumptions, the random-walk Metropolis algorithm in d dimensions takes O(d) iterations to converge to stationarity, while the Metropolis-adjusted Langevin algorithm takes O(d⅓) iterations to converge to stationarity.},
 author = {Gareth O. Roberts and Jeffrey S. Rosenthal},
 journal = {Journal of Applied Probability},
 number = {2},
 pages = {410--420},
 publisher = {Applied Probability Trust},
 title = {{Complexity Bounds for Markov Chain Monte Carlo Algorithms via Diffusion Limits}},
 urldate = {2024-06-05},
 volume = {53},
 year = {2016}
}


@article{Besag1994,
    author          = {Julian E. Besag},
    year            = {1994},
    title           = {{Comments on ‘Representations of Knowledge in Complex Systems’ by U. Grenander and M. I. Miller}},
    journal         = {Journal of the Royal Statistical Society. Series B (Methodological)},
    volume          = {56},
    number          = {4},
    pages           = {591-592},
    url             = {https://www.jstor.org/stable/2346184}
}

@misc{Corenflos-Finke2024,
      title={Particle-MALA and Particle-mGRAD: Gradient-based MCMC methods for high-dimensional state-space models}, 
      author={Adrien Corenflos and Axel Finke},
      year={2024},
      eprint={2401.14868},
      archivePrefix={arXiv},
      primaryClass={stat.CO},
      url          = {https://arxiv.org/abs/2401.14868},
}


@article{Neal1994,
	abstract = {The probability of accepting a candidate move in the hybrid Monte Carlo algorithm can be increased by considering a transition to be between windows of several states at the beginning and end of the trajectory, with a particular state within the selected window then being chosen according to the Boltzmann probabilities. The detailed balance condition used to justify the algorithm still holds with this procedure, provided the start state is randomly positioned within its window. The new procedure is shown empirically to significantly improve the acceptance rate for a test system of uncoupled oscillators. It also allows expectations to be estimated using data from all states in the windows, rather than just states that are accepted.},
	author = {Radford M. Neal},
	doi = {10.1006/jcph.1994.1054},
	issn = {0021-9991},
	journal = {Journal of Computational Physics},
	number = {1},
	pages = {194-203},
	title = {An Improved Acceptance Procedure for the Hybrid Monte Carlo Algorithm},
	url = {https://www.sciencedirect.com/science/article/pii/S0021999184710540},
	volume = {111},
	year = {1994},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0021999184710540},
	bdsk-url-2 = {https://doi.org/10.1006/jcph.1994.1054}}

@article{Sheu1991,
 ISSN = {00911798},
 URL = {http://www.jstor.org/stable/2244362},
 abstract = {In this paper we study the transition density Pt(x, y) of a nondegenerate diffusion process by using the stochastic control method invented by Fleming and the idea of stochastic parallel translation. We obtain a two-sided estimate for Pt(x, y) as well as some bounds for the derivatives of log Pt(x, y).},
 author = {Shuenn-Jyi Sheu},
 journal = {The Annals of Probability},
 number = {2},
 pages = {538--561},
 publisher = {Institute of Mathematical Statistics},
 title = {Some Estimates of the Transition Density of a Nondegenerate Diffusion Markov Process},
 urldate = {2024-04-23},
 volume = {19},
 year = {1991}
}

@article{Bortz+1975,
title = {A new algorithm for Monte Carlo simulation of Ising spin systems},
journal = {Journal of Computational Physics},
volume = {17},
number = {1},
pages = {10-18},
year = {1975},
issn = {0021-9991},
doi = {https://doi.org/10.1016/0021-9991(75)90060-1},
url = {https://www.sciencedirect.com/science/article/pii/0021999175900601},
author = {A.B. Bortz and M.H. Kalos and J.L. Lebowitz},
abstract = {We describe a new algorithm for Monte Carlo simulation of Ising spin systems and present results of a study comparing the speed of the new technique to that of a standard technique applied to a square lattice of 6400 spins evolving via single spin flips. We find that at temperatures T < Tc, the critical temperature, the new technique is faster than the standard technique, being ten times faster at T = 0.588 Tc. We expect that the new technique will be especially valuable in Monte Carlo simulation of the time evolution of binary alloy systems. The new algorithm is essentially a reorganization of the standard algorithm. It accounts for the a priori probability of changing spins before, rather than after, choosing the spin or spins to change.}
}
@book{Shiryaev2016,
    author         = {Albert N. Shiryaev},
    year           = {2016},
    title          = {Probability-1},
    series         = {Graduate Texts in Mathematics},
    volume         = {95},
    edition        = {3},
    url            = {https://link.springer.com/book/10.1007/978-0-387-72206-1},
    publisher      = {Springer New York}
}

@article{Terada-Toyoizumi2024,
	abstract = {Cortical neurons exhibit highly variable responses over trials and time. Theoretical works posit that this variability arises potentially from chaotic network dynamics of recurrently connected neurons. Here, we demonstrate that chaotic neural dynamics, formed through synaptic learning, allow networks to perform sensory cue integration in a sampling-based implementation. We show that the emergent chaotic dynamics provide neural substrates for generating samples not only of a static variable but also of a dynamical trajectory, where generic recurrent networks acquire these abilities with a biologically plausible learning rule through trial and error. Furthermore, the networks generalize their experience in the stimulus-evoked samples to the inference without partial or all sensory information, which suggests a computational role of spontaneous activity as a representation of the priors as well as a tractable biological computation for marginal distributions. These findings suggest that chaotic neural dynamics may serve for the brain function as a Bayesian generative model.},
	author = {Yu Terada and Taro Toyoizumi},
	doi = {10.1073/pnas.2312992121},
	eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.2312992121},
	journal = {Proceedings of the National Academy of Sciences},
	number = {18},
	pages = {e2312992121},
	title = {Chaotic neural dynamics facilitate probabilistic computations through sampling},
	url = {https://www.pnas.org/doi/abs/10.1073/pnas.2312992121},
	volume = {121},
	year = {2024},
	bdsk-url-1 = {https://www.pnas.org/doi/abs/10.1073/pnas.2312992121},
	bdsk-url-2 = {https://doi.org/10.1073/pnas.2312992121}}


@article{Berkes+2011,
	abstract = {Internal models of the environment optimize as the brain develops. The brain maintains internal models of its environment to interpret sensory inputs and to prepare actions. Although behavioral studies have demonstrated that these internal models are optimally adapted to the statistics of the environment, the neural underpinning of this adaptation is unknown. Using a Bayesian model of sensory cortical processing, we related stimulus-evoked and spontaneous neural activities to inferences and prior expectations in an internal model and predicted that they should match if the model is statistically optimal. To test this prediction, we analyzed visual cortical activity of awake ferrets during development. Similarity between spontaneous and evoked activities increased with age and was specific to responses evoked by natural scenes. This demonstrates the progressive adaptation of internal models to the statistics of natural stimuli at the neural level.},
	author = {Pietro Berkes and Gerg{\H o} Orb{\'a}n and M{\'a}t{\'e} Lengyel and J{\'o}zsef Fiser},
	doi = {10.1126/science.1195870},
	eprint = {https://www.science.org/doi/pdf/10.1126/science.1195870},
	journal = {Science},
	number = {6013},
	pages = {83-87},
	title = {Spontaneous Cortical Activity Reveals Hallmarks of an Optimal Internal Model of the Environment},
	url = {https://www.science.org/doi/abs/10.1126/science.1195870},
	volume = {331},
	year = {2011},
	bdsk-url-1 = {https://www.science.org/doi/abs/10.1126/science.1195870},
	bdsk-url-2 = {https://doi.org/10.1126/science.1195870}}

@book{Rogers-Williams2000,
    author         = {L. C. G. Rogers and David Williams},
    year           = {2000},
    title          = {Diffusions, Markov Processes, and Martingales. Volume I: Foundatinos.},
    series         = {Cambridge Mathematical Library},
    volume         = {},
    edition        = {2},
    url            = {https://www.cambridge.org/core/books/diffusions-markov-processes-and-martingales/188B6A2BAABAF735E61796C3CD18114B},
    publisher      = {Cambridge University Press}
}

@article{Stroock-Varadhan1969,
	author = {Stroock, Daniel W. and Varadhan, S. R. S.},
	doi = {https://doi.org/10.1002/cpa.3160220304},
	eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/cpa.3160220304},
	journal = {Communications on Pure and Applied Mathematics},
	number = {3},
	pages = {345-400},
	title = {Diffusion processes with continuous coefficients, I},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/cpa.3160220304},
	volume = {22},
	year = {1969},
	bdsk-url-1 = {https://onlinelibrary.wiley.com/doi/abs/10.1002/cpa.3160220304},
	bdsk-url-2 = {https://doi.org/10.1002/cpa.3160220304}}

@article{Harris1955,
author = {T. E. Harris},
title = {{On chains of infinite order}},
volume = {5},
journal = {Pacific Journal of Mathematics},
number = {S1},
publisher = {Pacific Journal of Mathematics, A Non-profit Corporation},
pages = {707 -- 724},
year = {1955},
}

@book{Petersen1983,
    author         = {Karl E. Petersen},
    year           = {1983},
    title          = {Ergodic Theory},
    series         = {Cambridge Studies in Advanced Mathematics},
    volume         = {2},
    edition        = {},
    url            = {https://doi.org/10.1017/CBO9780511608728},
    publisher      = {Cambridge University Press}
}

@article{Bierkens-Roberts2017,
 ISSN = {10505164, 21688737},
 URL = {http://www.jstor.org/stable/44249153},
 abstract = {In Turitsyn, Chertkov and Vucelja [Phys. D 240 (2011) 410-414] a nonreversible Markov Chain Monte Carlo (MCMC) method on an augmented state space was introduced, here referred to as Lifted Metropolis-Hastings (LMH). A scaling limit of the magnetization process in the Curie-Weiss model is derived for LMH, as well as for Metropolis–Hastings (MH). The required jump rate in the high (supercritical) temperature regime equals n½ for LMH, which should be compared to n for MH. At the critical temperature, the required jump rate equals n¾ for LMH and n3/2 for MH, in agreement with experimental results of Turitsyn, Chertkov and Vucelja (2011). The scaling limit of LMH turns out to be a nonreversible piecewise deterministic exponentially ergodic "zig-zag" Markov process.},
 author = {Joris Bierkens and Gareth Roberts},
 journal = {The Annals of Applied Probability},
 number = {2},
 pages = {846--882},
 publisher = {Institute of Mathematical Statistics},
 title = {{A Piecewise Deterministic Scaling Limit of Lifted Metropolis-Hastings in the Curie-Weiss Model}},
 urldate = {2024-05-17},
 volume = {27},
 year = {2017}
}
@article{Roberts+1997,
 ISSN = {10505164},
 URL = {http://www.jstor.org/stable/2245134},
 abstract = {This paper considers the problem of scaling the proposal distribution of a multidimensional random walk Metropolis algorithm in order to maximize the efficiency of the algorithm. The main result is a weak convergence result as the dimension of a sequence of target densities, n, converges to ∞. When the proposal variance is appropriately scaled according to n, the sequence of stochastic processes formed by the first component of each Markov chain converges to the appropriate limiting Langevin diffusion process. The limiting diffusion approximation admits a straightforward efficiency maximization problem, and the resulting asymptotically optimal policy is related to the asymptotic acceptance rate of proposed moves for the algorithm. The asymptotically optimal acceptance rate is 0.234 under quite general conditions. The main result is proved in the case where the target density has a symmetric product form. Extensions of the result are discussed.},
 author = {G. O. Roberts and A. Gelman and W. R. Gilks},
 journal = {The Annals of Applied Probability},
 number = {1},
 pages = {110--120},
 publisher = {Institute of Mathematical Statistics},
 title = {{Weak Convergence and Optimal Scaling of Random Walk Metropolis Algorithms}},
 urldate = {2024-05-21},
 volume = {7},
 year = {1997}
}
@incollection{Gelman+1996,
    author = {Gelman , A and Roberts, G O and Gilks, W R},
    isbn = {9780198523567},
    title = "{Efficient Metropolis Jumping Rules}",
    booktitle = "{Bayesian Statistics 5: Proceedings of the Fifth Valencia International Meeting}",
    publisher = {Oxford University Press},
    year = {1996},
    month = {05},
    abstract = "{The algorithm of Metropolis et al. (1953) and its generalizations have been increasingly popular in computational physics and, more recently, statistics, for sampling from intractable multivariate distributions. Much recent research has been devoted to increasing the efficiency of simulation algorithms by altering the jumping rules for Metropolis-like algorithms. We study a very specific question: What are the most efficient symmetric jumping kernels for simulating a normal target distribution using the Metropolis algorithmã We provide a general theoretical result as the dimension of a class of canonical problems goes to ∞ and numerical approximations and simulations for low-dimensional Gaussian target distributions that show that the limiting results provide extremely accurate approximations in six and higher dimensions.}",
    doi = {10.1093/oso/9780198523567.003.0038},
    url = {https://doi.org/10.1093/oso/9780198523567.003.0038},
    eprint = {https://academic.oup.com/book/0/chapter/422210114/chapter-pdf/52447340/isbn-9780198523567-book-part-38.pdf},
}



@article{Tierney1994,
	author = {Luke Tierney},
	doi = {10.1214/aos/1176325750},
	journal = {The Annals of Statistics},
	keywords = {62-04, Gibbs sampler, Metropolis-Hastings algorithm, Monte Carlo, variance reduction},
	number = {4},
	pages = {1701 -- 1728},
	publisher = {Institute of Mathematical Statistics},
	title = {{Markov Chains for Exploring Posterior Distributions}},
	url = {https://doi.org/10.1214/aos/1176325750},
	volume = {22},
	year = {1994},
	bdsk-url-1 = {https://doi.org/10.1214/aos/1176325750}}
@article{Vasdekis-Roberts2022, title={A note on the polynomial ergodicity of the one-dimensional Zig-Zag process}, volume={59}, DOI={10.1017/jpr.2021.97}, number={3}, journal={Journal of Applied Probability}, author={Vasdekis, Giorgos and Roberts, Gareth O.}, year={2022}, pages={895-903}}

@book{樺島祥介2002,
    author         = {樺島祥介},
    year           = {2002},
    title          = {学習と情報の平均場理論},
    series         = {岩波講座 物理の世界 物理と情報},
    volume         = {2},
    edition        = {},
    url            = {https://www.iwanami.co.jp/book/b476277.html},
    publisher      = {岩波書店}
}

@book{田崎晴明2008II,
    author         = {田崎晴明},
    title          = {統計力学II},
    year           = {2008},
    month          = {12},
    publisher      = {培風館},
    series         = {新物理学シリーズ},
    volume         = {38},
    edition        = {},
    howpublished   = {}
}

@article{Glauber1963,
    author = {Glauber, Roy J.},
    title = "{Time‐Dependent Statistics of the Ising Model}",
    journal = {Journal of Mathematical Physics},
    volume = {4},
    number = {2},
    pages = {294-307},
    year = {1963},
    month = {02},
    abstract = "{The individual spins of the Ising model are assumed to interact with an external agency (e.g., a heat reservoir) which causes them to change their states randomly with time. Coupling between the spins is introduced through the assumption that the transition probabilities for any one spin depend on the values of the neighboring spins. This dependence is determined, in part, by the detailed balancing condition obeyed by the equilibrium state of the model. The Markoff process which describes the spin functions is analyzed in detail for the case of a closed N‐member chain. The expectation values of the individual spins and of the products of pairs of spins, each of the pair evaluated at a different time, are found explicitly. The influence of a uniform, time‐varying magnetic field upon the model is discussed, and the frequency‐dependent magnetic susceptibility is found in the weak‐field limit. Some fluctuation‐dissipation theorems are derived which relate the susceptibility to the Fourier transform of the time‐dependent correlation function of the magnetization at equilibrium.}",
    issn = {0022-2488},
    doi = {10.1063/1.1703954},
    url = {https://doi.org/10.1063/1.1703954},
    eprint = {https://pubs.aip.org/aip/jmp/article-pdf/4/2/294/19156949/294\_1\_online.pdf},
}

@inproceedings{Yang2019,
	author = {Yang, Greg},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
	publisher = {Curran Associates, Inc.},
	title = {Wide Feedforward or Recurrent Neural Networks of Any Architecture are Gaussian Processes},
	url = {https://proceedings.neurips.cc/paper_files/paper/2019/file/5e69fda38cda2060819766569fd93aa5-Paper.pdf},
	volume = {32},
	year = {2019},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2019/file/5e69fda38cda2060819766569fd93aa5-Paper.pdf}}

@article{Kennedy-Pendleton1991,
	abstract = {We present the results of an analytic study of the Hybrid Monte Carlo algorithm for free field theory. We calculate the acceptance rate and autocorrelation function as a function of lattice volume, integration step size, and (average) trajectory length. We show that the dynamical critical exponent z can be tuned to unity by a judicious choice of average trajectory length.},
	author = {A.D. Kennedy and Brian Pendleton},
	doi = {https://doi.org/10.1016/0920-5632(91)90893-J},
	issn = {0920-5632},
	journal = {Nuclear Physics B - Proceedings Supplements},
	pages = {118-121},
	title = {Acceptances and autocorrelations in hybrid Monte Carlo},
	url = {https://www.sciencedirect.com/science/article/pii/092056329190893J},
	volume = {20},
	year = {1991},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/092056329190893J},
	bdsk-url-2 = {https://doi.org/10.1016/0920-5632(91)90893-J}}
@Article{Mountain-Thriumalai1994,
  author={Mountain, Raymond D. and Thirumalai, D.},
  title={{Quantative measure of efficiency of Monte Carlo simulations}},
  journal={Physica A: Statistical Mechanics and its Applications},
  year=1994,
  volume={210},
  number={3},
  pages={453-460},
  month={},
  keywords={},
  doi={10.1016/0378-4371(94)9009},
  abstract={An easily applied, physically motivated algorithm for determining the efficiency of Monte Carlo simulations is introduced. The theoretical basis for the algorithm is developed. As an illustration we apply the method to the Lennard-Jones liquid near the triple point. We show that an acceptance ratio of 0.2 is twice as efficient for the purpose of generating a satisfactory sample as is an acceptance ratio of 0.5. There is a strong correlation between the efficiency measure and the diffusion rate of liquid particles during the simulation. We argue that the optimal value of the acceptance ratio is calculable from short Monte Carlo simulations. The method is very general and is applicable to Monte Carlo simulations involving arbitrary potentials.},
  url={https://ideas.repec.org/a/eee/phsmap/v210y1994i3p453-460.html}
}

@techreport{Neal1993,
    author      = {Radford M. Neal},
    institution = {Department of Computer Science, University of Toronto},
    title       = {{Probabilistic Inference using Markov Chain Monte Carlo Methods}},
    year        = {1993},
    url         = {https://glizen.com/radfordneal/review.abstract.html},
}

@article{Bhattacharya1978,
	author = {R. N. Bhattacharya},
	doi = {10.1214/aop/1176995476},
	journal = {The Annals of Probability},
	keywords = {$L$-harmonic functions, Invariant measures, strong Markov property},
	number = {4},
	pages = {541 -- 553},
	publisher = {Institute of Mathematical Statistics},
	title = {{Criteria for Recurrence and Existence of Invariant Measures for Multidimensional Diffusions}},
	url = {https://doi.org/10.1214/aop/1176995476},
	volume = {6},
	year = {1978},
	bdsk-url-1 = {https://doi.org/10.1214/aop/1176995476}}

@book{Ikeda-Watanabe1981,
    author         = {Nobuyuki Ikeda and Shinzo Watanabe},
    year           = {1981},
    title          = {Stochastic Differential Equations and Diffusion Processes},
    series         = {North-Holland Mathematical Library},
    volume         = {24},
    edition        = {},
    url            = {https://doi.org/10.1016/S0924-6509(08)70217-4},
    publisher      = {Elsevier}
}
@article{Meyn-Tweedie1993,
 ISSN = {00018678},
 URL = {http://www.jstor.org/stable/1427521},
 abstract = {In this paper we extend the results of Meyn and Tweedie (1992b) from discrete-time parameter to continuous-parameter Markovian processes Φ evolving on a topological space. We consider a number of stability concepts for such processes in terms of the topology of the space, and prove connections between these and standard probabilistic recurrence concepts. We show that these structural results hold for a major class of processes (processes with continuous components) in a manner analogous to discrete-time results, and that complex operations research models such as storage models with state-dependent release rules, or diffusion models such as those with hypoelliptic generators, have this property. Also analogous to discrete time, 'petite sets', which are known to provide test sets for stability, are here also shown to provide conditions for continuous components to exist. New ergodic theorems for processes with irreducible and countably reducible skeleton chains are derived, and we show that when these conditions do not hold, then the process may be decomposed into an uncountable orbit of skeleton chains.},
 author = {Sean P. Meyn and R. L. Tweedie},
 journal = {Advances in Applied Probability},
 number = {3},
 pages = {487--517},
 publisher = {Applied Probability Trust},
 title = {Stability of Markovian Processes II: Continuous-Time Processes and Sampled Chains},
 urldate = {2024-06-04},
 volume = {25},
 year = {1993}
}

@book{Meyn-Tweedie2009,
    author         = {Sean Meyn and Richard L. Tweedie},
    year           = {2009},
    title          = {Markov Chains and Stochastic Stability},
    series         = {Cambridge Mathematical Library},
    volume         = {},
    edition        = {},
    url            = {https://doi.org/10.1017/CBO9780511626630},
    publisher      = {Cambridge University Press}
}

@book{Wickham2019,
    author         = {Hadley Wickham},
    year           = {2019},
    title          = {Advanced R},
    series         = {The R Series},
    volume         = {},
    edition        = {2},
    url            = {https://adv-r.hadley.nz/index.html},
    publisher      = {Chapman \& Hall}
}
@article{Burkner2017,
	abstract = {The brms package implements Bayesian multilevel models in R using the probabilistic programming language Stan. A wide range of distributions and link functions are supported, allowing users to fit - among others - linear, robust linear, binomial, Poisson, survival, ordinal, zero-inflated, hurdle, and even non-linear models all in a multilevel context. Further modeling options include autocorrelation of the response variable, user defined covariance structures, censored data, as well as meta-analytic standard errors. Prior specifications are flexible and explicitly encourage users to apply prior distributions that actually reflect their beliefs. In addition, model fit can easily be assessed and compared with the Watanabe-Akaike information criterion and leave-one-out cross-validation.},
	author = {B{\"u}rkner, Paul-Christian},
	doi = {10.18637/jss.v080.i01},
	journal = {Journal of Statistical Software},
	number = {1},
	pages = {1--28},
	title = {brms: An R Package for Bayesian Multilevel Models Using Stan},
	url = {https://www.jstatsoft.org/index.php/jss/article/view/v080i01},
	volume = {80},
	year = {2017},
	bdsk-url-1 = {https://www.jstatsoft.org/index.php/jss/article/view/v080i01},
	bdsk-url-2 = {https://doi.org/10.18637/jss.v080.i01}}
@article{Burkner2018,
  author = {Paul-Christian Bürkner},
  title = {{Advanced Bayesian Multilevel Modeling with the R Package
          brms}},
  year = {2018},
  journal = {{The R Journal}},
  doi = {10.32614/RJ-2018-017},
  url = {https://doi.org/10.32614/RJ-2018-017},
  pages = {395--411},
  volume = {10},
  number = {1}
}

@article{Burkner2021,
	abstract = {&amp;lt;p&amp;gt;Item response theory (IRT) is widely applied in the human sciences to model persons' responses on a set of items measuring one or more latent constructs. While several R packages have been developed that implement IRT models, they tend to be restricted to respective pre-specified classes of models. Further, most implementations are frequentist while the availability of Bayesian methods remains comparably limited. I demonstrate how to use the R package brms together with the probabilistic programming language Stan to specify and fit a wide range of Bayesian IRT models using flexible and intuitive multilevel formula syntax. Further, item and person parameters can be related in both a linear or non-linear manner. Various distributions for categorical, ordinal, and continuous responses are supported. Users may even define their own custom response distribution for use in the presented framework. Common IRT model classes that can be specified natively in the presented framework include 1PL and 2PL logistic models optionally also containing guessing parameters, graded response and partial credit ordinal models, as well as drift diffusion models of response times coupled with binary decisions. Posterior distributions of item and person parameters can be conveniently extracted and postprocessed. Model fit can be evaluated and compared using Bayes factors and efficient cross-validation procedures.&amp;lt;/p&amp;gt;},
	author = {B{\"u}rkner, Paul-Christian},
	doi = {10.18637/jss.v100.i05},
	journal = {Journal of Statistical Software},
	number = {5},
	pages = {1--54},
	title = {Bayesian Item Response Modeling in R with brms and Stan},
	url = {https://www.jstatsoft.org/index.php/jss/article/view/v100i05},
	volume = {100},
	year = {2021},
	bdsk-url-1 = {https://www.jstatsoft.org/index.php/jss/article/view/v100i05},
	bdsk-url-2 = {https://doi.org/10.18637/jss.v100.i05}}
@article{Aalen1978,
    author          = {Odd Aalen},
    year            = {1978},
    title           = {Nonparametric Inference for a Family of Counting Processes},
    journal         = {The Annals of Statistics},
    volume          = {6},
    number          = {4},
    pages           = {701-726},
    url             = {https://www.jstor.org/stable/2958850}
}

@book{Andersen+1993,
    author         = {Per Kragh Andersen and Ørnulf Borgan and Richard D. Gill and Niels Keiding},
    year           = {1993},
    title          = {Statistical Models Based on Counting Processes},
    series         = {Springer Series in Statistics},
    volume         = {},
    edition        = {},
    url            = {https://doi.org/10.1007/978-1-4612-4348-9},
    publisher      = {Springer New York}
}


@article{Cox1955,
	abstract = {SUMMARY The paper deals with a number of problems of statistical analysis connected with events occurring haphazardly in space or time. The topics discussed include: tests of randomness, components of variance, the correlation between events of different types, and a modification of the snap-round method used in operational research.},
	author = {Cox, D. R.},
	doi = {https://doi.org/10.1111/j.2517-6161.1955.tb00188.x},
	eprint = {https://rss.onlinelibrary.wiley.com/doi/pdf/10.1111/j.2517-6161.1955.tb00188.x},
	journal = {Journal of the Royal Statistical Society: Series B (Methodological)},
	number = {2},
	pages = {129-157},
	title = {Some Statistical Methods Connected with Series of Events},
	url = {https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/j.2517-6161.1955.tb00188.x},
	volume = {17},
	year = {1955},
	bdsk-url-1 = {https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/j.2517-6161.1955.tb00188.x},
	bdsk-url-2 = {https://doi.org/10.1111/j.2517-6161.1955.tb00188.x}}


@book{Mitov-Omey2014,
    author         = {Kosto V. Mitov and Edward Omey},
    year           = {2014},
    title          = {Renewal Processes},
    series         = {Springer Briefs in Statistics},
    volume         = {},
    edition        = {},
    url            = {https://doi.org/10.1007/978-3-319-05855-9},
    publisher      = {Springer Cham}
}
@book{Resnick2002,
    author         = {Sidney I. Resnick},
    year           = {2002},
    title          = {Adventures in Stochastic Processes},
    series         = {},
    volume         = {},
    edition        = {},
    url            = {https://doi.org/10.1007/978-1-4612-0387-2},
    publisher      = {Birkhäuser Boston}
}

@article{Thall-Vail1990,
 ISSN = {0006341X, 15410420},
 URL = {http://www.jstor.org/stable/2532086},
 abstract = {A family of covariance models for longitudinal counts with predictive covariates is presented. These models account for overdispersion, heteroscedasticity, and dependence among repeated observations. The approach is a quasi-likelihood regression similar to the formulation given by Liang and Zeger (1986, Biometrika 73, 13-22). Generalized estimating equations for both the covariate parameters and the variance-covariance parameters are presented. Large-sample properties of the parameter estimates are derived. The proposed methods are illustrated by an analysis of epileptic seizure count data arising from a study of progabide as an adjuvant therapy for partial seizures.},
 author = {Peter F. Thall and Stephen C. Vail},
 journal = {Biometrics},
 number = {3},
 pages = {657--671},
 publisher = {[Wiley, International Biometric Society]},
 title = {Some Covariance Models for Longitudinal Count Data with Overdispersion},
 urldate = {2024-06-11},
 volume = {46},
 year = {1990}
}

@article{Vehtari+2021,
	author = {Aki Vehtari and Andrew Gelman and Daniel Simpson and Bob Carpenter and Paul-Christian B{\"u}rkner},
	doi = {10.1214/20-BA1221},
	journal = {Bayesian Analysis},
	number = {2},
	pages = {667 -- 718},
	publisher = {International Society for Bayesian Analysis},
	title = {{Rank-Normalization, Folding, and Localization: An Improved $\widehat{R}$ for Assessing Convergence of MCMC (with Discussion)}},
	url = {https://doi.org/10.1214/20-BA1221},
	volume = {16},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1214/20-BA1221}}

@article{Bafumi-Gelman2007,
    author          = {Joseph Bafumi and Andrew Gelman},
    year            = {2007},
    title           = {{Fitting Multilevel Models When Predictors and Group Effects Correlate}},
    journal         = {SSRN},
    volume          = {},
    number          = {},
    pages           = {},
    url             = {https://dx.doi.org/10.2139/ssrn.1010095}
}

@inproceedings{Kincaid2005,
    author          = {Charles D. Kincaid},
    year            = {2005},
    title           = {{Guidelines for Selecting the Covariance Structure in Mixed Model Analysis}},
    booktitle       = {SAS User Group International},
    volume          = {30},
    pages           = {198-30},
    url             = {https://support.sas.com/resources/papers/proceedings/proceedings/sugi30/198-30.pdf}
}


@article{Chung+2015,
	abstract = { When fitting hierarchical regression models, maximum likelihood (ML) estimation has computational (and, for some users, philosophical) advantages compared to full Bayesian inference, but when the number of groups is small, estimates of the covariance matrix (Σ) of group-level varying coefficients are often degenerate. One can do better, even from a purely point estimation perspective, by using a prior distribution or penalty function. In this article, we use Bayes modal estimation to obtain positive definite covariance matrix estimates. We recommend a class of Wishart (not inverse-Wishart) priors for Σ with a default choice of hyperparameters, that is, the degrees of freedom are set equal to the number of varying coefficients plus 2, and the scale matrix is the identity matrix multiplied by a value that is large relative to the scale of the problem. This prior is equivalent to independent gamma priors for the eigenvalues of Σ with shape parameter 1.5 and rate parameter close to 0. It is also equivalent to independent gamma priors for the variances with the same hyperparameters multiplied by a function of the correlation coefficients. With this default prior, the posterior mode for Σ is always strictly positive definite. Furthermore, the resulting uncertainty for the fixed coefficients is less underestimated than under classical ML or restricted maximum likelihood estimation. We also suggest an extension of our method that can be used when stronger prior information is available for some of the variances or correlations. },
	author = {Yeojin Chung and Andrew Gelman and Sophia Rabe-Hesketh and Jingchen Liu and Vincent Dorie},
	doi = {10.3102/1076998615570945},
	eprint = {https://doi.org/10.3102/1076998615570945},
	journal = {Journal of Educational and Behavioral Statistics},
	number = {2},
	pages = {136-157},
	title = {Weakly Informative Prior for Point Estimation of Covariance Matrices in Hierarchical Models},
	url = {https://doi.org/10.3102/1076998615570945},
	volume = {40},
	year = {2015},
	bdsk-url-1 = {https://doi.org/10.3102/1076998615570945}}


@article{Chung+2013,
	abstract = {Group-level variance estimates of zero often arise when fitting multilevel or hierarchical linear models, especially when the number of groups is small. For situations where zero variances are implausible a priori, we propose a maximum penalized likelihood approach to avoid such boundary estimates. This approach is equivalent to estimating variance parameters by their posterior mode, given a weakly informative prior distribution. By choosing the penalty from the log-gamma family with shape parameter greater than 1, we ensure that the estimated variance will be positive. We suggest a default log-gamma(2,λ) penalty with λ→0, which ensures that the maximum penalized likelihood estimate is approximately one standard error from zero when the maximum likelihood estimate is zero, thus remaining consistent with the data while being nondegenerate. We also show that the maximum penalized likelihood estimator with this default penalty is a good approximation to the posterior median obtained under a noninformative prior.},
	author = {Chung, Yeojin and Rabe-Hesketh, Sophia and Dorie, Vincent and Gelman, Andrew and Liu, Jingchen},
	date = {2013/10/01},
	date-added = {2024-06-12 15:07:41 +0900},
	date-modified = {2024-06-12 15:07:41 +0900},
	doi = {10.1007/s11336-013-9328-2},
	id = {Chung2013},
	isbn = {1860-0980},
	journal = {Psychometrika},
	number = {4},
	pages = {685--709},
	title = {A Nondegenerate Penalized Likelihood Estimator for Variance Parameters in Multilevel Models},
	url = {https://doi.org/10.1007/s11336-013-9328-2},
	volume = {78},
	year = {2013},
	bdsk-url-1 = {https://doi.org/10.1007/s11336-013-9328-2}}

@article{Harville1977,
	author = {David A. Harville},
	doi = {10.1080/01621459.1977.10480998},
	eprint = {https://www.tandfonline.com/doi/pdf/10.1080/01621459.1977.10480998},
	journal = {Journal of the American Statistical Association},
	number = {358},
	pages = {320--338},
	publisher = {Taylor \& Francis},
	title = {Maximum Likelihood Approaches to Variance Component Estimation and to Related Problems},
	url = {https://www.tandfonline.com/doi/abs/10.1080/01621459.1977.10480998},
	volume = {72},
	year = {1977},
	bdsk-url-1 = {https://www.tandfonline.com/doi/abs/10.1080/01621459.1977.10480998},
	bdsk-url-2 = {https://doi.org/10.1080/01621459.1977.10480998}}
@article{Laird-Ware1982,
 ISSN = {0006341X, 15410420},
 URL = {http://www.jstor.org/stable/2529876},
 abstract = {Models for the analysis of longitudinal data must recognize the relationship between serial observations on the same unit. Multivariate models with general covariance structure are often difficult to apply to highly unbalanced data, whereas two-stage random-effects models can be used easily. In two-stage models, the probability distributions for the response vectors of different individuals belong to a single family, but some random-effects parameters vary across individuals, with a distribution specified at the second stage. A general family of models is discussed, which includes both growth models and repeated-measures models as special cases. A unified approach to fitting these models, based on a combination of empirical Bayes and maximum likelihood estimation of model parameters and using the EM algorithm, is discussed. Two examples are taken from a current epidemiological study of the health effects of air pollution.},
 author = {Nan M. Laird and James H. Ware},
 journal = {Biometrics},
 number = {4},
 pages = {963--974},
 publisher = {[Wiley, International Biometric Society]},
 title = {Random-Effects Models for Longitudinal Data},
 urldate = {2024-06-12},
 volume = {38},
 year = {1982}
}

@article{Leppik+1987,
	abstract = {The results of a multicenter, double-blind, placebo-controlled clinical trial of the efficacy and safety of progabide (PGB) in the treatment of partial seizures are presented. This study was performed with a number of rigorous controls not usually present in clinical trials. These included uniform co-medication in which all patients received only phenytoin and carbamazepine; concentrations of these two drugs were maintained within narrow, predefined concentration ranges. There was no statistically significant difference between PGB and placebo in seizure frequency and seizure duration for most of the analyses performed. One patient was withdrawn from the study because of hepatotoxicity. PGB was associated with a significant inhibition of phenytoin but not carbamazepine clearance. The results of this study indicate that PGB was not a potent antiepileptic drug in this population of persons with intractable epilepsy.},
	author = {I. E. Leppik and F. E. Dreifuss and R. Porter and T. Bowman and N. Santilli and M. Jacobs and C. Crosby and J. Cloyd and J. Stackman and N. Graves and T. Sutula and T. Welty and J. Vickery and R. Brundage and J. Gates and R. J. Gumnit and A. Gutierrez},
	doi = {10.1212/WNL.37.6.963},
	eprint = {https://www.neurology.org/doi/pdf/10.1212/WNL.37.6.963},
	journal = {Neurology},
	number = {6},
	pages = {963-963},
	title = {A controlled study of progabide in partial seizures},
	url = {https://www.neurology.org/doi/abs/10.1212/WNL.37.6.963},
	volume = {37},
	year = {1987},
	bdsk-url-1 = {https://www.neurology.org/doi/abs/10.1212/WNL.37.6.963},
	bdsk-url-2 = {https://doi.org/10.1212/WNL.37.6.963}}
@article{Liang-Zeger1986,
 ISSN = {00063444},
 URL = {http://www.jstor.org/stable/2336267},
 abstract = {This paper proposes an extension of generalized linear models to the analysis of longitudinal data. We introduce a class of estimating equations that give consistent estimates of the regression parameters and of their variance under mild assumptions about the time dependence. The estimating equations are derived without specifying the joint distribution of a subject's observations yet they reduce to the score equations for multivariate Gaussian outcomes. Asymptotic theory is presented for the general class of estimators. Specific cases in which we assume independence, m-dependence and exchangeable correlation structures from each subject are discussed. Efficiency of the proposed estimators in two simple situations is considered. The approach is closely related to quasi-likelihood.},
 author = {Kung-Yee Liang and Scott L. Zeger},
 journal = {Biometrika},
 number = {1},
 pages = {13--22},
 publisher = {[Oxford University Press, Biometrika Trust]},
 title = {Longitudinal Data Analysis Using Generalized Linear Models},
 urldate = {2024-06-12},
 volume = {73},
 year = {1986}
}

@article{Hubbard+2010,
	author = {Hubbard, Alan E. and Ahern, Jennifer and Fleischer, Nancy L. and Laan, Mark Van der and Lippman, Sheri A. and Jewell, Nicholas and Bruckner, Tim and Satariano, William A.},
	date-added = {2024-06-12 20:16:33 +0900},
	date-modified = {2024-06-12 20:16:33 +0900},
	id = {00001648-201007000-00007},
	isbn = {1044-3983},
	journal = {Epidemiology},
	n2 = {Abstract:  Two modeling approaches are commonly used to estimate the associations between neighborhood characteristics and individual-level health outcomes in multilevel studies (subjects within neighborhoods). Random effects models (or mixed models) use maximum likelihood estimation. Population average models typically use a generalized estimating equation (GEE) approach. These methods are used in place of basic regression approaches because the health of residents in the same neighborhood may be correlated, thus violating independence assumptions made by traditional regression procedures. This violation is particularly relevant to estimates of the variability of estimates. Though the literature appears to favor the mixed-model approach, little theoretical guidance has been offered to justify this choice. In this paper, we review the assumptions behind the estimates and inference provided by these 2 approaches. We propose a perspective that treats regression models for what they are in most circumstances: reasonable approximations of some true underlying relationship. We argue in general that mixed models involve unverifiable assumptions on the data-generating distribution, which lead to potentially misleading estimates and biased inference. We conclude that the estimation-equation approach of population average models provides a more useful approximation of the truth.},
	number = {4},
	title = {To GEE or Not to GEE: Comparing Population Average and Mixed Models for Estimating the Associations Between Neighborhood Risk Factors and Health},
	url = {https://journals.lww.com/epidem/fulltext/2010/07000/to_gee_or_not_to_gee__comparing_population_average.7.aspx},
	volume = {21},
	year = {2010},
	bdsk-url-1 = {https://journals.lww.com/epidem/fulltext/2010/07000/to_gee_or_not_to_gee__comparing_population_average.7.aspx}}
@article{Gardiner+2009,
author = {Gardiner, Joseph C. and Luo, Zhehui and Roman, Lee Anne},
title = {Fixed effects, random effects and GEE: What are the differences?},
journal = {Statistics in Medicine},
volume = {28},
number = {2},
pages = {221-239},
keywords = {linear mixed model, generalized linear mixed model, random effects, fixed effects, robust variance, conditional maximum likelihood, Hausman test, CES-D},
doi = {https://doi.org/10.1002/sim.3478},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.3478},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.3478},
abstract = {Abstract For analyses of longitudinal repeated-measures data, statistical methods include the random effects model, fixed effects model and the method of generalized estimating equations. We examine the assumptions that underlie these approaches to assessing covariate effects on the mean of a continuous, dichotomous or count outcome. Access to statistical software to implement these models has led to widespread application in numerous disciplines. However, careful consideration should be paid to their critical assumptions to ascertain which model might be appropriate in a given setting. To illustrate similarities and differences that might exist in empirical results, we use a study that assessed depressive symptoms in low-income pregnant women using a structured instrument with up to five assessments that spanned the pre-natal and post-natal periods. Understanding the conceptual differences between the methods is important in their proper application even though empirically they might not differ substantively. The choice of model in specific applications would depend on the relevant questions being addressed, which in turn informs the type of design and data collection that would be relevant. Copyright © 2008 John Wiley \& Sons, Ltd.},
year = {2009}
}
@article{Vaida-Blanchard2005,
 ISSN = {00063444},
 URL = {http://www.jstor.org/stable/20441193},
 abstract = {This paper focuses on the Akaike information criterion, AIC, for linear mixed-effects models in the analysis of clustered data. We make the distinction between questions regarding the population and questions regarding the particular clusters in the data. We show that the AIC in current use is not appropriate for the focus on clusters, and we propose instead the conditional Akaike information and its corresponding criterion, the conditional AIC, cAIC. The penalty term in cAIC is related to the effective degrees of freedom ρ for a linear mixed model proposed by Hodges & Sargent (2001); ρ reflects an intermediate level of complexity between a fixed-effects model with no cluster effect and a corresponding model with fixed cluster effects. The cAIC is defined for both maximum likelihood and residual maximum likelihood estimation. A pharmacokinetics data application is used to illuminate the distinction between the two inference settings, and to illustrate the use of the conditional AIC in model selection.},
 author = {Florin Vaida and Suzette Blanchard},
 journal = {Biometrika},
 number = {2},
 pages = {351--370},
 publisher = {[Oxford University Press, Biometrika Trust]},
 title = {Conditional Akaike Information for Mixed-Effects Models},
 urldate = {2024-06-12},
 volume = {92},
 year = {2005}
}
@article{Bulmer1974,
 ISSN = {0006341X, 15410420},
 URL = {http://www.jstor.org/stable/2529621},
 abstract = {An extension of MacArthur's "broken stick" model is proposed to explain why species abundances should be lognormally distributed. A method of fitting the compound Poisson lognormal distribution by maximum likelihood is described; a computer program is available for performing the calculations. It is shown how the information theory measure of species diversity can be estimated from the parameters of the fitted distribution.},
 author = {M. G. Bulmer},
 journal = {Biometrics},
 number = {1},
 pages = {101--110},
 publisher = {[Wiley, International Biometric Society]},
 title = {On Fitting the Poisson Lognormal Distribution to Species-Abundance Data},
 urldate = {2024-06-16},
 volume = {30},
 year = {1974}
}

@book{Iacus-Yoshida2018,
    author         = {Stefano M. Iacus and Nakahiro Yoshida},
    year           = {2018},
    title          = {Simulation and Inference for Stochastic Processes with YUIMA: A Comprehensive R Framework for SDEs and Other Stochastic Processes},
    series         = {Use R!},
    volume         = {},
    edition        = {},
    url            = {https://doi.org/10.1007/978-3-319-55569-0},
    publisher      = {Springer Cham}
}

@book{Abraham+1988,
    author         = {Ralph Abraham and Jerrold E. Marsden and Tudor Ratiu},
    year           = {1988},
    title          = {Manifolds, Tensor Analysis, and Applications},
    series         = {Applied Mathematical Sciences},
    volume         = {75},
    edition        = {2},
    url            = {https://doi.org/10.1007/978-1-4612-1029-0},
    publisher      = {Springer New York}
}@article{Faulkner-Livingstone2024,
author = {Michael F. Faulkner and Samuel Livingstone},
title = {{Sampling Algorithms in Statistical Physics: A Guide for Statistics and Machine Learning}},
volume = {39},
journal = {Statistical Science},
number = {1},
publisher = {Institute of Mathematical Statistics},
pages = {137 -- 164},
keywords = {event chain Monte Carlo, Glauber dynamics, hard-disk model, hybrid Monte Carlo, Ising model, Langevin dynamics, Markov chain Monte Carlo, Metropolis, molecular dynamics, molecular simulation, Potts model, sampling algorithms, statistical physics, XY model},
year = {2024},
doi = {10.1214/23-STS893},
URL = {https://doi.org/10.1214/23-STS893}
}


@article{Guidotti2022,
 title={calculus: High-Dimensional Numerical and Symbolic Calculus in R},
 volume={104},
 url={https://www.jstatsoft.org/index.php/jss/article/view/v104i05},
 doi={10.18637/jss.v104.i05},
 abstract={&amp;lt;p&amp;gt;The R package calculus implements C++-optimized functions for numerical and symbolic calculus, such as the Einstein summing convention, fast computation of the LeviCivita symbol and generalized Kronecker delta, Taylor series expansion, multivariate Hermite polynomials, high-order derivatives, ordinary differential equations, differential operators and numerical integration in arbitrary orthogonal coordinate systems. The library applies numerical methods when working with functions, or symbolic programming when working with characters or expressions. The package handles multivariate numerical calculus in arbitrary dimensions and coordinates. It implements the symbolic counterpart of the numerical methods whenever possible, without depending on external computer algebra systems. Except for Rcpp, the package has no strict dependencies in order to provide a stable self-contained toolbox that invites re-use.&amp;lt;/p&amp;gt;},
 number={5},
 journal={Journal of Statistical Software},
 author={Guidotti, Emanuele},
 year={2022},
 pages={1–37}
}
@misc{Storopoli2021,
  author = {Storopoli, Jose},
  title = {Bayesian Statistics with Julia and Turing},
  url = {https://storopoli.io/Bayesian-Julia},
  year = {2021}
}

@unpublished{Krzakala-Zdeborova2024,
    author = {Florent Krazakala and Lenka Zdeborová},
    year   = {2024},
    title  = {Statistical Physics Methods in Optimization and Machine Learning},
    url    = {https://sphinxteam.github.io/EPFLDoctoralLecture2021/Notes.pdf}
}

@techreport{Stein1972,
    author      = {Charles Stein},
    institution = {Stanford University},
    title       = {{A Bound for the Error in the Normal Approximation to the Distribution of a Sum of Dependent Random Variables}},
    year        = {1972},
    url         = {https://purl.stanford.edu/jc818sv7483},
}

@article{西森1980,
doi = {10.1088/0022-3719/13/21/012},
url = {https://dx.doi.org/10.1088/0022-3719/13/21/012},
year = {1980},
month = {jul},
publisher = {},
volume = {13},
number = {21},
pages = {4071},
author = {H Nishimori},
title = {Exact results and critical properties of the Ising model with competing interactions},
journal = {Journal of Physics C: Solid State Physics},
abstract = {The random Ising model with competing interactions is investigated on the basis of the gauge-invariant formulation of the problem. Exact results for the internal energy, specific heat and gauge-invariant correlation function are derived. The critical exponent alpha is shown to be negative at the phase boundary of the paramagnetic and ferromagnetic phases if the latter exists at fairly low concentration of antiferromagnetic bonds.}
}
@article{Derrida1980,
  title = {Random-Energy Model: Limit of a Family of Disordered Models},
  author = {Derrida, B.},
  journal = {Phys. Rev. Lett.},
  volume = {45},
  issue = {2},
  pages = {79--82},
  numpages = {0},
  year = {1980},
  month = {Jul},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevLett.45.79},
  url = {https://link.aps.org/doi/10.1103/PhysRevLett.45.79}
}
@ARTICLE{Guo+2005,
  author={Dongning Guo and Shamai, S. and Verdu, S.},
  journal={IEEE Transactions on Information Theory}, 
  title={Mutual information and minimum mean-square error in Gaussian channels}, 
  year={2005},
  volume={51},
  number={4},
  pages={1261-1282},
  keywords={Mutual information;Gaussian channels;Signal to noise ratio;Additive noise;Gaussian noise;Network address translation;Statistics;Power filters;Filtering;Smoothing methods;Gaussian channel;minimum mean-square error (MMSE);mutual information;nonlinear filtering;optimal estimation;smoothing;Wiener process},
  doi={10.1109/TIT.2005.844072}}

@article{樺島祥介2003,
    author          = {樺島祥介},
    year            = {2003},
    title           = {コトの物理学：誤り訂正符号を例として},
    journal         = {日本物理学会誌},
    volume          = {58},
    number          = {4},
    pages           = {239-246},
    url             = {https://doi.org/10.11316/butsuri1946.58.239}
}
@article{樺島-杉浦2008,
    author          = {樺島祥介 and 杉浦正康},
    year            = {2008},
    title           = {コトの物理学},
    journal         = {物性研究},
    volume          = {91},
    number          = {1},
    pages           = {1-33},
    url             = {http://hdl.handle.net/2433/142666}
}
@article{Sourlas1989,
	abstract = {DURING the transmission of information, errors may occur because of the presence of noise, such as thermal noise in electronic signals or interference with other sources of radiation. One wants to recover the information with the minimum error possible. In theory this is possible by increasing the power of the emitter source. But as the cost is proportional to the energy fed into the channel, it costs less to code the message before sending it, thus including redundant 'coding' bits, and to decode at the end. Coding theory provides rigorous bounds on the cost-effectiveness of any code. The explicit codes proposed so far for practical applications do not saturate these bounds; that is, they do not achieve optimal cost-efficiency. Here we show that theoretical models of magnetically disordered materials (spin glasses) provide a new class of error-correction codes. Their cost performance can be calculated using the methods of statistical mechanics, and is found to be excellent. These models can, under certain circumstances, constitute the first known codes to saturate Shannon's well-known cost-performance bounds.},
	author = {Sourlas, Nicolas},
	date = {1989/06/01},
	date-added = {2024-06-23 19:34:20 +0900},
	date-modified = {2024-06-23 19:34:20 +0900},
	doi = {10.1038/339693a0},
	id = {Sourlas1989},
	isbn = {1476-4687},
	journal = {Nature},
	number = {6227},
	pages = {693--695},
	title = {Spin-glass models as error-correcting codes},
	url = {https://doi.org/10.1038/339693a0},
	volume = {339},
	year = {1989},
	bdsk-url-1 = {https://doi.org/10.1038/339693a0}}
@book{西森秀稔1999,
    author         = {西森秀稔},
    year           = {1999},
    month          = {11},
    title          = {スピングラス理論と情報統計力学},
    series         = {新物理学選書},
    volume         = {},
    edition        = {},
    url            = {https://www.iwanami.co.jp/book/b259758.html},
    publisher      = {岩波書店}
}
@book{西森秀稔2005,
    author         = {西森秀稔},
    title          = {相転移・臨界現象の統計物理学},
    year           = {2005},
    month          = {11},
    publisher      = {培風館},
    series         = {新物理学シリーズ},
    volume         = {35},
    url             = {https://www.kyoritsu-pub.co.jp/book/b10003637.html},
}

@book{横尾英俊2004,
    author         = {横尾英俊},
    year           = {2004},
    title          = {情報理論の基礎},
    series         = {},
    volume         = {},
    edition        = {},
    url            = {https://www.kyoritsu-pub.co.jp/book/b10010336.html},
    publisher      = {共立出版}
}

@phdthesis{Gallager1960,
    author      = {Robert G. Gallager},
    school      = {MIT},
    title       = {{Low Density Parity Check Codes}},
    year        = {1960},
    url         = {http://hdl.handle.net/1721.1/11804},
}

@unpublished{Krzakala+2015,
    author = {Florent Krzakala and Lenka Zdeborova and Maria Chiara Angelini and Francesco Caltagirone},
    year   = {2015},
    title  = {Statistical Physics of Inference and Bayesian Estimation},
    url    = {https://indico.ictp.it/event/a14244/material/10/0.pdf},
    note   = {Lecture Notes in Spring College on the Physics of Complex Systems. Trieste, Italy}
}


@article{Zdeborova-Krzakala2016,
	author = {Lenka Zdeborov{\'a} and Florent Krzakala},
	doi = {10.1080/00018732.2016.1211393},
	eprint = {https://doi.org/10.1080/00018732.2016.1211393},
	journal = {Advances in Physics},
	number = {5},
	pages = {453--552},
	publisher = {Taylor \& Francis},
	title = {Statistical physics of inference: thresholds and algorithms},
	url = {https://doi.org/10.1080/00018732.2016.1211393},
	volume = {65},
	year = {2016},
	bdsk-url-1 = {https://doi.org/10.1080/00018732.2016.1211393}}
@article{Donoho-Johnstone1994,
 ISSN = {00063444},
 URL = {http://www.jstor.org/stable/2337118},
 abstract = {With ideal spatial adaptation, an oracle furnishes information about how best to adapt a spatially variable estimator, whether piecewise constant, piecewise polynomial, variable knot spline, or variable bandwidth kernel, to the unknown function. Estimation with the aid of an oracle offers dramatic advantages over traditional linear estimation by nonadaptive kernels; however, it is a priori unclear whether such performance can be obtained by a procedure relying on the data alone. We describe a new principle for spatially-adaptive estimation: selective wavelet reconstruction. We show that variable-knot spline fits and piecewise-polynomial fits, when equipped with an oracle to select the knots, are not dramatically more powerful than selective wavelet reconstruction with an oracle. We develop a practical spatially adaptive method, RiskShrink, which works by shrinkage of empirical wavelet coefficients. RiskShrink used in connection with sample rotation. Inclusion probabilities of any order can be written explicitly in closed form. Second-order inclusion probabilities πij satisfy the condition $0 < \pi_{ij} < \pi_{i}\pi_j$, which guarantees Yates & Grundy's variance estimator to be unbiased, definable for all samples and always nonnegative for any sample size.},
 author = {David L. Donoho and Iain M. Johnstone},
 journal = {Biometrika},
 number = {3},
 pages = {425--455},
 publisher = {[Oxford University Press, Biometrika Trust]},
 title = {Ideal Spatial Adaptation by Wavelet Shrinkage},
 urldate = {2024-06-24},
 volume = {81},
 year = {1994}
}
@article{Jaynes1957,
  title = {Information Theory and Statistical Mechanics},
  author = {Jaynes, E. T.},
  journal = {Phys. Rev.},
  volume = {106},
  issue = {4},
  pages = {620--630},
  numpages = {0},
  year = {1957},
  month = {May},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRev.106.620},
  url = {https://link.aps.org/doi/10.1103/PhysRev.106.620}
}
@book{Nishimori2001,
    author         = {Hidetoshi Nishimori},
    year           = {2001},
    title          = {Statistical Physics of Spin Glasses and Information Processing: An Introduction},
    series         = {},
    volume         = {},
    edition        = {},
    url            = {https://doi.org/10.1093/acprof:oso/9780198509417.001.0001},
    publisher      = {Oxford University Press}
}

@article{Bryngelson-Wolynes1987,
	abstract = {The theory of spin glasses was used to study a simple model of protein folding. The phase diagram of the model was calculated, and the results of dynamics calculations are briefly reported. The relation of these results to folding experiments, the relation of these hypotheses to previous protein folding theories, and the implication of these hypotheses for protein folding prediction schemes are discussed.},
	author = {J D Bryngelson and P G Wolynes},
	doi = {10.1073/pnas.84.21.7524},
	eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.84.21.7524},
	journal = {Proceedings of the National Academy of Sciences},
	number = {21},
	pages = {7524-7528},
	title = {Spin glasses and the statistical mechanics of protein folding.},
	url = {https://www.pnas.org/doi/abs/10.1073/pnas.84.21.7524},
	volume = {84},
	year = {1987},
	bdsk-url-1 = {https://www.pnas.org/doi/abs/10.1073/pnas.84.21.7524},
	bdsk-url-2 = {https://doi.org/10.1073/pnas.84.21.7524}}

@article{Jerrum1992,
	author = {Jerrum, Mark},
	journal = {Random Structures \& Algorithms},
	number = {4},
	pages = {347-359},
	title = {Large Cliques Elude the Metropolis Process},
	volume = {3},
	year = {1992}}
@INPROCEEDINGS{Jerrum-Sorkin1993,
  author={Jerrum, M. and Sorkin, G.B.},
  booktitle={Proceedings of 1993 IEEE 34th Annual Foundations of Computer Science}, 
  title={Simulated annealing for graph bisection}, 
  year={1993},
  volume={},
  number={},
  pages={94-103},
  keywords={Simulated annealing;Energy states;Computational modeling;Computer science;Temperature distribution;Stochastic processes;Polynomials;Costs;Temperature control;Power engineering and energy},
  doi={10.1109/SFCS.1993.366878}}
@INPROCEEDINGS{Achlioptas-Coja-Oghlan2008,
  author={Achlioptas, Dimitris and Coja-Oghlan, Amin},
  booktitle={2008 49th Annual IEEE Symposium on Foundations of Computer Science}, 
  title={Algorithmic Barriers from Phase Transitions}, 
  year={2008},
  volume={},
  number={},
  pages={793-802},
  keywords={Polynomials;Upper bound;Computer science;Phase estimation;Resilience;Geometry;Error correction codes;Physics;Injuries;Moment methods;Phase Transitions;Random Constraint Satisfaction Problems;Algorithms},
  doi={10.1109/FOCS.2008.11}}

@article{Bethe1935,
	abstract = { In a recent paper, Bragg and Williams have pointed out that the arrangement of the atoms in an alloy depends in a striking way on the temperature. At high temperatures, the atoms are distributed practically at random among the lattice points of the crystal, but at low temperatures a superlattice may be formed such that the atoms of one kind are arranged in a regular lattice of their own and the atoms of the other kind occupy the remaining ``sites'' in the crystal. The transition from the ordered to the disordered state occurs in a fairly small temperature range, and is accompanied by a large specific heat, an increase in electric resistance, etc. The mathematical method employed by Bragg and Williams is similar to that used in Weiss's theory of ferromagnetism. Both involve the assumption that the ``force'' tending to produce order at a given point is uniquely determined by the average state of order throughout the crystal. Actually it will depend on the configuration of the atoms in the immediate neighbourhood of the point under consideration. The order of the crystal as a whole determines this configuration only on the average. In the present paper, the effect of fluctuations in configuration, which was neglected by Bragg and Williams, will be taken into account. },
	author = {Bethe, H. A. and Bragg, William Lawrence},
	doi = {10.1098/rspa.1935.0122},
	eprint = {https://royalsocietypublishing.org/doi/pdf/10.1098/rspa.1935.0122},
	journal = {Proceedings of the Royal Society of London. Series A - Mathematical and Physical Sciences},
	number = {871},
	pages = {552-575},
	title = {Statistical theory of superlattices},
	url = {https://royalsocietypublishing.org/doi/abs/10.1098/rspa.1935.0122},
	volume = {150},
	year = {1935},
	bdsk-url-1 = {https://royalsocietypublishing.org/doi/abs/10.1098/rspa.1935.0122},
	bdsk-url-2 = {https://doi.org/10.1098/rspa.1935.0122}}


@article{Peierls+1936,
	abstract = { Recently, Bragg and Williams have given an explanation of the variation of the degree of order in an alloy with temperature. According to this explanation it is essential to consider the ordering as a co-operative phenomenon characteristic for large assemblies of atoms. The force, V, tending to produce order, is, according to this view, a function of the degree of order, because in order to remove an atom from a ``right'' to a ``wrong'' position one requires more energy the greater the number of ``right'' atoms in the neighbourhood, i. e., the greater the degree of order. One is thus led to consider two functions: V (s), describing the dependence of ordering force on order, and s (V), the order produced by a given ordering force, at given temperature. If both these functions are known, they implicitly define the degree of order as a function of the temperature. },
	author = {Peierls, R. and Bragg, William Lawrence},
	doi = {10.1098/rspa.1936.0047},
	eprint = {https://royalsocietypublishing.org/doi/pdf/10.1098/rspa.1936.0047},
	journal = {Proceedings of the Royal Society of London. Series A - Mathematical and Physical Sciences},
	number = {881},
	pages = {207-222},
	title = {Statistical theory of superlattices with unequal concentrations of the components},
	url = {https://royalsocietypublishing.org/doi/abs/10.1098/rspa.1936.0047},
	volume = {154},
	year = {1936},
	bdsk-url-1 = {https://royalsocietypublishing.org/doi/abs/10.1098/rspa.1936.0047},
	bdsk-url-2 = {https://doi.org/10.1098/rspa.1936.0047}}
@article{Plefka1982,
doi = {10.1088/0305-4470/15/6/035},
url = {https://dx.doi.org/10.1088/0305-4470/15/6/035},
year = {1982},
month = {jun},
publisher = {},
volume = {15},
number = {6},
pages = {1971},
author = {T Plefka},
title = {Convergence condition of the TAP equation for the infinite-ranged Ising spin glass model},
journal = {Journal of Physics A: Mathematical and General},
abstract = {It is shown that the power expansion of the Gibbs potential of the SK model up to second order in the exchange couplings leads to the TAP equation. This result remains valid for the general (including a ferromagnetic exchange) SK model. Theorems of power expansions and resolvent techniques are employed to solve the convergence problem. The convergence condition is presented for the whole temperature range and for general distributions of the local magnetisations.}
}
@article{Georges-Yedidia1991,
doi = {10.1088/0305-4470/24/9/024},
url = {https://dx.doi.org/10.1088/0305-4470/24/9/024},
year = {1991},
month = {may},
publisher = {},
volume = {24},
number = {9},
pages = {2173},
author = {A Georges and  J S Yedidia},
title = {How to expand around mean-field theory using high-temperature expansions},
journal = {Journal of Physics A: Mathematical and General},
abstract = {High-temperature expansions performed at a fixed-order parameter provide a simple and systematic way to derive and correct mean-field theories for statistical mechanical models. For models like spin glasses which have general couplings between spins, the authors show that these expansions generate the Thouless-Anderson-Palmer equations at low order. They explicitly calculate the corrections to TAP theory for these models. For ferromagnetic models, they show that their expansions can easily be converted into 1/d expansions around mean-field theory, where d is the number of spatial dimensions. Only a small finite number of graphs need to be calculated to generate each order in 1/d for thermodynamic quantities like free energy or magnetization. Unlike previous 1/d expansions, the expansions are valid in the low-temperature phases of the models considered. They consider alternative ways to expand around mean-field theory besides 1/d expansions. In contrast to the 1/d expansion for the critical temperature, which is presumably asymptotic, these schemes can be used to devise convergent expansions for the critical temperature. They also appear to give convergent series for thermodynamic quantities and critical exponents. They test the schemes using the spherical model, where their properties can be studied using exact expressions.}
}
@article{Kikuchi1951,
  title = {A Theory of Cooperative Phenomena},
  author = {Kikuchi, Ryoichi},
  journal = {Phys. Rev.},
  volume = {81},
  issue = {6},
  pages = {988--1003},
  numpages = {0},
  year = {1951},
  month = {Mar},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRev.81.988},
  url = {https://link.aps.org/doi/10.1103/PhysRev.81.988}
}
@ARTICLE{Gallager1962,
  author={Gallager, R.},
  journal={IRE Transactions on Information Theory}, 
  title={Low-density parity-check codes}, 
  year={1962},
  volume={8},
  number={1},
  pages={21-28},
  keywords={Parity check codes;Maximum likelihood decoding;Equations;Channel capacity;Information theory;Error probability;Linear approximation;Data communication;Error correction codes;Communication systems},
  doi={10.1109/TIT.1962.1057683}}
@proceedings{Pearl1982,
    editor          = {},
    author          = {Judea Pearl},
    published       = {},
    series          = {Proceedings of the AAAI Conference on Artificial Intelligence},
    title           = {{Reverend Bayes on Inference Engines: A Distributed Hierarchical Approach}},
    volume          = {2},
    year            = {1982},
    pages           = {133},
    url             = {https://aaai.org/papers/00133-aaai82-032-reverend-bayes-on-inference-engines-a-distributed-hierarchical-approach/},
}
@inbook{Yedidia+2003,
author = {Yedidia, Jonathan S. and Freeman, William T. and Weiss, Yair},
title = {Understanding belief propagation and its generalizations},
year = {2003},
isbn = {1558608117},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {"Inference" problems arise in statistical physics, computer vision, error-correcting coding theory, and AI. We explain the principles behind the belief propagation (BP) algorithm, which is an efficient way to solve inference problems based on passing local messages. We develop a unified approach, with examples, notation, and graphical models borrowed from the relevant disciplines.We explain the close connection between the BP algorithm and the Bethe approximation of statistical physics. In particular, we show that BP can only converge to a fixed point that is also a stationary point of the Bethe approximation to the free energy. This result helps explaining the successes of the BP algorithm and enables connections to be made with variational approaches to approximate inference.The connection of BP with the Bethe approximation also suggests a way to construct new message-passing algorithms based on improvements to Bethe's approximation introduced Kikuchi and others. The new generalized belief propagation (GBP) algorithms are significantly more accurate than ordinary BP for some problems. We illustrate how to construct GBP algorithms with a detailed example.},
booktitle = {Exploring Artificial Intelligence in the New Millennium},
pages = {239–269},
numpages = {31}
}
@ARTICLE{Abbe+2014,
  author={Abbe, Emmanuel and Bandeira, Afonso S. and Bracher, Annina and Singer, Amit},
  journal={IEEE Transactions on Network Science and Engineering}, 
  title={Decoding Binary Node Labels from Censored Edge Measurements: Phase Transition and Efficient Recovery}, 
  year={2014},
  volume={1},
  number={1},
  pages={10-22},
  keywords={Noise measurement;Clustering;Decoding;Graph theory;Image edge detection;Inverse problems;Synchronization;information theory;Synchronization problem;Information theoretic bounds;Erdos-Renyi graphs;Semidefinite relaxations;graphbased codes;Synchronization problem;Information theoretic bounds;Stochastic block model;Semidefinite relaxations;graph-based codes},
  doi={10.1109/TNSE.2014.2368716}}

@article{Mattis1976,
	abstract = {It is shown that if the bonds connecting spins on a lattice are separable functions of random variables, the thermodynamic and magnetic parameters may be obtained using the known properties of a spin system with non-random bonds.},
	author = {D.C. Mattis},
	doi = {https://doi.org/10.1016/0375-9601(76)90396-0},
	issn = {0375-9601},
	journal = {Physics Letters A},
	number = {5},
	pages = {421-422},
	title = {Solvable spin systems with random interactions},
	url = {https://www.sciencedirect.com/science/article/pii/0375960176903960},
	volume = {56},
	year = {1976},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/0375960176903960},
	bdsk-url-2 = {https://doi.org/10.1016/0375-9601(76)90396-0}}
@article{Nishimori1980,
doi = {10.1088/0022-3719/13/21/012},
url = {https://dx.doi.org/10.1088/0022-3719/13/21/012},
year = {1980},
month = {jul},
publisher = {},
volume = {13},
number = {21},
pages = {4071},
author = {H Nishimori},
title = {Exact results and critical properties of the Ising model with competing interactions},
journal = {Journal of Physics C: Solid State Physics},
abstract = {The random Ising model with competing interactions is investigated on the basis of the gauge-invariant formulation of the problem. Exact results for the internal energy, specific heat and gauge-invariant correlation function are derived. The critical exponent alpha is shown to be negative at the phase boundary of the paramagnetic and ferromagnetic phases if the latter exists at fairly low concentration of antiferromagnetic bonds.}
}

@article{Mezard+2002,
	abstract = {We study the satisfiability of random Boolean expressions built from many clauses with K variables per clause (K-satisfiability). Expressions with a ratio α of clauses to variables less than a threshold αc are almost always satisfiable, whereas those with a ratio above this threshold are almost always unsatisfiable. We show the existence of an intermediate phase below αc, where the proliferation of metastable states is responsible for the onset of complexity in search algorithms. We introduce a class of optimization algorithms that can deal with these metastable states; one such algorithm has been tested successfully on the largest existing benchmark of K-satisfiability.},
	author = {M. M{\'e}zard and G. Parisi and R. Zecchina},
	doi = {10.1126/science.1073287},
	eprint = {https://www.science.org/doi/pdf/10.1126/science.1073287},
	journal = {Science},
	number = {5582},
	pages = {812-815},
	title = {Analytic and Algorithmic Solution of Random Satisfiability Problems},
	url = {https://www.science.org/doi/abs/10.1126/science.1073287},
	volume = {297},
	year = {2002},
	bdsk-url-1 = {https://www.science.org/doi/abs/10.1126/science.1073287},
	bdsk-url-2 = {https://doi.org/10.1126/science.1073287}}

@article{Kabashima2005,
	abstract = { A scheme to provide various mean-field-type approximation algorithms is presented by employing the Bethe free energy formalism to a family of replicated systems in conjunction with analytical continuation with respect to the number of replicas. In the scheme, survey propagation (SP), which is an efficient algorithm developed recently for analyzing the microscopic properties of glassy states for a fixed sample of disordered systems, can be reproduced by assuming the simplest replica symmetry on stationary points of the replicated Bethe free energy. Belief propagation and generalized SP can also be offered in the identical framework under assumptions of the highest and broken replica symmetries, respectively. },
	author = {Kabashima ,Yoshiyuki},
	doi = {10.1143/JPSJ.74.2133},
	eprint = {https://doi.org/10.1143/JPSJ.74.2133},
	journal = {Journal of the Physical Society of Japan},
	number = {8},
	pages = {2133-2136},
	title = {Replicated Bethe Free Energy: A Variational Principle behind Survey Propagation},
	url = {https://doi.org/10.1143/JPSJ.74.2133},
	volume = {74},
	year = {2005},
	bdsk-url-1 = {https://doi.org/10.1143/JPSJ.74.2133}}

@book{Talagrand2011,
    author         = {Michel Talagrand},
    year           = {2011},
    title          = {{Mean Field Models for Spin Glasses. Volume I: Basic Examples}},
    series         = {},
    volume         = {},
    edition        = {},
    url            = {https://doi.org/10.1007/978-3-642-15202-3},
    publisher      = {Springer Berlin, Heidelberg}
}
@article{Kosterlitz-Thouless1973,
doi = {10.1088/0022-3719/6/7/010},
url = {https://dx.doi.org/10.1088/0022-3719/6/7/010},
year = {1973},
month = {apr},
publisher = {},
volume = {6},
number = {7},
pages = {1181},
author = {J M Kosterlitz and  D J Thouless},
title = {Ordering, metastability and phase transitions in two-dimensional systems},
journal = {Journal of Physics C: Solid State Physics},
abstract = {A new definition of order called topological order is proposed for two-dimensional systems in which no long-range order of the conventional type exists. The possibility of a phase transition characterized by a change in the response of the system to an external perturbation is discussed in the context of a mean field type of approximation. The critical behaviour found in this model displays very weak singularities. The application of these ideas to the xy model of magnetism, the solid-liquid transition, and the neutral superfluid are discussed. This type of phase transition cannot occur in a superconductor nor in a Heisenberg ferromagnet.}
}
@article{Thouless1986,
  title = {{Spin-Glass on a Bethe Lattice}},
  author = {Thouless, D. J.},
  journal = {Phys. Rev. Lett.},
  volume = {56},
  issue = {10},
  pages = {1082--1085},
  numpages = {0},
  year = {1986},
  month = {Mar},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevLett.56.1082},
  url = {https://link.aps.org/doi/10.1103/PhysRevLett.56.1082}
}

@article{Onsager1944,
  title = {Crystal Statistics. I. A Two-Dimensional Model with an Order-Disorder Transition},
  author = {Onsager, Lars},
  journal = {Phys. Rev.},
  volume = {65},
  issue = {3-4},
  pages = {117--149},
  numpages = {0},
  year = {1944},
  month = {Feb},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRev.65.117},
  url = {https://link.aps.org/doi/10.1103/PhysRev.65.117}
}
@inproceedings{Husimi1954,
    author          = {K. Husimi},
    year            = {1954},
    title           = {Statistical Mechanics of Condensation},
    booktitle       = {Proceedings of the International Conference of Theoretical Physics. Science Council of Japan},
    editor          = {Hideki Yukawa},
    volume          = {},
    pages           = {},
    url             = {}
}
@article{Temperley1954,
doi = {10.1088/0370-1298/67/3/306},
url = {https://dx.doi.org/10.1088/0370-1298/67/3/306},
year = {1954},
month = {mar},
publisher = {},
volume = {67},
number = {3},
pages = {233},
author = {H N V Temperley},
title = {The Mayer Theory of Condensation Tested Against a Simple Model of the Imperfect Gas},
journal = {Proceedings of the Physical Society. Section A},
abstract = {A very simple model of an imperfect gas, analogous to the Weiss model of a ferromagnetic, is used to check various conflicting conclusions about the relationship between the condensation of an imperfect gas and the divergence of Mayer's virial series. For this model, the divergence of the virial series has no physical significance, and the interpretations of this divergence suggested by Mayer and by Born and Green are both incorrect. The model also turns out to be one for which care is necessary in going to the limit of a very large assembly.}
}
@article{Kac-Thompson1966,
 ISSN = {00278424},
 URL = {http://www.jstor.org/stable/57450},
 author = {Mark Kac and Colin J. Thompson},
 journal = {Proceedings of the National Academy of Sciences of the United States of America},
 number = {4},
 pages = {676--683},
 publisher = {National Academy of Sciences},
 title = {On the Mathematical Mechanism of Phase Transition},
 urldate = {2024-05-30},
 volume = {55},
 year = {1966}
}
@article{Potts1952, title={Some generalized order-disorder transformations}, volume={48}, DOI={10.1017/S0305004100027419}, number={1}, journal={Mathematical Proceedings of the Cambridge Philosophical Society}, author={Potts, R. B.}, year={1952}, pages={106–109}}


@article{Storath+2015,
	abstract = {We propose a new algorithmic approach to the non-smooth and non-convex Potts problem (also called piecewise-constant Mumford--Shah problem) for inverse imaging problems. We derive a suitable splitting into specific subproblems that can all be solved efficiently. Our method does not require a priori knowledge on the gray levels nor on the number of segments of the reconstruction. Further, it avoids anisotropic artifacts such as geometric staircasing. We demonstrate the suitability of our method for joint image reconstruction and segmentation. We focus on Radon data, where we in particular consider limited data situations. For instance, our method is able to recover all segments of the Shepp--Logan phantom from seven angular views only. We illustrate the practical applicability on a real positron emission tomography dataset. As further applications, we consider spherical Radon data as well as blurred data.},
	author = {Martin Storath and Andreas Weinmann and J{\"u}rgen Frikel and Michael Unser},
	doi = {10.1088/0266-5611/31/2/025003},
	journal = {Inverse Problems},
	month = {jan},
	number = {2},
	pages = {025003},
	publisher = {IOP Publishing},
	title = {Joint image reconstruction and segmentation using the Potts model},
	url = {https://dx.doi.org/10.1088/0266-5611/31/2/025003},
	volume = {31},
	year = {2015},
	bdsk-url-1 = {https://dx.doi.org/10.1088/0266-5611/31/2/025003}}
@article{Engel+2013,
  title = {Hard-disk equation of state: First-order liquid-hexatic transition in two dimensions with three simulation methods},
  author = {Engel, Michael and Anderson, Joshua A. and Glotzer, Sharon C. and Isobe, Masaharu and Bernard, Etienne P. and Krauth, Werner},
  journal = {Phys. Rev. E},
  volume = {87},
  issue = {4},
  pages = {042134},
  numpages = {8},
  year = {2013},
  month = {Apr},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevE.87.042134},
  url = {https://link.aps.org/doi/10.1103/PhysRevE.87.042134}
}
@article{Thorneywork2017,
  title = {Two-Dimensional Melting of Colloidal Hard Spheres},
  author = {Thorneywork, Alice L. and Abbott, Joshua L. and Aarts, Dirk G. A. L. and Dullens, Roel P. A.},
  journal = {Phys. Rev. Lett.},
  volume = {118},
  issue = {15},
  pages = {158001},
  numpages = {5},
  year = {2017},
  month = {Apr},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevLett.118.158001},
  url = {https://link.aps.org/doi/10.1103/PhysRevLett.118.158001}
}
@article{Wei+2011,
    author = {Wei, Dongshan and Song, Yang and Wang, Feng},
    title = "{A simple molecular mechanics potential for μm scale graphene simulations from the adaptive force matching method}",
    journal = {The Journal of Chemical Physics},
    volume = {134},
    number = {18},
    pages = {184704},
    year = {2011},
    month = {05},
    abstract = "{A simple molecular mechanics force field for graphene (PPBE-G) was created by force matching the density functional theory Perdew-Burke-Ernzerhof forces using the adaptive force matching method recently developed in our group. The PPBE-G potential was found to provide significantly more accurate forces than other existing force fields. Several properties of graphene, such as Young's modulus, bending rigidity, and thermal conductivity, have been studied with our potential. The calculated properties are in good agreement with corresponding density functional theory and experimental values. The thermal conductivity calculated with reverse non-equilibrium molecular dynamics depends sensitively on graphene size thus requiring the simulation of large sheets for convergence. Since the PPBE-G potential only contains simple additive energy expressions, it is very computationally efficient and is capable of modeling large graphene sheets in the μm length scale.}",
    issn = {0021-9606},
    doi = {10.1063/1.3589163},
    url = {https://doi.org/10.1063/1.3589163},
    eprint = {https://pubs.aip.org/aip/jcp/article-pdf/doi/10.1063/1.3589163/14062575/184704\_1\_online.pdf},
}
@article{Wood-Parker1957,
    author = {Wood, W. W. and Parker, F. R.},
    title = "{Monte Carlo Equation of State of Molecules Interacting with the Lennard‐Jones Potential. I. A Supercritical Isotherm at about Twice the Critical Temperature}",
    journal = {The Journal of Chemical Physics},
    volume = {27},
    number = {3},
    pages = {720-733},
    year = {1957},
    month = {09},
    abstract = "{Values obtained by Monte Carlo calculations are reported for the compressibility factor, excess internal energy, excess constant‐volume heat capacity, and the radial distribution function of Lennard‐Jones (12,6) molecules at the reduced temperature kT/ε*=2.74, and at thirteen volumes between v/v*=0.75 and 7.5. (v is the molar volume; v*=2—½N0r*3; N0 is Avogadro's number; ε* is the depth, and r* the radius of the Lennard‐Jones potential well.) The results are compared with the experimental observations of Michels (∼150–2000 atmos) and Bridgman (∼2000–15 000 atmos) on argon at 55°C, using Michels' second virial coefficient values for the potential parameters. Close agreement with Michels is found, but significant disagreement with Bridgman. The Monte Carlo calculations display the fluid‐solid transition; the transition pressure and the volume and enthalpy increments are not precisely determined. The Lennard‐Jones‐Devonshire cell theory gives results which disagree throughout the fluid phase, but agree on the solid branch of the isotherm. Limited comparisons with the Kirkwood‐Born‐Green results indicate that the superposition approximation yields useful results at least up to v/v*=2.5.}",
    issn = {0021-9606},
    doi = {10.1063/1.1743822},
    url = {https://doi.org/10.1063/1.1743822},
    eprint = {https://pubs.aip.org/aip/jcp/article-pdf/27/3/720/18812357/720\_1\_online.pdf},
}
@article{Alder-Wainwright1962,
  title = {Phase Transition in Elastic Disks},
  author = {Alder, B. J. and Wainwright, T. E.},
  journal = {Phys. Rev.},
  volume = {127},
  issue = {2},
  pages = {359--361},
  numpages = {0},
  year = {1962},
  month = {Jul},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRev.127.359},
  url = {https://link.aps.org/doi/10.1103/PhysRev.127.359}
}

@article{Simanyi2003,
	abstract = {We consider the system of N (≥2) hard disks of masses m1,...,mNand radius r in the flat unit torus 𝕋2. We prove the ergodicity (actually, the B-mixing property) of such systems for almost every selection (m1,...,mN;r) of the outer geometric parameters.},
	author = {Sim{\'a}nyi, N{\'a}ndor},
	date = {2003/10/01},
	date-added = {2024-06-29 12:33:25 +0900},
	date-modified = {2024-06-29 12:33:25 +0900},
	doi = {10.1007/s00222-003-0304-9},
	id = {Sim{\'a}nyi2003},
	isbn = {1432-1297},
	journal = {Inventiones mathematicae},
	number = {1},
	pages = {123--178},
	title = {Proof of the Boltzmann-Sinai ergodic hypothesis for typical hard disk systems},
	url = {https://doi.org/10.1007/s00222-003-0304-9},
	volume = {154},
	year = {2003},
	bdsk-url-1 = {https://doi.org/10.1007/s00222-003-0304-9}}

@book{Tabachnikov2005,
    author         = {Serge Tabachnikov},
    year           = {2005},
    title          = {Geometry and Billiards},
    series         = {Student Mathematical Library},
    volume         = {30},
    edition        = {},
    url            = {https://bookstore.ams.org/view?ProductCode=STML/30},
    publisher      = {American Mathematical Society}
}
@article{Bou-Rabee_Sanz-Serna2018, title={Geometric integrators and the Hamiltonian Monte Carlo method}, volume={27}, DOI={10.1017/S0962492917000101}, journal={Acta Numerica}, author={Bou-Rabee, Nawaf and Sanz-Serna, J. M.}, year={2018}, pages={113–206}}

@article{Verlet1967,
  title = {Computer "Experiments" on Classical Fluids. I. Thermodynamical Properties of Lennard-Jones Molecules},
  author = {Verlet, Loup},
  journal = {Phys. Rev.},
  volume = {159},
  issue = {1},
  pages = {98--103},
  numpages = {0},
  year = {1967},
  month = {Jul},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRev.159.98},
  url = {https://link.aps.org/doi/10.1103/PhysRev.159.98}
}

@article{Stormer1907,
    author          = {Carl Störmer},
    year            = {1907},
    title           = {Sur les trajectoires des corpuscules électrisés dans l’espace. Applications à l’aurore boréale et aux perturbations magnétiques},
    journal         = {Radium (Paris)},
    volume          = {4},
    number          = {1},
    pages           = {2-5},
    doi             = {10.1051/ra-dium:01907004010201},
}

@book{Leimkuhler-Matthews2015,
    author         = {Ben Leimkuhler and Charles Matthews},
    year           = {2015},
    title          = {Molecular Dynamics: With Deterministic and Stochastic Numerical Methods},
    series         = {},
    volume         = {},
    edition        = {},
    url            = {https://doi.org/10.1007/978-3-319-16375-8},
    publisher      = {Springer Cham}
}

@book{Pavliotis2014,
    author         = {Grigorios A. Pavliotis},
    year           = {2014},
    title          = {Stochastic Processes and Applications: Diffusion Processes, the Fokker-Planck and Langevin Equations},
    series         = {Texts in Applied Mathematics},
    volume         = {60},
    edition        = {},
    url            = {https://link.springer.com/book/10.1007/978-1-4939-1323-7},
    publisher      = {Springer New York}
}
@book{Leimkuhler-Reich2005, place={Cambridge}, series={Cambridge Monographs on Applied and Computational Mathematics}, title={Simulating Hamiltonian Dynamics}, publisher={Cambridge University Press}, author={Leimkuhler, Benedict and Reich, Sebastian}, year={2005}, collection={Cambridge Monographs on Applied and Computational Mathematics}}

@article{Eberle+2019,
	author = {Andreas Eberle and Arnaud Guillin and Raphael Zimmer},
	doi = {10.1214/18-AOP1299},
	journal = {The Annals of Probability},
	keywords = {Convergence to equilibrium, hypocoercivity, kinetic Fokker--Planck equation, Langevin diffusion, Lyapunov functions, quantitative bounds, Reflection coupling, stochastic Hamiltonian dynamics, Wasserstein distance},
	number = {4},
	pages = {1982 -- 2010},
	publisher = {Institute of Mathematical Statistics},
	title = {{Couplings and quantitative contraction rates for Langevin dynamics}},
	url = {https://doi.org/10.1214/18-AOP1299},
	volume = {47},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1214/18-AOP1299}}


@article{Dalalyan-Riou-Durand2020,
	author = {Arnak S. Dalalyan and Lionel Riou-Durand},
	doi = {10.3150/19-BEJ1178},
	journal = {Bernoulli},
	keywords = {Hamiltonian Monte Carlo, kinetic Langevin, Langevin algorithm, Markov chain Monte Carlo, mixing rate},
	number = {3},
	pages = {1956 -- 1988},
	publisher = {Bernoulli Society for Mathematical Statistics and Probability},
	title = {{On sampling from a log-concave density using kinetic Langevin diffusions}},
	url = {https://doi.org/10.3150/19-BEJ1178},
	volume = {26},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.3150/19-BEJ1178}}
@misc{Stoltz2021,
      title={Computational statistical physics and hypocoercivity}, 
      author={Gabriel Stoltz},
      year={2021},
      eprint={2112.08221},
      archivePrefix={arXiv},
      primaryClass={math.AP},
      url={https://arxiv.org/abs/2112.08221}, 
}
@article{Rossky+1978,
    author = {Rossky, P. J. and Doll, J. D. and Friedman, H. L.},
    title = "{Brownian dynamics as smart Monte Carlo simulation}",
    journal = {The Journal of Chemical Physics},
    volume = {69},
    number = {10},
    pages = {4628-4633},
    year = {1978},
    month = {11},
    abstract = "{A new Monte Carlo simulation procedure is developed which is expected to produce more rapid convergence than the standard Metropolis method. The trial particle moves are chosen in accord with a Brownian dynamics algorithm rather than at random. For two model systems, a string of point masses joined by harmonic springs and a cluster of charged soft spheres, the new procedure is compared to the standard one and shown to manifest a more rapid convergence rate for some important energetic and structural properties.}",
    issn = {0021-9606},
    doi = {10.1063/1.436415},
    url = {https://doi.org/10.1063/1.436415},
    eprint = {https://pubs.aip.org/aip/jcp/article-pdf/69/10/4628/18914248/4628\_1\_online.pdf},
}
@article{Li+2022-HD,
    author = {Li, Botao and Nishikawa, Yoshihiko and Höllmer, Philipp and Carillo, Louis and Maggs, A. C. and Krauth, Werner},
    title = "{Hard-disk pressure computations—a historic perspective}",
    journal = {The Journal of Chemical Physics},
    volume = {157},
    number = {23},
    pages = {234111},
    year = {2022},
    month = {12},
    abstract = "{We discuss pressure computations for the hard-disk model performed since 1953 and compare them to the results that we obtain with a powerful event-chain Monte Carlo and a massively parallel Metropolis algorithm. Like other simple models in the sciences, such as the Drosophila model of biology, the hard-disk model has needed monumental efforts to be understood. In particular, we argue that the difficulty of estimating the pressure has not been fully realized in the decades-long controversy over the hard-disk phase-transition scenario. We present the physics of the hard-disk model, the definition of the pressure and its unbiased estimators, several of which are new. We further treat different sampling algorithms and crucial criteria for bounding mixing times in the absence of analytical predictions. Our definite results for the pressure, for up to one million disks, may serve as benchmarks for future sampling algorithms. A synopsis of hard-disk pressure data as well as different versions of the sampling algorithms and pressure estimators are made available in an open-source repository.}",
    issn = {0021-9606},
    doi = {10.1063/5.0126437},
    url = {https://doi.org/10.1063/5.0126437},
    eprint = {https://pubs.aip.org/aip/jcp/article-pdf/doi/10.1063/5.0126437/16715104/234111\_1\_online.pdf},
}
@book{Sato2013,
    author         = {Keniti Sato},
    year           = {2013},
    title          = {Lévy Processes and Infinitely Divisible Distributions},
    series         = {Cambridge Studies in Advanced Mathematics},
    volume         = {},
    edition        = {2},
    url            = {},
    publisher      = {Cambridge University Press}
}

@book{Last_Penrose2017, place={Cambridge}, series={Institute of Mathematical Statistics Textbooks}, title={Lectures on the Poisson Process}, publisher={Cambridge University Press}, author={Last, Günter and Penrose, Mathew}, year={2017}, collection={Institute of Mathematical Statistics Textbooks}}

@article{Kingman2006,
    author          = {J. F. C. Kingman},
    year            = {2006},
    title           = {{Poisson Processes Revisited}},
    journal         = {Probability and Mathematical Statistics},
    volume          = {26},
    number          = {1},
    pages           = {77-95},
    url             = {http://yadda.icm.edu.pl/baztech/element/bwmeta1.element.baztech-66de0dee-771a-4cf4-bc02-57f689661a36}
}

@book{Kingman1992,
    author         = {J. F. C. Kingman},
    year           = {1992},
    title          = {Poisson Processes},
    series         = {Oxford Studies in Probability},
    volume         = {},
    edition        = {},
    url            = {},
    publisher      = {Clarendon Press}
}

@article{Madan+1998,
    author = {Madan, Dilip B. and Carr, Peter P. and Chang, Eric C.},
    title = "{The Variance Gamma Process and Option Pricing}",
    journal = {Review of Finance},
    volume = {2},
    number = {1},
    pages = {79-105},
    year = {1998},
    month = {04},
    abstract = "{A three parameter stochastic process, termed the variance gamma process, that generalizes Brownian motion is developed as a model for the dynamics of log stock prices. Theprocess is obtained by evaluating Brownian motion with drift at a random time given by a gamma process. The two additional parameters are the drift of the Brownian motion and the volatility of the time change. These additional parameters provide control over the skewness and kurtosis of the return distribution. Closed forms are obtained for the return density and the prices of European options.The statistical and risk neutral densities are estimated for data on the S\\&amp;P500 Index and the prices of options on this Index. It is observed that the statistical density is symmetric with some kurtosis, while the risk neutral density is negatively skewed with a larger kurtosis. The additional parameters also correct for pricing biases of the Black Scholes model that is a parametric special case of the option pricing model developed here.}",
    issn = {1572-3097},
    doi = {10.1023/A:1009703431535},
    url = {https://doi.org/10.1023/A:1009703431535},
    eprint = {https://academic.oup.com/rof/article-pdf/2/1/79/26315662/2-1-79.pdf},
}

@techreport{Todorovic-Yevjevich1969,
    institution = {Colorado State University},
    author          = {P. Todorovic and V. Yevjevich},
    year            = {1969},
    title           = {Stochastic Process of Precipitation},
    journal         = {Hydrology Papers},
}

@article{西村克己-江藤剛治1981,
    author          = {西村克己 and 江藤剛治},
    year            = {1981},
    title           = {Marked point Processモデルによる降水量時系列の解析},
    journal         = {水理講演会論文集},
    volume          = {25},
    number          = {},
    pages           = {191-196},
    url             = {https://doi.org/10.2208/prohe1975.25.191}
}

@ARTICLE{Ogata1981,
  author={Ogata, Y.},
  journal={IEEE Transactions on Information Theory}, 
  title={On Lewis' simulation method for point processes}, 
  year={1981},
  volume={27},
  number={1},
  pages={23-31},
  keywords={},
  doi={10.1109/TIT.1981.1056305}}
@article{Campbell1909,
  added-at = {2008-07-07T11:54:00.000+0200},
  author = {Campbell, N.},
  biburl = {https://www.bibsonomy.org/bibtex/2c6cd14bf56c439b248b0a2126f243abe/srd27},
  date = {1909},
  interhash = {e5c8dace5d02dca967de594892943460},
  intrahash = {c6cd14bf56c439b248b0a2126f243abe},
  journal = {Proc Cambr. Phil. Soc},
  keywords = {imported},
  pages = {pp. 117-136},
  timestamp = {2008-07-07T11:54:00.000+0200},
  title = {The study of discontinuous phenomena},
  volume = {vol. 15},
  year = 1909
}

@unpublished{Eberle2012,
    author = {Andreas Eberle},
    year   = {2012},
    title  = {Stochastic Analysis},
    url    = {https://wt.iam.uni-bonn.de/fileadmin/WT/Inhalt/people/Andreas_Eberle/StoAn1112/StochasticAnalysisFinal.pdf}
}


@article{Ito1941,
	author = {Kiyosi Ito},
	doi = {10.4099/jjm1924.18.0_261},
	journal = {Japanese journal of mathematics :transactions and abstracts},
	pages = {261-301},
	title = {On stochastic processes (I)},
	volume = {18},
	year = {1941},
	bdsk-url-1 = {https://doi.org/10.4099/jjm1924.18.0_261}}

@book{Arteaga-Sato2019,
    author         = {Alfonso Rocha-Arteaga and Keniti Sato},
    year           = {2019},
    title          = {Topics in Infinitely Divisible Distributions and Lévy Processes, Revised Edition},
    series         = {Springer Briefs in Probability and Mathematical Statistics},
    volume         = {},
    edition        = {},
    url            = {https://link.springer.com/book/10.1007/978-3-030-22700-5},
    publisher      = {Springer Cham}
}

@book{Bottcher+2013,
    author         = {Björn Böttcher and René Schilling and Jian Wang},
    year           = {2013},
    title          = {Lévy Matters III: Lévy-Type Processes: Construction, Approximation and Sample Path Properties},
    series         = {Lecture Notes in Mathematics},
    volume         = {2099},
    edition        = {},
    url            = {https://link.springer.com/book/10.1007/978-3-319-02684-8},
    publisher      = {Springer Cham}
}
@book{Osswald2012,
    author         = {Horst Osswald},
    year           = {2012},
    title          = {Malliavin Calculus for Lévy Processes and Infinite-Dimentional Brownian Motion},
    series         = {Cambridge Tracts in Mathematics},
    volume         = {191},
    edition        = {},
    url            = {https://www.cambridge.org/core/books/malliavin-calculus-for-levy-processes-and-infinitedimensional-brownian-motion/860213B6477038E51E829940593AB15D},
    publisher      = {Cambridge University Press}
}
@book{佐藤健一1990,
    author         = {佐藤健一},
    year           = {1990},
    month          = {12},
    title          = {加法過程},
    series         = {紀伊国屋数学叢書},
    volume         = {33},
    edition        = {},
    url            = {},
    publisher      = {紀伊国屋書店}
}
@article{佐藤健一2011,
    author          = {佐藤健一},
    year            = {2011},
    title           = {レヴィ過程による確率積分と無限分解可能分布},
    journal         = {数学},
    volume          = {63},
    number          = {2},
    pages           = {161-181},
    url             = {https://www.jstage.jst.go.jp/article/sugaku/63/2/63_0632161/_article/-char/ja}
}

@article{Khinchin-Levy1936,
    author          = {A. Ya. Khinchin and Paul Lévy},
    year            = {1936},
    title           = {Sur les lois stables},
    journal         = {Comptes Rendus de l'Académie des Sciences},
    volume          = {202},
    number          = {},
    pages           = {374-376},
    url             = {}
}

@book{Moran1959,
    author         = {P. A. P. Moran},
    year           = {1959},
    title          = {The Theory of Storage},
    series         = {},
    volume         = {},
    edition        = {},
    url            = {},
    publisher      = {London: Methuen and Co. Led. New York: Wiley and Sons, Inc.}
}

@unpublished{AlmostSure2011,
    author = {George Lowther},
    year   = {2011},
    title  = {Properties of Lévy Processes},
    url    = {https://almostsuremath.com/2011/02/25/properties-of-levy-processes/}
}

@article{Karamata1933,
author = {Karamata, J.},
journal = {Bulletin de la Société Mathématique de France},
keywords = {Analysis},
language = {fre},
pages = {55-62},
publisher = {Société mathématique de France},
title = {Sur un mode de croissance régulière. Théorèmes fondamentaux},
url = {http://eudml.org/doc/86621},
volume = {61},
year = {1933},
}

@book{Ibragimov-Linnik1971,
    author         = {I. A. Ibragimov and Yu V. Linnik},
    year           = {1971},
    title          = {Independent and Stationary Sequences of Random Variables},
    series         = {},
    volume         = {},
    edition        = {},
    url            = {},
    publisher      = {Groningen, Wolters-Noordhoff}
}

@book{Embrechts-Maejima2002,
    author         = {Paul Embrechts and Makoto Maejima},
    year           = {2002},
    title          = {Selfsimilar Processes},
    series         = {Princeton Series in Applied Mathematics},
    volume         = {},
    edition        = {},
    url            = {https://www.jstor.org/stable/j.ctt7t1hk},
    publisher      = {Princeton University Press}
}

@article{Ferguson1973,
 ISSN = {00905364},
 URL = {http://www.jstor.org/stable/2958008},
 author = {Thomas S. Ferguson},
 journal = {The Annals of Statistics},
 number = {2},
 pages = {209--230},
 publisher = {Institute of Mathematical Statistics},
 title = {A Bayesian Analysis of Some Nonparametric Problems},
 urldate = {2024-07-01},
 volume = {1},
 year = {1973}
}
@book{Davis1993,
    author         = {M. H. A. Davis},
    year           = {1993},
    title          = {Markov Models and Optimization},
    series         = {Monographs on Statistics and Applied Probability},
    volume         = {49},
    edition        = {},
    url            = {https://doi.org/10.1201/9780203748039},
    publisher      = {Chapman \& Hall},
}
@phdthesis{Vasdekis2021,
           title = {On zig-zag extensions and related ergodicity properties},
           month = {February},
            note = {Unpublished},
          school = {University of Warwick},
            year = {2021},
             url = {http://webcat.warwick.ac.uk/record=b3714913},
        abstract = {In this thesis, we study the Zig-Zag process, which was recently proposed as an MCMC algorithm. We propose two extensions for this process and we prove geometric  ergodicity results for both of them. The first extension, allows the process  to move in more directions than just parallel to f},
          author = {Vasdekis, Georgios}
}

@article{Vasdekis-Roberts2023,
	author = {G. Vasdekis and G. O. Roberts},
	doi = {10.1214/23-AAP1930},
	journal = {The Annals of Applied Probability},
	keywords = {central limit theorem, exponential ergodicity, Markov chain Monte Carlo, Piecewise deterministic Markov process},
	number = {6A},
	pages = {4693 -- 4746},
	publisher = {Institute of Mathematical Statistics},
	title = {{Speed up Zig-Zag}},
	url = {https://doi.org/10.1214/23-AAP1930},
	volume = {33},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1214/23-AAP1930}}
@article{Goldstein1951,
    author = {Goldstein, S.},
    title = "{On Diffusion by Discontinuous Movements, and on the Telegraph Equation}",
    journal = {The Quarterly Journal of Mechanics and Applied Mathematics},
    volume = {4},
    number = {2},
    pages = {129-156},
    year = {1951},
    month = {01},
    abstract = "{At time t=0 a large number of non-interacting particles start from an origin and move with a uniform velocity υ along a straight line for an interval of time τ. To begin with, half move in each direction. Thereafter, and at the end of each successive interval of time τ, each particle starts a new partial path; it still moves with speed υ, and there is a probability p that it will continue to move in the same direction as in its previous path, and a probability q (= 1−p) that the direction of its velocity will be reversed, so the directions in any two consecutive intervals are correlated with a correlation coefficient c=p−q. The partial correlations for non-consecutive intervals are zero. The difference equation is found for the fraction γ(n,ν) of the number of particles at a distance y=νυτ from the origin after a time t=nτ; it is shown how γ(n,ν) may be computed; asymptotic formulae for large n are found, both for a fixed value p/q and for a fixed value of nq/p. The limiting density distribution (and the limiting characteristic function) are found when n→∞, τ→0, with nτ=t, νυτ=y, and in the limiting operation c=1−τ/A, with A constant, so that c→1, and the speed υ is kept constant; the limiting form of the difference equation is the telegraph equation, with υ2=(LC)−1, A=L/R, where L, C, and R are the self-inductance, capacitance, and resistance per unit length; and the limiting density distribution is the solution of this equation for an instantaneous source. If p+q≠1, and there is, at the end of each interval of time τ, a non-zero probability, 1−p−q, that a particle will escape from the system, then the same limiting operation, with T/(1−p−q)=G, G constant, leads to the telegraph equation with leakage, the leakage resistance being G/C. The solution of the telegraph equation is further considered (and in particular is given in terms of Lommel's function of two variables) when one end of a long cable is held at a constant potential for t≥0.}",
    issn = {0033-5614},
    doi = {10.1093/qjmam/4.2.129},
    url = {https://doi.org/10.1093/qjmam/4.2.129},
    eprint = {https://academic.oup.com/qjmam/article-pdf/4/2/129/5301574/4-2-129.pdf},
}

@article{Andrieu-Livingstone2021,
	author = {Christophe Andrieu and Samuel Livingstone},
	doi = {10.1214/20-AOS2008},
	journal = {The Annals of Statistics},
	keywords = {Markov chain Monte Carlo, Peskun ordering, Piecewise deterministic Markov processes},
	number = {4},
	pages = {1958 -- 1981},
	publisher = {Institute of Mathematical Statistics},
	title = {{Peskun--Tierney ordering for Markovian Monte Carlo: Beyond the reversible scenario}},
	url = {https://doi.org/10.1214/20-AOS2008},
	volume = {49},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1214/20-AOS2008}}
@article{Sen+2020,
    author = {Sen, Deborshee and Sachs, Matthias and Lu, Jianfeng and Dunson, David B},
    title = "{Efficient posterior sampling for high-dimensional imbalanced logistic regression}",
    journal = {Biometrika},
    volume = {107},
    number = {4},
    pages = {1005-1012},
    year = {2020},
    month = {06},
    abstract = "{Classification with high-dimensional data is of widespread interest and often involves dealing with imbalanced data. Bayesian classification approaches are hampered by the fact that current Markov chain Monte Carlo algorithms for posterior computation become inefficient as the number \\$p\\$ of predictors or the number \\$n\\$ of subjects to classify gets large, because of the increasing computational time per step and worsening mixing rates. One strategy is to employ a gradient-based sampler to improve mixing while using data subsamples to reduce the per-step computational complexity. However, the usual subsampling breaks down when applied to imbalanced data. Instead, we generalize piecewise-deterministic Markov chain Monte Carlo algorithms to include importance-weighted and mini-batch subsampling. These maintain the correct stationary distribution with arbitrarily small subsamples and substantially outperform current competitors. We provide theoretical support for the proposed approach and demonstrate its performance gains in simulated data examples and an application to cancer data.}",
    issn = {0006-3444},
    doi = {10.1093/biomet/asaa035},
    url = {https://doi.org/10.1093/biomet/asaa035},
    eprint = {https://academic.oup.com/biomet/article-pdf/107/4/1005/34865861/asaa035.pdf},
}

@InProceedings{Pakman+2017,
  title = 	 {Stochastic Bouncy Particle Sampler},
  author =       {Ari Pakman and Dar Gilboa and David Carlson and Liam Paninski},
  booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
  pages = 	 {2741--2750},
  year = 	 {2017},
  editor = 	 {Precup, Doina and Teh, Yee Whye},
  volume = 	 {70},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {06--11 Aug},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v70/pakman17a/pakman17a.pdf},
  url = 	 {https://proceedings.mlr.press/v70/pakman17a.html},
  abstract = 	 {We introduce a stochastic version of the non-reversible, rejection-free Bouncy Particle Sampler (BPS), a Markov process whose sample trajectories are piecewise linear, to efficiently sample Bayesian posteriors in big datasets. We prove that in the BPS no bias is introduced by noisy evaluations of the log-likelihood gradient. On the other hand, we argue that efficiency considerations favor a small, controllable bias, in exchange for faster mixing. We introduce a simple method that controls this trade-off. We illustrate these ideas in several examples which outperform previous approaches.}
}

@article{Bierkens+2023,
	abstract = {We construct a new class of efficient Monte Carlo methods based on continuous-time piecewise deterministic Markov processes (PDMPs) suitable for inference in high dimensional sparse models, i.e. models for which there is prior knowledge that many coordinates are likely to be exactly 0. This is achieved with the fairly simple idea of endowing existing PDMP samplers with ``sticky''coordinate axes, coordinate planes etc. Upon hitting those subspaces, an event is triggered during which the process sticks to the subspace, this way spending some time in a sub-model. This results in non-reversible jumps between different (sub-)models. While we show that PDMP samplers in general can be made sticky, we mainly focus on the Zig-Zag sampler. Compared to the Gibbs sampler for variable selection, we heuristically derive favourable dependence of the Sticky Zig-Zag sampler on dimension and data size. The computational efficiency of the Sticky Zig-Zag sampler is further established through numerical experiments where both the sample size and the dimension of the parameter space are large.},
	author = {Bierkens, Joris and Grazzi, Sebastiano and Meulen, Frank van der and Schauer, Moritz},
	date = {2022/11/28},
	date-added = {2024-07-03 12:51:59 +0900},
	date-modified = {2024-07-03 12:51:59 +0900},
	doi = {10.1007/s11222-022-10180-5},
	id = {Bierkens2022},
	isbn = {1573-1375},
	journal = {Statistics and Computing},
	number = {1},
	pages = {8},
	title = {{Sticky PDMP Samplers for Sparse and Local Inference Problems}},
	url = {https://doi.org/10.1007/s11222-022-10180-5},
	volume = {33},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1007/s11222-022-10180-5}}

@unpublished{Grazzi2020,
    author = {Sebastiano Grazzi},
    year   = {2020},
    title  = {Piecewise Deterministic Monte Carlo},
    url    = {https://diamhomes.ewi.tudelft.nl/~jorisbierkens/pdmps.html}
}

@inproceedings{Ge+2018,
  author    = {Hong Ge and
               Kai Xu and
               Zoubin Ghahramani},
  title     = {Turing: a language for flexible probabilistic inference},
  booktitle = {International Conference on Artificial Intelligence and Statistics,
               {AISTATS} 2018, 9-11 April 2018, Playa Blanca, Lanzarote, Canary Islands,
               Spain},
  pages     = {1682--1690},
  year      = {2018},
  url       = {http://proceedings.mlr.press/v84/ge18b.html},
  biburl    = {https://dblp.org/rec/bib/conf/aistats/GeXG18},
}
@misc{Veltz2015,
      title={A new twist for the simulation of hybrid systems using the true jump method}, 
      author={Romain Veltz},
      year={2015},
      eprint={1504.06873},
      archivePrefix={arXiv},
      primaryClass={math.NA},
      url={https://arxiv.org/abs/1504.06873}, 
}

@unpublished{Lawler2019,
    author = {Gregory F. Lawler},
    year   = {2019},
    title  = {Notes on the Bessel Process},
    url    = {https://www.math.uchicago.edu/~lawler/bessel18new.pdf}
}


@article{Kohatsu-Higa2022,
	abstract = {We consider a real-valued diffusion process with a linear jump term driven by a Poisson point process and we assume that the jump amplitudes have a centered density with finite moments. We show upper and lower estimates for the density of the solution in the case that the jump amplitudes follow a Gaussian or Laplacian law. The proof of the lower bound uses a general expression for the density of the solution in terms of the convolution of the density of the continuous part and the jump amplitude density. The upper bound uses an upper tail estimate in terms of the jump amplitude distribution and techniques of the Malliavin calculus in order to bound the density by the tails of the solution. We also extend the lower bounds to the multidimensional case.},
	author = {Arturo Kohatsu-Higa and Eulalia Nualart and Ngoc Khue Tran},
	doi = {https://doi.org/10.1016/j.amc.2021.126814},
	issn = {0096-3003},
	journal = {Applied Mathematics and Computation},
	keywords = {Density estimates, Jump diffusion process, Malliavin calculus},
	pages = {126814},
	title = {Density estimates for jump diffusion processes},
	url = {https://www.sciencedirect.com/science/article/pii/S0096300321008973},
	volume = {420},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0096300321008973},
	bdsk-url-2 = {https://doi.org/10.1016/j.amc.2021.126814}}

@inproceedings{Kohatsu-Higa2003,
	abstract = {In this article we interpret heuristically the conditions of the definition of a uniformly elliptic random variable on Wiener space which allow to obtain Aronson type estimates for the density of this random variable. As an example we apply this concept to uniformly elliptic non-homogeneous diffusions.},
	address = {Basel},
	author = {Kohatsu-Higa, Arturo},
	booktitle = {Stochastic Inequalities and Applications},
	editor = {Gin{\'e}, Evariste and Houdr{\'e}, Christian and Nualart, David},
	isbn = {978-3-0348-8069-5},
	pages = {323--338},
	publisher = {Birkh{\"a}user Basel},
	title = {Lower Bounds for Densities of Uniformly Elliptic Non-homogeneous Diffusions},
	year = {2003},
    url             = {https://doi.org/10.1007/978-3-0348-8069-5_18},
}

@article{Taniguchi1985,
	author = {Setsuo Taniguchi},
	journal = {Osaka Journal of Mathematics},
	number = {2},
	pages = {307 -- 320},
	publisher = {Osaka University and Osaka Metropolitan University, Departments of Mathematics},
	title = {{Applications of Malliavin's calculus to time-dependent systems of heat equations}},
	volume = {22},
	year = {1985},
    url             = {https://projecteuclid.org/journals/osaka-journal-of-mathematics/volume-22/issue-2/Applications-of-Malliavins-calculus-to-time-dependent-systems-of-heat/ojm/1200778261.full},
}
@article{Li+2017,
    author = {Li, Cheng and Srivastava, Sanvesh and Dunson, David B.},
    title = "{Simple, scalable and accurate posterior interval estimation}",
    journal = {Biometrika},
    volume = {104},
    number = {3},
    pages = {665-680},
    year = {2017},
    month = {06},
    abstract = "{Standard posterior sampling algorithms, such as Markov chain Monte Carlo procedures, face major challenges in scaling up to massive datasets. We propose a simple and general posterior interval estimation algorithm to rapidly and accurately estimate quantiles of the posterior distributions for one-dimensional functionals. Our algorithm runs Markov chain Monte Carlo in parallel for subsets of the data, and then averages quantiles estimated from each subset. We provide strong theoretical guarantees and show that the credible intervals from our algorithm asymptotically approximate those from the full posterior in the leading parametric order. Our algorithm has a better balance of accuracy and efficiency than its competitors across a variety of simulations and a real-data example.}",
    issn = {0006-3444},
    doi = {10.1093/biomet/asx033},
    url = {https://doi.org/10.1093/biomet/asx033},
    eprint = {https://academic.oup.com/biomet/article-pdf/104/3/665/19563079/asx033.pdf},
}

@InProceedings{Srivastava+2015,
  title = 	 {{WASP: Scalable Bayes via barycenters of subset posteriors}},
  author = 	 {Srivastava, Sanvesh and Cevher, Volkan and Dinh, Quoc and Dunson, David},
  booktitle = 	 {Proceedings of the Eighteenth International Conference on Artificial Intelligence and Statistics},
  pages = 	 {912--920},
  year = 	 {2015},
  editor = 	 {Lebanon, Guy and Vishwanathan, S. V. N.},
  volume = 	 {38},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {San Diego, California, USA},
  month = 	 {09--12 May},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v38/srivastava15.pdf},
  url = 	 {https://proceedings.mlr.press/v38/srivastava15.html},
  abstract = 	 {The promise of Bayesian methods for big data sets has not fully been realized due to the lack of scalable computational algorithms. For massive data, it is necessary to store and process subsets on different machines in a distributed manner. We propose a simple, general, and highly efficient approach, which first runs a posterior sampling algorithm in parallel on different machines for subsets of a large data set. To combine these subset posteriors, we calculate the Wasserstein barycenter via a highly efficient linear program. The resulting estimate for the Wasserstein posterior (WASP) has an atomic form, facilitating straightforward estimation of posterior summaries of functionals of interest. The WASP approach allows posterior sampling algorithms for smaller data sets to be trivially scaled to huge data. We provide theoretical justification in terms of posterior consistency and algorithm efficiency.  Examples are provided in complex settings including Gaussian process regression and nonparametric Bayes mixture models.}
}
@inproceedings{Welling-Teh2011,
author = {Welling, Max and Teh, Yee Whye},
title = {Bayesian learning via stochastic gradient langevin dynamics},
year = {2011},
isbn = {9781450306195},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {In this paper we propose a new framework for learning from large scale datasets based on iterative learning from small mini-batches. By adding the right amount of noise to a standard stochastic gradient optimization algorithm we show that the iterates will converge to samples from the true posterior distribution as we anneal the stepsize. This seamless transition between optimization and Bayesian posterior sampling provides an inbuilt protection against overfitting. We also propose a practical method for Monte Carlo estimates of posterior statistics which monitors a "sampling threshold" and collects samples after it has been surpassed. We apply the method to three models: a mixture of Gaussians, logistic regression and ICA with natural gradients.},
booktitle = {Proceedings of the 28th International Conference on International Conference on Machine Learning},
pages = {681–688},
numpages = {8},
location = {Bellevue, Washington, USA},
series = {ICML'11},
url             = {https://dl.acm.org/doi/10.5555/3104482.3104568},
}

@article{Jarner-Tweedie2003,
 ISSN = {13507265},
 URL = {http://www.jstor.org/stable/3318785},
 abstract = {We give necessary conditions for geometric and polynomial convergence rates of random-walk-type Markov chains to stationarity in terms of existence of exponential and polynomial moments of the invariant distribution and the Markov transition kernel. These results complement the use of Foster-Lyapunov drift conditions for establishing geometric and polynomial ergodicity. For polynomially ergodic Markov chains, the results allow us to derive exact rates of convergence and exact relations between the moments of the invariant distribution and the Markov transition kernel. In an application to Markov chain Monte Carlo we derive tight rates of convergence for symmetric random walk Metropolis algorithms and Langevin algorithms with polynomial target densities.},
 author = {Søren F. Jarner and Richard L. Tweedie},
 journal = {Bernoulli},
 number = {4},
 pages = {559--578},
 publisher = {International Statistical Institute (ISI) and Bernoulli Society for Mathematical Statistics and Probability},
 title = {Necessary Conditions for Geometric and Polynomial Ergodicity of Random-Walk-Type Markov Chains},
 urldate = {2024-07-10},
 volume = {9},
 year = {2003}
}

@article{Polson+2013,
	author = {Nicholas G. Polson and James G. Scott and Jesse Windle},
	doi = {10.1080/01621459.2013.829001},
	eprint = {https://doi.org/10.1080/01621459.2013.829001},
	journal = {Journal of the American Statistical Association},
	number = {504},
	pages = {1339--1349},
	publisher = {Taylor \& Francis},
	title = {Bayesian Inference for Logistic Models Using P{\'o}lya--Gamma Latent Variables},
	url = {https://doi.org/10.1080/01621459.2013.829001},
	volume = {108},
	year = {2013},
	bdsk-url-1 = {https://doi.org/10.1080/01621459.2013.829001}}

@article{Albert-Chib1993,
	author = {James H. Albert and Siddhartha Chib},
	doi = {10.1080/01621459.1993.10476321},
	eprint = {https://www.tandfonline.com/doi/pdf/10.1080/01621459.1993.10476321},
	journal = {Journal of the American Statistical Association},
	number = {422},
	pages = {669--679},
	publisher = {Taylor \& Francis},
	title = {Bayesian Analysis of Binary and Polychotomous Response Data},
	url = {https://www.tandfonline.com/doi/abs/10.1080/01621459.1993.10476321},
	volume = {88},
	year = {1993},
	bdsk-url-1 = {https://www.tandfonline.com/doi/abs/10.1080/01621459.1993.10476321},
	bdsk-url-2 = {https://doi.org/10.1080/01621459.1993.10476321}}

@article{Johndrow+2019,
	author = {James E. Johndrow and Aaron Smith and Natesh Pillai and David B. Dunson},
	doi = {10.1080/01621459.2018.1505626},
	eprint = {https://doi.org/10.1080/01621459.2018.1505626},
	journal = {Journal of the American Statistical Association},
	number = {527},
	pages = {1394--1403},
	publisher = {Taylor \& Francis},
	title = {MCMC for Imbalanced Categorical Data},
	url = {https://doi.org/10.1080/01621459.2018.1505626},
	volume = {114},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1080/01621459.2018.1505626}}
@article{Owen2007,
  author  = {Art B. Owen},
  title   = {Infinitely Imbalanced Logistic Regression},
  journal = {Journal of Machine Learning Research},
  year    = {2007},
  volume  = {8},
  number  = {27},
  pages   = {761--773},
  url     = {http://jmlr.org/papers/v8/owen07a.html}
}

@article{Bierkens+2021,
	author = {Joris Bierkens and Pierre Nyquist and Mikola C. Schlottke},
	doi = {10.1214/21-AAP1663},
	journal = {The Annals of Applied Probability},
	keywords = {empirical measure, large deviations, Piecewise deterministic Markov process, zig-zag process},
	number = {6},
	pages = {2811 -- 2843},
	publisher = {Institute of Mathematical Statistics},
	title = {{Large Deviations for the Empirical Measure of the Zig-Zag Process}},
	url = {https://doi.org/10.1214/21-AAP1663},
	volume = {31},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1214/21-AAP1663}}
@misc{Fearnhead+2024,
      title={Stochastic Gradient Piecewise Deterministic Monte Carlo Samplers}, 
      author={Paul Fearnhead and Sebastiano Grazzi and Chris Nemeth and Gareth O. Roberts},
      year={2024},
      eprint={2406.19051},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/2406.19051}, 
}
@article{Johnson1970,
 ISSN = {00034851, 21688990},
 URL = {http://www.jstor.org/stable/2239239},
 author = {Richard A. Johnson},
 journal = {The Annals of Mathematical Statistics},
 number = {3},
 pages = {851--864},
 publisher = {Institute of Mathematical Statistics},
 title = {Asymptotic Expansions Associated with Posterior Distributions},
 urldate = {2024-07-17},
 volume = {41},
 year = {1970}
}

@article{Sur-Candes2019,
	abstract = {Students in statistics or data science usually learn early on that when the sample size n is large relative to the number of variables p, fitting a logistic model by the method of maximum likelihood produces estimates that are consistent and that there are well-known formulas that quantify the variability of these estimates which are used for the purpose of statistical inference. We are often told that these calculations are approximately valid if we have 5 to 10 observations per unknown parameter. This paper shows that this is far from the case, and consequently, inferences produced by common software packages are often unreliable. Consider a logistic model with independent features in which n and p become increasingly large in a fixed ratio. We prove that (i) the maximum-likelihood estimate (MLE) is biased, (ii) the variability of the MLE is far greater than classically estimated, and (iii) the likelihood-ratio test (LRT) is not distributed as a χ2. The bias of the MLE yields wrong predictions for the probability of a case based on observed values of the covariates. We present a theory, which provides explicit expressions for the asymptotic bias and variance of the MLE and the asymptotic distribution of the LRT. We empirically demonstrate that these results are accurate in finite samples. Our results depend only on a single measure of signal strength, which leads to concrete proposals for obtaining accurate inference in finite samples through the estimate of this measure.},
	author = {Pragya Sur and Emmanuel J. Cand{\`e}s},
	doi = {10.1073/pnas.1810420116},
	eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.1810420116},
	journal = {Proceedings of the National Academy of Sciences},
	number = {29},
	pages = {14516-14525},
	title = {{A Modern Maximum-Likelihood Theory for High-Dimensional Logistic Regression}},
	url = {https://www.pnas.org/doi/abs/10.1073/pnas.1810420116},
	volume = {116},
	year = {2019},
	bdsk-url-1 = {https://www.pnas.org/doi/abs/10.1073/pnas.1810420116},
	bdsk-url-2 = {https://doi.org/10.1073/pnas.1810420116}}
@ARTICLE{Candes-Tao2006,
  author={Candes, Emmanuel J. and Tao, Terence},
  journal={IEEE Transactions on Information Theory}, 
  title={Near-Optimal Signal Recovery From Random Projections: Universal Encoding Strategies?}, 
  year={2006},
  volume={52},
  number={12},
  pages={5406-5425},
  keywords={Encoding;Vectors;Image reconstruction;Mathematics;Digital images;Image coding;Measurement standards;Linear programming;Geometry;Concrete;Concentration of measure;convex optimization;duality in optimization;linear programming;random matrices;random projections;signal recovery;singular values of random matrices;sparsity;trigonometric expansions;uncertainty principle},
  doi={10.1109/TIT.2006.885507}}
@inproceedings{池田思朗2012,
 address = {京都大学},
 author = {池田思朗},
 booktitle = {第55回自動制御連合講演会講演論文集},
 month = {11},
 pages = {1043--1046},
 title = {スパースモデリングとベイズ統計},
 year = {2012},
 yomi = {Ikeda, Shiro}
}
@article{池田思朗+2004,
 author = {池田思朗 and 田中利幸 and 甘利俊一},
 journal = {統計数理},
 month = {12},
 number = {2},
 pages = {393--405},
 title = {情報幾何学に基づく確率伝搬法の解析},
 volume = {52},
 year = {2004},
 yomi = {Ikeda, Shiro}
}




@article{Sachs+2023,
	author = {Matthias Sachs and Deborshee Sen and Jianfeng Lu and David Dunson},
	doi = {10.1214/22-BA1319},
	journal = {Bayesian Analysis},
	keywords = {Gibbs sampler, Markov chain Monte Carlo, non-reversible, Piecewise deterministic Markov process, sub-sampling},
	number = {3},
	pages = {909 -- 927},
	publisher = {International Society for Bayesian Analysis},
	title = {{Posterior Computation with the Gibbs Zig-Zag Sampler}},
	url = {https://doi.org/10.1214/22-BA1319},
	volume = {18},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1214/22-BA1319}}

@article{Quiroz+2019,
	author = {Matias Quiroz and Robert Kohn and Mattias Villani and Minh-Ngoc Tran},
	doi = {10.1080/01621459.2018.1448827},
	eprint = {https://doi.org/10.1080/01621459.2018.1448827},
	journal = {Journal of the American Statistical Association},
	number = {526},
	pages = {831--843},
	publisher = {Taylor \& Francis},
	title = {Speeding Up MCMC by Efficient Data Subsampling},
	url = {https://doi.org/10.1080/01621459.2018.1448827},
	volume = {114},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1080/01621459.2018.1448827}}

@article{Dang+2019,
  author  = {Khue-Dung Dang and Matias Quiroz and Robert Kohn and Minh-Ngoc Tran and Mattias Villani},
  title   = {Hamiltonian Monte Carlo with Energy Conserving Subsampling},
  journal = {Journal of Machine Learning Research},
  year    = {2019},
  volume  = {20},
  number  = {100},
  pages   = {1-31},
  url     = {http://jmlr.org/papers/v20/17-452.html}
}

@inproceedings{Zhang+2020,
	author = {Zhang, Ruqi and Cooper, A. Feder and De Sa, Christopher M},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
	pages = {19500--19510},
	publisher = {Curran Associates, Inc.},
	title = {Asymptotically Optimal Exact Minibatch Metropolis-Hastings},
	url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/e2a7555f7cabd6e31aef45cb8cda4999-Paper.pdf},
	volume = {33},
	year = {2020},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2020/file/e2a7555f7cabd6e31aef45cb8cda4999-Paper.pdf}}
@article{Scott+2016,
title	= {Bayes and Big Data: The Consensus Monte Carlo Algorithm},
author	= {Steven L. Scott and Alexander W. Blocker and Fernando V. Bonassi and Hugh A. Chipman and Edward I. George and Robert E. McCulloch},year	= {2016},URL	= {http://www.tandfonline.com/doi/full/10.1080/17509653.2016.1142191},journal	= {International Journal of Management Science and Engineering Management},pages	= {78-88},volume	= {11}}


@article{Corbella+2022,
	abstract = {Novel Monte Carlo methods to generate samples from a target distribution, such as a posterior from a Bayesian analysis, have rapidly expanded in the past decade. Algorithms based on Piecewise Deterministic Markov Processes (PDMPs), non-reversible continuous-time processes, are developing into their own research branch, thanks their important properties (e.g., super-efficiency). Nevertheless, practice has not caught up with the theory in this field, and the use of PDMPs to solve applied problems is not widespread. This might be due, firstly, to several implementational challenges that PDMP-based samplers present with and, secondly, to the lack of papers that showcase the methods and implementations in applied settings. Here, we address both these issues using one of the most promising PDMPs, the Zig-Zag sampler, as an archetypal example. After an explanation of the key elements of the Zig-Zag sampler, its implementation challenges are exposed and addressed. Specifically, the formulation of an algorithm that draws samples from a target distribution of interest is provided. Notably, the only requirement of the algorithm is a closed-form differentiable function to evaluate the log-target density of interest, and, unlike previous implementations, no further information on the target is needed. The performance of the algorithm is evaluated against canonical Hamiltonian Monte Carlo, and it is proven to be competitive, in simulation and real-data settings. Lastly, we demonstrate that the super-efficiency property, i.e. the ability to draw one independent sample at a lesser cost than evaluating the likelihood of all the data, can be obtained in practice.},
	author = {Corbella, Alice and Spencer, Simon E. F. and Roberts, Gareth O.},
	date = {2022/11/09},
	date-added = {2024-07-24 21:14:56 +0900},
	date-modified = {2024-07-24 21:14:56 +0900},
	doi = {10.1007/s11222-022-10142-x},
	id = {Corbella2022},
	isbn = {1573-1375},
	journal = {Statistics and Computing},
	number = {6},
	pages = {107},
	title = {{Automatic Zig-Zag Sampling in Practice}},
	url = {https://doi.org/10.1007/s11222-022-10142-x},
	volume = {32},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1007/s11222-022-10142-x}}

@article{Sutton-Fearnhead2023,
	author = {Matthew Sutton and Paul Fearnhead},
	doi = {10.1080/10618600.2023.2203735},
	eprint = {https://doi.org/10.1080/10618600.2023.2203735},
	journal = {Journal of Computational and Graphical Statistics},
	number = {4},
	pages = {1425--1435},
	publisher = {Taylor \& Francis},
	title = {{Concave-Convex PDMP-based Sampling}},
	url = {https://doi.org/10.1080/10618600.2023.2203735},
	volume = {32},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1080/10618600.2023.2203735}}

@article{Pagani+2023,
	abstract = {Markov chain Monte Carlo (MCMC) is a key algorithm in computational statistics, and as datasets grow larger and models grow more complex, many popular MCMC algorithms become too computationally expensive to be practical. Recent progress has been made on this problem through development of MCMC algorithms based on Piecewise Deterministic Markov Processes (PDMPs), irreversible processes which can be engineered to converge at a rate which is independent of the size of the dataset. While there has understandably been a surge of theoretical studies following these results, PDMPs have so far only been implemented for models where certain gradients can be bounded in closed form, which is not possible in many relevant statistical problems. Furthermore, there has been substantionally less focus on practical implementation, or the efficiency of PDMP dynamics in exploring challenging densities. Focusing on the Zig-Zag process, we present the Numerical Zig-Zag (NuZZ) algorithm, which is applicable to general statistical models without the need for bounds on the gradient of the log posterior. This allows us to perform numerical experiments on: (i) how the Zig-Zag dynamics behaves on some test problems with common challenging features; and (ii) how the error between the target and sampled distributions evolves as a function of computational effort for different MCMC algorithms including NuZZ. Moreover, due to the specifics of the NuZZ algorithms, we are able to give an explicit bound on the Wasserstein distance between the exact posterior and its numerically perturbed counterpart in terms of the user-specified numerical tolerances of NuZZ.},
	author = {Pagani, Filippo and Chevallier, Augustin and Power, Sam and House, Thomas and Cotter, Simon},
	date = {2024/01/05},
	date-added = {2024-07-24 21:17:39 +0900},
	date-modified = {2024-07-24 21:17:39 +0900},
	doi = {10.1007/s11222-023-10363-8},
	id = {Pagani2024},
	isbn = {1573-1375},
	journal = {Statistics and Computing},
	number = {1},
	pages = {61},
	title = {NuZZ: Numerical Zig-Zag for general models},
	url = {https://doi.org/10.1007/s11222-023-10363-8},
	volume = {34},
	year = {2024},
	bdsk-url-1 = {https://doi.org/10.1007/s11222-023-10363-8}}

@article{Bierkens-Roberts-Zigg2019,
	author = {Joris Bierkens and Gareth O. Roberts and Pierre-Andr{\'e} Zitt},
	doi = {10.1214/18-AAP1453},
	journal = {The Annals of Applied Probability},
	keywords = {central limit theorem, ergodicity, exponential ergodicity, irreducibility, Piecewise deterministic Markov process},
	number = {4},
	pages = {2266 -- 2301},
	publisher = {Institute of Mathematical Statistics},
	title = {{Ergodicity of the zigzag process}},
	url = {https://doi.org/10.1214/18-AAP1453},
	volume = {29},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1214/18-AAP1453}}
@misc{Carbone2024,
      title={Hitchhiker's guide on Energy-Based Models: a comprehensive review on the relation with other generative models, sampling and statistical physics}, 
      author={Davide Carbone},
      year={2024},
      eprint={2406.13661},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2406.13661}, 
}

@article{Winn-Bishop2005,
  author  = {John Winn and Christopher M. Bishop},
  title   = {Variational Message Passing},
  journal = {Journal of Machine Learning Research},
  year    = {2005},
  volume  = {6},
  number  = {23},
  pages   = {661--694},
  url     = {http://jmlr.org/papers/v6/winn05a.html}
}

@article{Fortunato2010,
	abstract = {The modern science of networks has brought significant advances to our understanding of complex systems. One of the most relevant features of graphs representing real systems is community structure, or clustering, i.e. the organization of vertices in clusters, with many edges joining vertices of the same cluster and comparatively few edges joining vertices of different clusters. Such clusters, or communities, can be considered as fairly independent compartments of a graph, playing a similar role like, e.g., the tissues or the organs in the human body. Detecting communities is of great importance in sociology, biology and computer science, disciplines where systems are often represented as graphs. This problem is very hard and not yet satisfactorily solved, despite the huge effort of a large interdisciplinary community of scientists working on it over the past few years. We will attempt a thorough exposition of the topic, from the definition of the main elements of the problem, to the presentation of most methods developed, with a special focus on techniques designed by statistical physicists, from the discussion of crucial issues like the significance of clustering and how methods should be tested and compared against each other, to the description of applications to real networks.},
	author = {Santo Fortunato},
	doi = {https://doi.org/10.1016/j.physrep.2009.11.002},
	issn = {0370-1573},
	journal = {Physics Reports},
	keywords = {Graphs, Clusters, Statistical physics},
	number = {3},
	pages = {75-174},
	title = {Community detection in graphs},
	url = {https://www.sciencedirect.com/science/article/pii/S0370157309002841},
	volume = {486},
	year = {2010},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0370157309002841},
	bdsk-url-2 = {https://doi.org/10.1016/j.physrep.2009.11.002}}

@article{Girvan-Newman2002,
	abstract = {A number of recent studies have focused on the statistical properties of networked systems such as social networks and the Worldwide Web. Researchers have concentrated particularly on a few properties that seem to be common to many networks: the small-world property, power-law degree distributions, and network transitivity. In this article, we highlight another property that is found in many networks, the property of community structure, in which network nodes are joined together in tightly knit groups, between which there are only looser connections. We propose a method for detecting such communities, built around the idea of using centrality indices to find community boundaries. We test our method on computer-generated and real-world graphs whose community structure is already known and find that the method detects this known structure with high sensitivity and reliability. We also apply the method to two networks whose community structure is not well known---a collaboration network and a food web---and find that it detects significant and informative community divisions in both cases.},
	author = {M. Girvan and M. E. J. Newman},
	doi = {10.1073/pnas.122653799},
	eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.122653799},
	journal = {Proceedings of the National Academy of Sciences},
	number = {12},
	pages = {7821-7826},
	title = {Community structure in social and biological networks},
	url = {https://www.pnas.org/doi/abs/10.1073/pnas.122653799},
	volume = {99},
	year = {2002},
	bdsk-url-1 = {https://www.pnas.org/doi/abs/10.1073/pnas.122653799},
	bdsk-url-2 = {https://doi.org/10.1073/pnas.122653799}}

@article{村田剛志2009,
	author = {村田剛志},
	doi = {10.3156/jsoft.21.4_500},
	journal = {知能と情報},
	number = {4},
	pages = {500-508},
	title = {ネットワークからのコミュニティ抽出},
	volume = {21},
	year = {2009},
	bdsk-url-1 = {https://doi.org/10.3156/jsoft.21.4_500}}
@article{Newman2004,
  title = {Fast algorithm for detecting community structure in networks},
  author = {Newman, M. E. J.},
  journal = {Phys. Rev. E},
  volume = {69},
  issue = {6},
  pages = {066133},
  numpages = {5},
  year = {2004},
  month = {Jun},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevE.69.066133},
  url = {https://link.aps.org/doi/10.1103/PhysRevE.69.066133}
}
@article{Clauset+2004,
  title = {Finding community structure in very large networks},
  author = {Clauset, Aaron and Newman, M. E. J. and Moore, Cristopher},
  journal = {Phys. Rev. E},
  volume = {70},
  issue = {6},
  pages = {066111},
  numpages = {6},
  year = {2004},
  month = {Dec},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevE.70.066111},
  url = {https://link.aps.org/doi/10.1103/PhysRevE.70.066111}
}
@article{Guimera+2004,
  title = {Modularity from fluctuations in random graphs and complex networks},
  author = {Guimer\`a, Roger and Sales-Pardo, Marta and Amaral, Lu\'{\i}s A. Nunes},
  journal = {Phys. Rev. E},
  volume = {70},
  issue = {2},
  pages = {025101},
  numpages = {4},
  year = {2004},
  month = {Aug},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevE.70.025101},
  url = {https://link.aps.org/doi/10.1103/PhysRevE.70.025101}
}
@article{Reichardt-Bornholdt2006,
  title = {Statistical mechanics of community detection},
  author = {Reichardt, J\"org and Bornholdt, Stefan},
  journal = {Phys. Rev. E},
  volume = {74},
  issue = {1},
  pages = {016110},
  numpages = {14},
  year = {2006},
  month = {Jul},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevE.74.016110},
  url = {https://link.aps.org/doi/10.1103/PhysRevE.74.016110}
}
@ARTICLE{Dpnath-Hoffman1973,
  author={Donath, W. E. and Hoffman, A. J.},
  journal={IBM Journal of Research and Development}, 
  title={Lower Bounds for the Partitioning of Graphs}, 
  year={1973},
  volume={17},
  number={5},
  pages={420-425},
  keywords={},
  doi={10.1147/rd.175.0420}}
@article{Donetti-Munoz2004,
doi = {10.1088/1742-5468/2004/10/P10012},
url = {https://dx.doi.org/10.1088/1742-5468/2004/10/P10012},
year = {2004},
month = {oct},
publisher = {},
volume = {2004},
number = {10},
pages = {P10012},
author = {Luca Donetti and  Miguel A Muñoz},
title = {Detecting network communities: a new systematic and efficient algorithm},
journal = {Journal of Statistical Mechanics: Theory and Experiment},
abstract = {An efficient and relatively fast algorithm for the detection of communities in complex networks is introduced. The method exploits spectral properties of the graph Laplacian matrix combined with hierarchical clustering techniques, and includes a procedure for maximizing the ‘modularity’ of the output. Its performance is compared with that of other existing methods, as applied to different well-known instances of complex networks with a community structure, both computer generated and from the real world. Our results are, in all the cases tested, at least as good as the best ones obtained with any other methods, and faster in most of the cases than methods providing similar quality results. This converts the algorithm into a valuable computational tool for detecting and analysing communities and modular structures in complex networks.}
}
@article{Hastings2006,
  title = {Community detection as an inference problem},
  author = {Hastings, M. B.},
  journal = {Phys. Rev. E},
  volume = {74},
  issue = {3},
  pages = {035102},
  numpages = {4},
  year = {2006},
  month = {Sep},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevE.74.035102},
  url = {https://link.aps.org/doi/10.1103/PhysRevE.74.035102}
}

@article{Newman-Leicht2007,
	abstract = {Networks are widely used in the biological, physical, and social sciences as a concise mathematical representation of the topology of systems of interacting components. Understanding the structure of these networks is one of the outstanding challenges in the study of complex systems. Here we describe a general technique for detecting structural features in large-scale network data that works by dividing the nodes of a network into classes such that the members of each class have similar patterns of connection to other nodes. Using the machinery of probabilistic mixture models and the expectation--maximization algorithm, we show that it is possible to detect, without prior knowledge of what we are looking for, a very broad range of types of structure in networks. We give a number of examples demonstrating how the method can be used to shed light on the properties of real-world networks, including social and information networks.},
	author = {M. E. J. Newman and E. A. Leicht},
	doi = {10.1073/pnas.0610537104},
	eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.0610537104},
	journal = {Proceedings of the National Academy of Sciences},
	number = {23},
	pages = {9564-9569},
	title = {Mixture models and exploratory analysis in networks},
	url = {https://www.pnas.org/doi/abs/10.1073/pnas.0610537104},
	volume = {104},
	year = {2007},
	bdsk-url-1 = {https://www.pnas.org/doi/abs/10.1073/pnas.0610537104},
	bdsk-url-2 = {https://doi.org/10.1073/pnas.0610537104}}

@article{Snijders-Nowicki1997,
	abstract = {blockmodeling for graphs is proposed. The model assumes that the vertices of the graph are partitioned into two unknown blocks and that the probability of an edge between two vertices depends only on the blocks to which they belong. Statistical procedures are derived for estimating the probabilities of edges and for predicting the block structure from observations of the edge pattern only. ML estimators can be computed using the EM algorithm, but this strategy is practical only for small graphs. A Bayesian estimator, based on the Gibbs sampling, is proposed. This estimator is practical also for large graphs. When ML estimators are used, the block structure can be predicted based on predictive likelihood. When Gibbs sampling is used, the block structure can be predicted from posterior predictive probabilities. A side result is that when the number of vertices tends to infinity while the probabilities remain constant, the block structure can be recovered correctly with probability tending to 1.},
	author = {Snijders, Tom A. B. and Nowicki, Krzysztof},
	date = {1997/01/01},
	date-added = {2024-07-26 20:42:31 +0900},
	date-modified = {2024-07-26 20:42:31 +0900},
	doi = {10.1007/s003579900004},
	id = {Snijders1997},
	isbn = {1432-1343},
	journal = {Journal of Classification},
	number = {1},
	pages = {75--100},
	title = {Estimation and Prediction for Stochastic Blockmodels for Graphs with Latent Block Structure},
	url = {https://doi.org/10.1007/s003579900004},
	volume = {14},
	year = {1997},
	bdsk-url-1 = {https://doi.org/10.1007/s003579900004}}
@article{Decelle2011,
  title = {Asymptotic analysis of the stochastic block model for modular networks and its algorithmic applications},
  author = {Decelle, Aurelien and Krzakala, Florent and Moore, Cristopher and Zdeborov\'a, Lenka},
  journal = {Phys. Rev. E},
  volume = {84},
  issue = {6},
  pages = {066106},
  numpages = {19},
  year = {2011},
  month = {Dec},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevE.84.066106},
  url = {https://link.aps.org/doi/10.1103/PhysRevE.84.066106}
}

@article{Lustig+2007,
	abstract = {Abstract The sparsity which is implicit in MR images is exploited to significantly undersample k-space. Some MR images such as angiograms are already sparse in the pixel representation; other, more complicated images have a sparse representation in some transform domain--for example, in terms of spatial finite-differences or their wavelet coefficients. According to the recently developed mathematical theory of compressed-sensing, images with a sparse representation can be recovered from randomly undersampled k-space data, provided an appropriate nonlinear recovery scheme is used. Intuitively, artifacts due to random undersampling add as noise-like interference. In the sparse transform domain the significant coefficients stand out above the interference. A nonlinear thresholding scheme can recover the sparse coefficients, effectively recovering the image itself. In this article, practical incoherent undersampling schemes are developed and analyzed by means of their aliasing interference. Incoherence is introduced by pseudo-random variable-density undersampling of phase-encodes. The reconstruction is performed by minimizing the ℓ1 norm of a transformed image, subject to data fidelity constraints. Examples demonstrate improved spatial resolution and accelerated acquisition for multislice fast spin-echo brain imaging and 3D contrast enhanced angiography. Magn Reson Med, 2007. {\copyright} 2007 Wiley-Liss, Inc.},
	author = {Lustig, Michael and Donoho, David and Pauly, John M.},
	doi = {https://doi.org/10.1002/mrm.21391},
	eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/mrm.21391},
	journal = {Magnetic Resonance in Medicine},
	keywords = {compressed sensing, compressive sampling, random sampling, rapid MRI, sparsity, sparse reconstruction, nonlinear reconstruction},
	number = {6},
	pages = {1182-1195},
	title = {Sparse MRI: The application of compressed sensing for rapid MR imaging},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/mrm.21391},
	volume = {58},
	year = {2007},
	bdsk-url-1 = {https://onlinelibrary.wiley.com/doi/abs/10.1002/mrm.21391},
	bdsk-url-2 = {https://doi.org/10.1002/mrm.21391}}

@book{Pinkus1985,
    author         = {Allan Pinkus},
    year           = {1985},
    title          = {{$n$-Width in Approximation Theory}},
    series         = {},
    volume         = {},
    edition        = {},
    url            = {https://doi.org/10.1007/978-3-642-69894-1},
    publisher      = {Springer Berlin, Heidelberg}
}

@article{Chen+1998,
	abstract = { The time-frequency and time-scale communities have recently developed a large number of overcomplete waveform dictionaries --- stationary wavelets, wavelet packets, cosine packets, chirplets, and warplets, to name a few. Decomposition into overcomplete systems is not unique, and several methods for decomposition have been proposed, including the method of frames (MOF), Matching pursuit (MP), and, for special dictionaries, the best orthogonal basis (BOB).Basis Pursuit (BP) is a principle for decomposing a signal into an "optimal" superposition of dictionary elements, where optimal means having the smallest l1 norm of coefficients among all such decompositions. We give examples exhibiting several advantages over MOF, MP, and BOB, including better sparsity and superresolution. BP has interesting relations to ideas in areas as diverse as ill-posed problems, in abstract harmonic analysis, total variation denoising, and multiscale edge denoising.BP in highly overcomplete dictionaries leads to large-scale optimization problems. With signals of length 8192 and a wavelet packet dictionary, one gets an equivalent linear program of size 8192 by 212,992. Such problems can be attacked successfully only because of recent advances in linear programming by interior-point methods. We obtain reasonable success with a primal-dual logarithmic barrier method and conjugate-gradient solver. },
	author = {Chen, Scott Shaobing and Donoho, David L. and Saunders, Michael A.},
	doi = {10.1137/S1064827596304010},
	eprint = {https://doi.org/10.1137/S1064827596304010},
	journal = {SIAM Journal on Scientific Computing},
	number = {1},
	pages = {33-61},
	title = {Atomic Decomposition by Basis Pursuit},
	url = {https://doi.org/10.1137/S1064827596304010},
	volume = {20},
	year = {1998},
	bdsk-url-1 = {https://doi.org/10.1137/S1064827596304010}}
@ARTICLE{Donoho2016,
  author={Donoho, D.L.},
  journal={IEEE Transactions on Information Theory}, 
  title={Compressed sensing}, 
  year={2006},
  volume={52},
  number={4},
  pages={1289-1306},
  keywords={Compressed sensing;Image reconstruction;Pixel;Vectors;Digital images;Image coding;Transform coding;Size measurement;Signal processing;Data mining;Adaptive sampling;almost-spherical sections of Banach spaces;Basis Pursuit;eigenvalues of random matrices;Gel'fand;information-based complexity;integrated sensing and processing;minimum;optimal recovery;Quotient-of-a-Subspace theorem;sparse solution of linear equations},
  doi={10.1109/TIT.2006.871582}}
@ARTICLE{Maleki-Donoho2010,
  author={Maleki, Arian and Donoho, David L.},
  journal={IEEE Journal of Selected Topics in Signal Processing}, 
  title={Optimally Tuned Iterative Reconstruction Algorithms for Compressed Sensing}, 
  year={2010},
  volume={4},
  number={2},
  pages={330-341},
  keywords={Reconstruction algorithms;Compressed sensing;Iterative algorithms;Signal processing algorithms;Proposals;Equations;Pursuit algorithms;Sparse matrices;Linear systems;Distributed computing;Compressed sensing;iterative algorithms;phase transition;random matrices;thresholding},
  doi={10.1109/JSTSP.2009.2039176}}
@ARTICLE{Kschischang+2001,
  author={Kschischang, F.R. and Frey, B.J. and Loeliger, H.-A.},
  journal={IEEE Transactions on Information Theory}, 
  title={Factor graphs and the sum-product algorithm}, 
  year={2001},
  volume={47},
  number={2},
  pages={498-519},
  keywords={Graph theory},
  doi={10.1109/18.910572}}

@article{Donoho+2009,
	abstract = {Compressed sensing aims to undersample certain high-dimensional signals yet accurately reconstruct them by exploiting signal characteristics. Accurate reconstruction is possible when the object to be recovered is sufficiently sparse in a known basis. Currently, the best known sparsity--undersampling tradeoff is achieved when reconstructing by convex optimization, which is expensive in important large-scale applications. Fast iterative thresholding algorithms have been intensively studied as alternatives to convex optimization for large-scale problems. Unfortunately known fast algorithms offer substantially worse sparsity--undersampling tradeoffs than convex optimization. We introduce a simple costless modification to iterative thresholding making the sparsity--undersampling tradeoff of the new algorithms equivalent to that of the corresponding convex optimization procedures. The new iterative-thresholding algorithms are inspired by belief propagation in graphical models. Our empirical measurements of the sparsity--undersampling tradeoff for the new algorithms agree with theoretical calculations. We show that a state evolution formalism correctly derives the true sparsity--undersampling tradeoff. There is a surprising agreement between earlier calculations based on random convex polytopes and this apparently very different theoretical formalism.},
	author = {David L. Donoho and Arian Maleki and Andrea Montanari},
	doi = {10.1073/pnas.0909892106},
	eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.0909892106},
	journal = {Proceedings of the National Academy of Sciences},
	number = {45},
	pages = {18914-18919},
	title = {Message-passing algorithms for compressed sensing},
	url = {https://www.pnas.org/doi/abs/10.1073/pnas.0909892106},
	volume = {106},
	year = {2009},
	bdsk-url-1 = {https://www.pnas.org/doi/abs/10.1073/pnas.0909892106},
	bdsk-url-2 = {https://doi.org/10.1073/pnas.0909892106}}

@article{Antenucci+2019,
	abstract = {Approximate message passing algorithm enjoyed considerable attention in the last decade. In this paper we introduce a variant of the AMP algorithm that takes into account glassy nature of the system under consideration. We coin this algorithm as the approximate survey propagation (ASP) and derive it for a class of low-rank matrix estimation problems. We derive the state evolution for the ASP algorithm and prove that it reproduces the one-step replica symmetry breaking (1RSB) fixed-point equations, well-known in physics of disordered systems. Our derivation thus gives a concrete algorithmic meaning to the 1RSB equations that is of independent interest. We characterize the performance of ASP in terms of convergence and mean-squared error as a function of the free Parisi parameter s. We conclude that when there is a model mismatch between the true generative model and the inference model, the performance of AMP rapidly degrades both in terms of MSE and of convergence, while for well-chosen values of the Parisi parameter s ASP converges in a larger regime and can reach lower errors. Among other results, our analysis leads us to a striking hypothesis that whenever s (or other parameters) can be set in such a way that the Nishimori condition M  =  Q  &gt;  0 is restored, then the corresponding algorithm is able to reach mean-squared error as low as the Bayes-optimal error obtained when the model and its parameters are known and exactly matched in the inference procedure. The remaining drawback is that we have not found a procedure that would systematically find a value of s leading to such low errors, this is a challenging problem let for future work.},
	author = {Fabrizio Antenucci and Florent Krzakala and Pierfrancesco Urbani and Lenka Zdeborov{\'a}},
	doi = {10.1088/1742-5468/aafa7d},
	journal = {Journal of Statistical Mechanics: Theory and Experiment},
	month = {feb},
	number = {2},
	pages = {023401},
	publisher = {IOP Publishing and SISSA},
	title = {Approximate survey propagation for statistical inference},
	url = {https://dx.doi.org/10.1088/1742-5468/aafa7d},
	volume = {2019},
	year = {2019},
	bdsk-url-1 = {https://dx.doi.org/10.1088/1742-5468/aafa7d}}
@article{El-Showk+2012,
  title = {Solving the 3D Ising model with the conformal bootstrap},
  author = {El-Showk, Sheer and Paulos, Miguel F. and Poland, David and Rychkov, Slava and Simmons-Duffin, David and Vichi, Alessandro},
  journal = {Phys. Rev. D},
  volume = {86},
  issue = {2},
  pages = {025022},
  numpages = {17},
  year = {2012},
  month = {Jul},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevD.86.025022},
  url = {https://link.aps.org/doi/10.1103/PhysRevD.86.025022}
}

@article{Bierkens-Kamatani-Roberts2022,
	author = {Joris Bierkens and Kengo Kamatani and Gareth O. Roberts},
	doi = {10.1214/21-AAP1762},
	journal = {The Annals of Applied Probability},
	keywords = {exponential ergodicity, Gaussian process, Markov chain Monte Carlo, Piecewise deterministic Markov processes, weak convergence},
	number = {5},
	pages = {3361 -- 3407},
	publisher = {Institute of Mathematical Statistics},
	title = {{High-dimensional scaling limits of piecewise deterministic sampling algorithms}},
	url = {https://doi.org/10.1214/21-AAP1762},
	volume = {32},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1214/21-AAP1762}}

@article{Deligiannidis+2021,
	author = {George Deligiannidis and Daniel Paulin and Alexandre Bouchard-C{\^o}t{\'e} and Arnaud Doucet},
	doi = {10.1214/20-AAP1659},
	journal = {The Annals of Applied Probability},
	keywords = {Bouncy particle sampler, coupling, hypocoercivity, Randomized Hamiltonian Monte Carlo, weak convergence},
	number = {6},
	pages = {2612 -- 2662},
	publisher = {Institute of Mathematical Statistics},
	title = {{Randomized Hamiltonian Monte Carlo as scaling limit of the bouncy particle sampler and dimension-free convergence rates}},
	url = {https://doi.org/10.1214/20-AAP1659},
	volume = {31},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1214/20-AAP1659}}

@article{Andrieu+2021,
	author = {Christophe Andrieu and Alain Durmus and Nikolas N{\"u}sken and Julien Roussel},
	doi = {10.1214/20-AAP1653},
	journal = {The Annals of Applied Probability},
	keywords = {geometric convergence, Hypoellipticity, PDMCMC},
	number = {5},
	pages = {2478 -- 2517},
	publisher = {Institute of Mathematical Statistics},
	title = {{Hypocoercivity of piecewise deterministic Markov process-Monte Carlo}},
	url = {https://doi.org/10.1214/20-AAP1653},
	volume = {31},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1214/20-AAP1653}}
@article{Fabrizio+2019,
  title = {Glassy Nature of the Hard Phase in Inference Problems},
  author = {Antenucci, Fabrizio and Franz, Silvio and Urbani, Pierfrancesco and Zdeborov\'a, Lenka},
  journal = {Phys. Rev. X},
  volume = {9},
  issue = {1},
  pages = {011020},
  numpages = {10},
  year = {2019},
  month = {Jan},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevX.9.011020},
  url = {https://link.aps.org/doi/10.1103/PhysRevX.9.011020}
}

@book{ラトゥール1999,
    author         = {ブルーノ・ラトゥール},
    year           = {1999},
    title          = {科学が作られているとき――人類学的考察},
    series         = {},
    volume         = {},
    edition        = {},
    url            = {},
    publisher      = {産業図書},
    note           = {川崎勝・高田紀代志訳},
}

@phdthesis{本武陽一2017,
    author      = {本武陽一},
    school      = {東京大学},
    title       = {高次元データセットに潜む幾何構造と深層学習 : その解析と大自由度力学系への応用},
    year        = {2017},
    url         = {https://repository.dl.itc.u-tokyo.ac.jp/records/48134},
}

@book{ConvexityAndConcentration2017,
    author         = {},
    editor         = {Eric Carlen and Mokshay Madiman and Elisabeth M. Werner},
    year           = {2017},
    title          = {Convexity and Concentration},
    series         = {The IMA Volumes in Mathematics and its Applications},
    volume         = {161},
    edition        = {},
    url            = {https://doi.org/10.1007/978-1-4939-7005-6},
    publisher      = {Springer New York}
}

@misc{Zhu+2024,
      title={Is Sora a World Simulator? A Comprehensive Survey on General World Models and Beyond}, 
      author={Zheng Zhu and Xiaofeng Wang and Wangbo Zhao and Chen Min and Nianchen Deng and Min Dou and Yuqi Wang and Botian Shi and Kai Wang and Chi Zhang and Yang You and Zhaoxiang Zhang and Dawei Zhao and Liang Xiao and Jian Zhao and Jiwen Lu and Guan Huang},
      year={2024},
      eprint={2405.03520},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2405.03520}, 
}

@article{Kersten+2004,
	abstract = {We perceive the shapes and material properties of objects quickly and reliably despite the complexity and objective ambiguities of natural images. Typical images are highly complex because they consist of many objects embedded in background clutter. Moreover, the image features of an object are extremely variable and ambiguous owing to the effects of projection, occlusion, background clutter, and illumination. The very success of everyday vision implies neural mechanisms, yet to be understood, that discount irrelevant information and organize ambiguous or noisy local image features into objects and surfaces. Recent work in Bayesian theories of visual perception has shown how complexity may be managed and ambiguity resolved through the task-dependent, probabilistic integration of prior object knowledge with image features.},
	author = {Kersten, Daniel and Mamassian, Pascal and Yuille, Alan},
	doi = {https://doi.org/10.1146/annurev.psych.55.090902.142005},
	issn = {1545-2085},
	journal = {Annual Review of Psychology},
	keywords = {vision},
	number = {Volume 55, 2004},
	pages = {271-304},
	publisher = {Annual Reviews},
	title = {Object Perception as Bayesian Inference},
	type = {Journal Article},
	url = {https://www.annualreviews.org/content/journals/10.1146/annurev.psych.55.090902.142005},
	volume = {55},
	year = {2004},
	bdsk-url-1 = {https://www.annualreviews.org/content/journals/10.1146/annurev.psych.55.090902.142005},
	bdsk-url-2 = {https://doi.org/10.1146/annurev.psych.55.090902.142005}}

@article{Yuille-Kersten2006,
	abstract = {We argue that the study of human vision should be aimed at determining how humans perform natural tasks with natural images. Attempts to understand the phenomenology of vision from artificial stimuli, although worthwhile as a starting point, can lead to faulty generalizations about visual systems, because of the enormous complexity of natural images. Dealing with this complexity is daunting, but Bayesian inference on structured probability distributions offers the ability to design theories of vision that can deal with the complexity of natural images, and that use `analysis by synthesis' strategies with intriguing similarities to the brain. We examine these strategies using recent examples from computer vision, and outline some important imlications for cognitive science.},
	author = {Alan Yuille and Daniel Kersten},
	doi = {https://doi.org/10.1016/j.tics.2006.05.002},
	issn = {1364-6613},
	journal = {Trends in Cognitive Sciences},
	note = {Special issue: Probabilistic models of cognition},
	number = {7},
	pages = {301-308},
	title = {{Vision as Bayesian Inference: Analysis by Synthesis?}},
	url = {https://www.sciencedirect.com/science/article/pii/S1364661306001264},
	volume = {10},
	year = {2006},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S1364661306001264},
	bdsk-url-2 = {https://doi.org/10.1016/j.tics.2006.05.002}}
@inproceedings{Chen+2017,
title={{Variational Lossy Autoencoder}},
author={Xi Chen and Diederik P. Kingma and Tim Salimans and Yan Duan and Prafulla Dhariwal and John Schulman and Ilya Sutskever and Pieter Abbeel},
booktitle={International Conference on Learning Representations},
year={2017},
url={https://openreview.net/forum?id=BysvGP5ee}
}
@article{Lawrence2005,
  author  = {Neil Lawrence},
  title   = {Probabilistic Non-linear Principal Component Analysis with Gaussian Process Latent Variable Models},
  journal = {Journal of Machine Learning Research},
  year    = {2005},
  volume  = {6},
  number  = {60},
  pages   = {1783--1816},
  url     = {http://jmlr.org/papers/v6/lawrence05a.html}
}

@article{Bourlard-Kamp1988,
	abstract = {The multilayer perceptron, when working in auto-association mode, is sometimes considered as an interesting candidate to perform data compression or dimensionality reduction of the feature space in information processing applications. The present paper shows that, for auto-association, the nonlinearities of the hidden units are useless and that the optimal parameter values can be derived directly by purely linear techniques relying on singular value decomposition and low rank matrix approximation, similar in spirit to the well-known Karhunen-Lo{\`e}ve transform. This approach appears thus as an efficient alternative to the general error back-propagation algorithm commonly used for training multilayer perceptrons. Moreover, it also gives a clear interpretation of the r{\^o}le of the different parameters.},
	author = {Bourlard, H. and Kamp, Y.},
	date = {1988/09/01},
	date-added = {2024-07-29 12:12:02 +0900},
	date-modified = {2024-07-29 12:12:02 +0900},
	doi = {10.1007/BF00332918},
	id = {Bourlard1988},
	isbn = {1432-0770},
	journal = {Biological Cybernetics},
	number = {4},
	pages = {291--294},
	title = {Auto-association by multilayer perceptrons and singular value decomposition},
	url = {https://doi.org/10.1007/BF00332918},
	volume = {59},
	year = {1988},
	bdsk-url-1 = {https://doi.org/10.1007/BF00332918}}
@inproceedings{Vincent+2008,
author = {Vincent, Pascal and Larochelle, Hugo and Bengio, Yoshua and Manzagol, Pierre-Antoine},
title = {Extracting and composing robust features with denoising autoencoders},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390294},
doi = {10.1145/1390156.1390294},
abstract = {Previous work has shown that the difficulties in learning deep generative or discriminative models can be overcome by an initial unsupervised learning step that maps inputs to useful intermediate representations. We introduce and motivate a new training principle for unsupervised learning of a representation based on the idea of making the learned representations robust to partial corruption of the input pattern. This approach can be used to train autoencoders, and these denoising autoencoders can be stacked to initialize deep architectures. The algorithm can be motivated from a manifold learning and information theoretic perspective or from a generative model perspective. Comparative experiments clearly show the surprising advantage of corrupting the input of autoencoders on a pattern classification benchmark suite.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {1096–1103},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}
@InProceedings{He+2022,
    author    = {He, Kaiming and Chen, Xinlei and Xie, Saining and Li, Yanghao and Doll\'ar, Piotr and Girshick, Ross},
    title     = {Masked Autoencoders Are Scalable Vision Learners},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2022},
    pages     = {16000-16009}
}
@inproceedings{Gershman-Goodman2014,
    author          = {Samuel Gershman and Noah Goodman},
    year            = {2014},
    title           = {{Amortized Inference in Probabilistic Reasoning}},
    booktitle       = {Proceedings of the Annual Meating of the Cognitive Science Society},
    volume          = {36},
    pages           = {},
    url             = {https://escholarship.org/uc/item/34j1h7k5}
}
@inproceedings{Bowman+2016,
    title = "Generating Sentences from a Continuous Space",
    author = "Bowman, Samuel R.  and
      Vilnis, Luke  and
      Vinyals, Oriol  and
      Dai, Andrew  and
      Jozefowicz, Rafal  and
      Bengio, Samy",
    editor = "Riezler, Stefan  and
      Goldberg, Yoav",
    booktitle = "Proceedings of the 20th {SIGNLL} Conference on Computational Natural Language Learning",
    month = aug,
    year = "2016",
    address = "Berlin, Germany",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/K16-1002",
    doi = "10.18653/v1/K16-1002",
    pages = "10--21",
}
@inproceedings{Balle+2017,
title={End-to-end Optimized Image Compression},
author={Johannes Ball{\'e} and Valero Laparra and Eero P. Simoncelli},
booktitle={International Conference on Learning Representations},
year={2017},
url={https://openreview.net/forum?id=rJxdQ3jeg}
}
@inproceedings{Yu+2022VIM,
title={Vector-quantized Image Modeling with Improved {VQGAN}},
author={Jiahui Yu and Xin Li and Jing Yu Koh and Han Zhang and Ruoming Pang and James Qin and Alexander Ku and Yuanzhong Xu and Jason Baldridge and Yonghui Wu},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=pfNyExj7z2}
}

@inproceedings{Locatello+2019,title	= {Challenging Common Assumptions in the Unsupervised Learning of Disentangled Representations},author	= {Francesco Locatello and Stefan Bauer and Mario Lučić and Gunnar Rätsch and Sylvain Gelly and Bernhard Schölkopf and Olivier Frederic Bachem},year	= {2019},URL	= {http://proceedings.mlr.press/v97/locatello19a.html},note	= {Best Paper Award},booktitle	= {International Conference on Machine Learning}}

@misc{Oord+2019,
      title={Representation Learning with Contrastive Predictive Coding}, 
      author={Aaron van den Oord and Yazhe Li and Oriol Vinyals},
      year={2019},
      eprint={1807.03748},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1807.03748}, 
}


@InProceedings{Hyvarinen+2019,
  title = 	 {Nonlinear ICA Using Auxiliary Variables and Generalized Contrastive Learning},
  author =       {Hyvarinen, Aapo and Sasaki, Hiroaki and Turner, Richard},
  booktitle = 	 {Proceedings of the Twenty-Second International Conference on Artificial Intelligence and Statistics},
  pages = 	 {859--868},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Sugiyama, Masashi},
  volume = 	 {89},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {16--18 Apr},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v89/hyvarinen19a/hyvarinen19a.pdf},
  url = 	 {https://proceedings.mlr.press/v89/hyvarinen19a.html},
  abstract = 	 {Nonlinear ICA is a fundamental problem for unsupervised representation learning, emphasizing the capacity to recover the underlying latent variables generating the data (i.e., identifiability). Recently, the very first identifiability proofs for nonlinear ICA have been proposed, leveraging the temporal structure of the independent components. Here, we propose a general framework for nonlinear ICA, which, as a special case, can make use of temporal structure. It is based on augmenting the data by an auxiliary variable, such as the time index, the history of the time series, or any other available information. We propose to learn  nonlinear ICA by discriminating between true augmented data, or data in which the auxiliary variable has been randomized.  This enables  the framework to be implemented algorithmically through logistic regression, possibly in a neural network. We provide a comprehensive proof of the identifiability of the model as well as the consistency of our estimation method. The approach not only provides a general theoretical framework combining and generalizing  previously proposed nonlinear ICA models and algorithms, but also brings practical advantages.}
}
@inproceedings{Hyvarinen-Morioka2016,
author = {Hyv\"{a}rinen, Aapo and Morioka, Hiroshi},
title = {Unsupervised feature extraction by time-contrastive learning and nonlinear ICA},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Nonlinear independent component analysis (ICA) provides an appealing framework for unsupervised feature learning, but the models proposed so far are not identifiable. Here, we first propose a new intuitive principle of unsupervised deep learning from time series which uses the nonstationary structure of the data. Our learning principle, time-contrastive learning (TCL), finds a representation which allows optimal discrimination of time segments (windows). Surprisingly, we show how TCL can be related to a nonlinear ICA model, when ICA is redefined to include temporal nonstationarities. In particular, we show that TCL combined with linear ICA estimates the nonlinear ICA model up to point-wise transformations of the sources, and this solution is unique — thus providing the first identifiability result for nonlinear ICA which is rigorous, constructive, as well as very general.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {3772–3780},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@InProceedings{Khemakhem+2020,
  title = 	 {Variational Autoencoders and Nonlinear ICA: A Unifying Framework},
  author =       {Khemakhem, Ilyes and Kingma, Diederik and Monti, Ricardo and Hyvarinen, Aapo},
  booktitle = 	 {Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics},
  pages = 	 {2207--2217},
  year = 	 {2020},
  editor = 	 {Chiappa, Silvia and Calandra, Roberto},
  volume = 	 {108},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {26--28 Aug},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v108/khemakhem20a/khemakhem20a.pdf},
  url = 	 {https://proceedings.mlr.press/v108/khemakhem20a.html},
  abstract = 	 {The framework of variational autoencoders allows us to efficiently learn deep latent-variable models, such that the model’s marginal distribution over observed variables fits the data. Often, we’re interested in going a step further, and want to approximate the true joint distribution over observed and latent variables, including the true prior and posterior distributions over latent variables. This is known to be generally impossible due to unidentifiability of the model. We address this issue by showing that for a broad family of deep latent-variable models, identification of the true joint distribution over observed and latent variables is actually possible up to  very simple transformations, thus achieving a principled and powerful form of disentanglement. Our result requires a factorized prior distribution over the latent variables that is conditioned on an additionally observed variable, such as a class label or almost any other observation. We build on recent developments in nonlinear ICA, which we extend to the case with noisy, undercomplete or discrete observations, integrated in a maximum likelihood framework. The result also trivially contains identifiable flow-based generative models as a special case. }
}

@InProceedings{Hyvarinen-Morioka2017,
  title = 	 {{Nonlinear ICA of Temporally Dependent Stationary Sources}},
  author = 	 {Hyvarinen, Aapo and Morioka, Hiroshi},
  booktitle = 	 {Proceedings of the 20th International Conference on Artificial Intelligence and Statistics},
  pages = 	 {460--469},
  year = 	 {2017},
  editor = 	 {Singh, Aarti and Zhu, Jerry},
  volume = 	 {54},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {20--22 Apr},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v54/hyvarinen17a/hyvarinen17a.pdf},
  url = 	 {https://proceedings.mlr.press/v54/hyvarinen17a.html},
  abstract = 	 {We develop a nonlinear generalization of independent component analysis (ICA) or blind source separation, based on temporal dependencies (e.g. autocorrelations). We introduce a nonlinear generative model where the independent sources are assumed to be temporally dependent, non-Gaussian, and stationary, and we observe arbitrarily nonlinear mixtures of them. We develop a method for estimating the model (i.e. separating the sources) based on logistic regression in a neural network which learns to discriminate between a short temporal window of the data vs. a temporal window of temporally permuted data. We prove that the method estimates the sources for general smooth mixing nonlinearities, assuming the sources have sufficiently strong temporal dependencies, and these dependencies are in a certain way different from dependencies found in Gaussian processes. For Gaussian (and similar) sources, the method estimates the nonlinear part of the mixing. We thus provide the first rigorous and general proof of identifiability of nonlinear ICA for temporally dependent sources, together with a practical method for its estimation.}
}
@ARTICLE{Elias1955,
  author={Elias, P.},
  journal={IRE Transactions on Information Theory}, 
  title={Predictive coding--I}, 
  year={1955},
  volume={1},
  number={1},
  pages={16-24},
  keywords={Transmitters;Predictive coding;Pulse modulation;Entropy;Feeds;Decoding;Error correction codes;Information filtering;Information filters;Bandwidth},
  doi={10.1109/TIT.1955.1055126}}

@article{Rao-Ballard1999,
	abstract = {We describe a model of visual processing in which feedback connections from a higher- to a lower-order visual cortical area carry predictions of lower-level neural activities, whereas the feedforward connections carry the residual errors between the predictions and the actual lower-level activities. When exposed to natural images, a hierarchical network of model neurons implementing such a model developed simple-cell-like receptive fields. A subset of neurons responsible for carrying the residual errors showed endstopping and other extra-classical receptive-field effects. These results suggest that rather than being exclusively feedforward phenomena, nonclassical surround effects in the visual cortex may also result from cortico-cortical feedback as a consequence of the visual system using an efficient hierarchical strategy for encoding natural images.},
	author = {Rao, Rajesh P. N. and Ballard, Dana H.},
	date = {1999/01/01},
	date-added = {2024-07-29 19:49:49 +0900},
	date-modified = {2024-07-29 19:49:49 +0900},
	doi = {10.1038/4580},
	id = {Rao1999},
	isbn = {1546-1726},
	journal = {Nature Neuroscience},
	number = {1},
	pages = {79--87},
	title = {Predictive coding in the visual cortex:  a functional interpretation of some extra-classical receptive-field effects},
	url = {https://doi.org/10.1038/4580},
	volume = {2},
	year = {1999},
	bdsk-url-1 = {https://doi.org/10.1038/4580}}
@inproceedings{Larochelle+2007,
author = {Larochelle, Hugo and Erhan, Dumitru and Courville, Aaron and Bergstra, James and Bengio, Yoshua},
title = {An empirical evaluation of deep architectures on problems with many factors of variation},
year = {2007},
isbn = {9781595937933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273496.1273556},
doi = {10.1145/1273496.1273556},
abstract = {Recently, several learning algorithms relying on models with deep architectures have been proposed. Though they have demonstrated impressive performance, to date, they have only been evaluated on relatively simple problems such as digit recognition in a controlled environment, for which many machine learning algorithms already report reasonable results. Here, we present a series of experiments which indicate that these models show promise in solving harder learning problems that exhibit many factors of variation. These models are compared with well-established algorithms such as Support Vector Machines and single hidden-layer feed-forward neural networks.},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {473–480},
numpages = {8},
location = {Corvalis, Oregon, USA},
series = {ICML '07}
}

@article{Luxburg2007,
	abstract = {In recent years, spectral clustering has become one of the most popular modern clustering algorithms. It is simple to implement, can be solved efficiently by standard linear algebra software, and very often outperforms traditional clustering algorithms such as the k-means algorithm. On the first glance spectral clustering appears slightly mysterious, and it is not obvious to see why it works at all and what it really does. The goal of this tutorial is to give some intuition on those questions. We describe different graph Laplacians and their basic properties, present the most common spectral clustering algorithms, and derive those algorithms from scratch by several different approaches. Advantages and disadvantages of the different spectral clustering algorithms are discussed.},
	author = {von Luxburg, Ulrike},
	date = {2007/12/01},
	date-added = {2024-07-30 10:48:12 +0900},
	date-modified = {2024-07-30 10:48:12 +0900},
	doi = {10.1007/s11222-007-9033-z},
	id = {von Luxburg2007},
	isbn = {1573-1375},
	journal = {Statistics and Computing},
	number = {4},
	pages = {395--416},
	title = {A tutorial on spectral clustering},
	url = {https://doi.org/10.1007/s11222-007-9033-z},
	volume = {17},
	year = {2007},
	bdsk-url-1 = {https://doi.org/10.1007/s11222-007-9033-z}}

@article{Fefferman+2016,
    author          = {Charles Fefferman and Sanjoy Mitter and Hariharan Narayanan},
    year            = {2016},
    title           = {Testing the Manifold Hypothesis},
    journal         = {Journal of the American Mathematical Society},
    volume          = {29},
    number          = {},
    pages           = {983-1049},
    url             = {https://doi.org/10.1090/jams/852}
}

@article{Agrawal+2021,
	author = {Akshay Agrawal and Alnur Ali and Stephen Boyd},
	doi = {10.1561/2200000090},
	issn = {1935-8237},
	journal = {Foundations and Trends{\textregistered} in Machine Learning},
	number = {3},
	pages = {211-378},
	title = {Minimum-Distortion Embedding},
	url = {http://dx.doi.org/10.1561/2200000090},
	volume = {14},
	year = {2021},
	bdsk-url-1 = {http://dx.doi.org/10.1561/2200000090}}

@article{Wilk+2020,
	abstract = {There is an urgent need to better understand the pathophysiology of Coronavirus disease 2019 (COVID-19), the global pandemic caused by SARS-CoV-2, which has infected more than three million people worldwide1. Approximately 20{\%} of patients with COVID-19 develop severe disease and 5{\%} of patients require intensive care2. Severe disease has been associated with changes in peripheral immune activity, including increased levels of pro-inflammatory cytokines3,4 that may be produced by a subset of inflammatory monocytes5,6, lymphopenia7,8 and T cell exhaustion9,10. To elucidate pathways in peripheral immune cells that might lead to immunopathology or protective immunity in severe COVID-19, we applied single-cell RNA sequencing (scRNA-seq) to profile peripheral blood mononuclear cells (PBMCs) from seven patients hospitalized for COVID-19, four of whom had acute respiratory distress syndrome, and six healthy controls. We identify reconfiguration of peripheral immune cell phenotype in COVID-19, including a heterogeneous interferon-stimulated gene signature, HLA class II downregulation and a developing neutrophil population that appears closely related to plasmablasts appearing in patients with acute respiratory failure requiring mechanical ventilation. Importantly, we found that peripheral monocytes and lymphocytes do not express substantial amounts of pro-inflammatory cytokines. Collectively, we provide a cell atlas of the peripheral immune response to severe COVID-19.},
	author = {Wilk, Aaron J. and Rustagi, Arjun and Zhao, Nancy Q. and Roque, Jonasel and Mart{\'\i}nez-Col{\'o}n, Giovanny J. and McKechnie, Julia L. and Ivison, Geoffrey T. and Ranganath, Thanmayi and Vergara, Rosemary and Hollis, Taylor and Simpson, Laura J. and Grant, Philip and Subramanian, Aruna and Rogers, Angela J. and Blish, Catherine A.},
	date = {2020/07/01},
	date-added = {2024-07-30 14:22:55 +0900},
	date-modified = {2024-07-30 14:22:55 +0900},
	doi = {10.1038/s41591-020-0944-y},
	id = {Wilk2020},
	isbn = {1546-170X},
	journal = {Nature Medicine},
	number = {7},
	pages = {1070--1076},
	title = {A single-cell atlas of the peripheral immune response in patients with severe COVID-19},
	url = {https://doi.org/10.1038/s41591-020-0944-y},
	volume = {26},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1038/s41591-020-0944-y}}

@article{Burges2010,
	author = {Christopher J. C. Burges},
	doi = {10.1561/2200000002},
	issn = {1935-8237},
	journal = {Foundations and Trends{\textregistered} in Machine Learning},
	number = {4},
	pages = {275-365},
	title = {Dimension Reduction: A Guided Tour},
	url = {http://dx.doi.org/10.1561/2200000002},
	volume = {2},
	year = {2010},
	bdsk-url-1 = {http://dx.doi.org/10.1561/2200000002}}

@article{Karhunen-Joutsensalo1995,
	abstract = {We derive and discuss various generalizations of neural PCA (Principal Component Analysis)-type learning algorithms containing nonlinearities using optimization-based approach. Standard PCA arises as an optimal solution to several different information representation problems. We justify that this is essentially due to the fact that the solution is based on the second-order statistics only. If the respective optimization problems are generalized for nonquadratic criteria so that higher-order statistics are taken into account, their solutions will in general be different. The solutions define in a natural way several meaningful extensions of PCA and give a solid foundation for them. In this framework, we study more closely generalizations of the problems of variance maximization and mean-square error minimization. For these problems, we derive gradient-type neural learning algorithms both for symmetric and hierarchic PCA-type networks. As an important special case, the well-known Sanger's generalized Hebbian algorithm (GHA) is shown to emerge from natural optimization problems.},
	author = {Juha Karhunen and Jyrki Joutsensalo},
	doi = {https://doi.org/10.1016/0893-6080(94)00098-7},
	issn = {0893-6080},
	journal = {Neural Networks},
	keywords = {Principal components, Optimization, Neural network, Unsupervised learning, Nonlinearity, Robust statistics, Generalized Hebbian algorithm, Oja's rule},
	number = {4},
	pages = {549-562},
	title = {Generalizations of principal component analysis, optimization problems, and neural networks},
	url = {https://www.sciencedirect.com/science/article/pii/0893608094000987},
	volume = {8},
	year = {1995},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/0893608094000987},
	bdsk-url-2 = {https://doi.org/10.1016/0893-6080(94)00098-7}}
@ARTICLE{Japkowicz+2000,
  author={Japkowicz, Nathalie and Hanson, Stephen José and Gluck, Mark A.},
  journal={Neural Computation}, 
  title={Nonlinear Autoassociation Is Not Equivalent to PCA}, 
  year={2000},
  volume={12},
  number={3},
  pages={531-545},
  keywords={},
  doi={10.1162/089976600300015691}}
@article{Vincent+2010,
  author  = {Pascal Vincent and Hugo Larochelle and Isabelle Lajoie and Yoshua Bengio and Pierre-Antoine Manzagol},
  title   = {Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion},
  journal = {Journal of Machine Learning Research},
  year    = {2010},
  volume  = {11},
  number  = {110},
  pages   = {3371--3408},
  url     = {http://jmlr.org/papers/v11/vincent10a.html}
}

@InProceedings{Glorot+2011,
  title = 	 {Deep Sparse Rectifier Neural Networks},
  author = 	 {Glorot, Xavier and Bordes, Antoine and Bengio, Yoshua},
  booktitle = 	 {Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics},
  pages = 	 {315--323},
  year = 	 {2011},
  editor = 	 {Gordon, Geoffrey and Dunson, David and Dudík, Miroslav},
  volume = 	 {15},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Fort Lauderdale, FL, USA},
  month = 	 {11--13 Apr},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v15/glorot11a/glorot11a.pdf},
  url = 	 {https://proceedings.mlr.press/v15/glorot11a.html},
  abstract = 	 {While logistic sigmoid neurons are more biologically plausible than hyperbolic tangent neurons, the latter work better for training multi-layer neural networks. This paper shows that rectifying neurons are an even better model of biological neurons and yield equal or better performance than hyperbolic tangent networks in spite of the hard non-linearity and non-differentiability at zero, creating sparse representations with true zeros which seem remarkably suitable for naturally sparse data. Even though they can take advantage of semi-supervised setups with extra-unlabeled data, deep rectifier networks can reach their best performance without requiring any unsupervised pre-training on purely supervised tasks with large labeled datasets. Hence, these results can be seen as a new milestone in the attempts at understanding the difficulty in training deep but purely supervised neural networks, and closing the performance gap between neural networks learnt with and without unsupervised pre-training.}
}

@article{Beyeler+2019,
	abstract = {Supported by recent computational studies, there is increasing evidence that a wide range of neuronal responses can be understood as an emergent property of nonnegative sparse coding (NSC), an efficient population coding scheme based on dimensionality reduction and sparsity constraints. We review evidence that NSC might be employed by sensory areas to efficiently encode external stimulus spaces, by some associative areas to conjunctively represent multiple behaviorally relevant variables, and possibly by the basal ganglia to coordinate movement. In addition, NSC might provide a useful theoretical framework under which to understand the often complex and nonintuitive response properties of neurons in other brain areas. Although NSC might not apply to all brain areas (for example, motor or executive function areas) the success of NSC-based models, especially in sensory areas, warrants further investigation for neural correlates in other regions.},
	author = {Beyeler, Michael AND Rounds, Emily L. AND Carlson, Kristofor D. AND Dutt, Nikil AND Krichmar, Jeffrey L.},
	doi = {10.1371/journal.pcbi.1006908},
	journal = {PLOS Computational Biology},
	month = {06},
	number = {6},
	pages = {1-33},
	publisher = {Public Library of Science},
	title = {Neural correlates of sparse coding and dimensionality reduction},
	url = {https://doi.org/10.1371/journal.pcbi.1006908},
	volume = {15},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1371/journal.pcbi.1006908}}
@inproceedings{Rifai+2011,
author = {Rifai, Salah and Vincent, Pascal and Muller, Xavier and Glorot, Xavier and Bengio, Yoshua},
title = {Contractive auto-encoders: explicit invariance during feature extraction},
year = {2011},
isbn = {9781450306195},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {We present in this paper a novel approach for training deterministic auto-encoders. We show that by adding a well chosen penalty term to the classical reconstruction cost function, we can achieve results that equal or surpass those attained by other regularized auto-encoders as well as denoising auto-encoders on a range of datasets. This penalty term corresponds to the Frobenius norm of the Jacobian matrix of the encoder activations with respect to the input. We show that this penalty term results in a localized space contraction which in turn yields robust features on the activation layer. Furthermore, we show how this penalty term is related to both regularized auto-encoders and denoising auto-encoders and how it can be seen as a link between deterministic and non-deterministic auto-encoders. We find empirically that this penalty helps to carve a representation that better captures the local directions of variation dictated by the data, corresponding to a lower-dimensional non-linear manifold, while being more invariant to the vast majority of directions orthogonal to the manifold. Finally, we show that by using the learned features to initialize a MLP, we achieve state of the art classification error on a range of datasets, surpassing other methods of pretraining.},
booktitle = {Proceedings of the 28th International Conference on International Conference on Machine Learning},
pages = {833–840},
numpages = {8},
location = {Bellevue, Washington, USA},
series = {ICML'11}
}

@inproceedings{Papamakarios+2017,
	author = {Papamakarios, George and Pavlakou, Theo and Murray, Iain},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
	publisher = {Curran Associates, Inc.},
	title = {Masked Autoregressive Flow for Density Estimation},
	url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/6c1da886822c67822bcf3679d04369fa-Paper.pdf},
	volume = {30},
	year = {2017},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2017/file/6c1da886822c67822bcf3679d04369fa-Paper.pdf}}

@inproceedings{Kingma+2016,
	author = {D. Kingma and Salimans, Tim and Jozefowicz, Rafal and Chen, Xi and Sutskever, Ilya and Welling, Max},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {D. Lee and M. Sugiyama and U. Luxburg and I. Guyon and R. Garnett},
	publisher = {Curran Associates, Inc.},
	title = {Improved Variational Inference with Inverse Autoregressive Flow},
	url = {https://proceedings.neurips.cc/paper_files/paper/2016/file/ddeebdeefdb7e7e7a697e1c3e3d8ef54-Paper.pdf},
	volume = {29},
	year = {2016},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2016/file/ddeebdeefdb7e7e7a697e1c3e3d8ef54-Paper.pdf}}

@InProceedings{Oord+2018,
  title = 	 {Parallel {W}ave{N}et: Fast High-Fidelity Speech Synthesis},
  author =       {van den Oord, Aaron and Li, Yazhe and Babuschkin, Igor and Simonyan, Karen and Vinyals, Oriol and Kavukcuoglu, Koray and van den Driessche, George and Lockhart, Edward and Cobo, Luis and Stimberg, Florian and Casagrande, Norman and Grewe, Dominik and Noury, Seb and Dieleman, Sander and Elsen, Erich and Kalchbrenner, Nal and Zen, Heiga and Graves, Alex and King, Helen and Walters, Tom and Belov, Dan and Hassabis, Demis},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {3918--3926},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {10--15 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v80/oord18a/oord18a.pdf},
  url = 	 {https://proceedings.mlr.press/v80/oord18a.html},
  abstract = 	 {The recently-developed WaveNet architecture is the current state of the art in realistic speech synthesis, consistently rated as more natural sounding for many different languages than any previous system. However, because WaveNet relies on sequential generation of one audio sample at a time, it is poorly suited to today’s massively parallel computers, and therefore hard to deploy in a real-time production setting. This paper introduces Probability Density Distillation, a new method for training a parallel feed-forward network from a trained WaveNet with no significant difference in quality. The resulting system is capable of generating high-fidelity speech samples at more than 20 times faster than real-time, a 1000x speed up relative to the original WaveNet, and capable of serving multiple English and Japanese voices in a production setting.}
}

@InProceedings{Behrmann+2019,
  title = 	 {Invertible Residual Networks},
  author =       {Behrmann, Jens and Grathwohl, Will and Chen, Ricky T. Q. and Duvenaud, David and Jacobsen, Joern-Henrik},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {573--582},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/behrmann19a/behrmann19a.pdf},
  url = 	 {https://proceedings.mlr.press/v97/behrmann19a.html},
  abstract = 	 {We show that standard ResNet architectures can be made invertible, allowing the same model to be used for classification, density estimation, and generation. Typically, enforcing invertibility requires partitioning dimensions or restricting network architectures. In contrast, our approach only requires adding a simple normalization step during training, already available in standard frameworks. Invertible ResNets define a generative model which can be trained by maximum likelihood on unlabeled data. To compute likelihoods, we introduce a tractable approximation to the Jacobian log-determinant of a residual block. Our empirical evaluation shows that invertible ResNets perform competitively with both state-of-the-art image classifiers and flow-based generative models, something that has not been previously achieved with a single architecture.}
}

@inproceedings{Chen+2019,
	author = {Chen, Ricky T. Q. and Behrmann, Jens and Duvenaud, David K and Jacobsen, Joern-Henrik},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
	publisher = {Curran Associates, Inc.},
	title = {Residual Flows for Invertible Generative Modeling},
	url = {https://proceedings.neurips.cc/paper_files/paper/2019/file/5d0d5594d24f0f955548f0fc0ff83d10-Paper.pdf},
	volume = {32},
	year = {2019},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2019/file/5d0d5594d24f0f955548f0fc0ff83d10-Paper.pdf}}

@article{Hutchinson1990,
	annote = {doi: 10.1080/03610919008812866},
	author = {Hutchinson, M. F. },
	date = {1990/01/01},
	date-added = {2024-07-30 16:36:50 +0900},
	date-modified = {2024-07-30 16:36:50 +0900},
	doi = {10.1080/03610919008812866},
	isbn = {0361-0918},
	journal = {Communications in Statistics - Simulation and Computation},
	journal1 = {Communications in Statistics - Simulation and Computation},
	journal2 = {Communications in Statistics - Simulation and Computation},
	month = {01},
	number = {2},
	pages = {433--450},
	publisher = {Taylor \& Francis},
	title = {{A Stochastic Estimator of the Trace of the Influence Matrix for Laplacian Smoothing Splines}},
	type = {doi: 10.1080/03610919008812866},
	url = {https://doi.org/10.1080/03610919008812866},
	volume = {19},
	year = {1990},
	year1 = {1990},
	bdsk-url-1 = {https://doi.org/10.1080/03610919008812866}}

@inbook{Skilling1989,
	abstract = {Often, we need to know some integral property of the eigenvalues {\{}x{\}} of a large N {\texttimes} N symmetric matrix A. For example, determinants det (A) = exp(∑ log (x)) play a role in the classic maximum entropy algorithm [Gull, 1988] . Likewise in physics, the specific heat of a system is a temperature- -dependent sum over the eigenvalues of the Hamiltonian matrix. However, the matrix may be so large that direct O (N3 calculation of all N eigenvalues is prohibited. Indeed, if A is coded as a ``fast'' procedure, then O (N2 operations may also be prohibited.},
	address = {Dordrecht},
	author = {Skilling, John},
	booktitle = {Maximum Entropy and Bayesian Methods: Cambridge, England, 1988},
	doi = {10.1007/978-94-015-7860-8_48},
	editor = {Skilling, J.},
	isbn = {978-94-015-7860-8},
	pages = {455--466},
	publisher = {Springer Netherlands},
	title = {The Eigenvalues of Mega-dimensional Matrices},
	url = {https://doi.org/10.1007/978-94-015-7860-8_48},
	year = {1989},
	bdsk-url-1 = {https://doi.org/10.1007/978-94-015-7860-8_48}}
@inproceedings{vandenBerg+2019,
    title={Sylvester Normalizing Flows for Variational Inference}, 
      author={Rianne van den Berg and Leonard Hasenclever and Jakub M. Tomczak and Max Welling},
      year={2019},
    booktitle       = {Conference on Uncertainty in Artificial Intelligence},
    volume          = {34},
    pages           = {393-402},
    url             = {http://auai.org/uai2018/accepted.php}
}

@article{Tabak-Vanden-Eijnden2010,
	author = {Esteban G. Tabak and Eric Vanden-Eijnden},
	date = {2010/3/1},
	date-added = {2024-07-30 18:00:10 +0900},
	date-modified = {2024-07-30 18:00:10 +0900},
	journal = {Communications in Mathematical Sciences},
	journal1 = {Communications in Mathematical Sciences},
	journal2 = {Communications in Mathematical Sciences},
	month = {3},
	number = {1},
	pages = {217--233 },
	title = {Density estimation by dual ascent of the log-likelihood},
	volume = {8},
	year = {2010},
    url             = {https://doi.org/10.4310/cms.2010.v8.n1.a11},
}

@article{Tabak-Turner2013,
	abstract = {Abstract A new methodology for density estimation is proposed. The methodology, which builds on the one developed by Tabak and Vanden-Eijnden, normalizes the data points through the composition of simple maps. The parameters of each map are determined through the maximization of a local quadratic approximation to the log-likelihood. Various candidates for the elementary maps of each step are proposed; criteria for choosing one includes robustness, computational simplicity, and good behavior in high-dimensional settings. A good choice is that of localized radial expansions, which depend on a single parameter: all the complexity of arbitrary, possibly convoluted probability densities can be built through the composition of such simple maps. {\copyright} 2012 Wiley Periodicals, Inc.},
	author = {Tabak, E. G. and Turner, Cristina V.},
	doi = {https://doi.org/10.1002/cpa.21423},
	eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/cpa.21423},
	journal = {Communications on Pure and Applied Mathematics},
	number = {2},
	pages = {145-164},
	title = {A Family of Nonparametric Density Estimation Algorithms},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/cpa.21423},
	volume = {66},
	year = {2013},
	bdsk-url-1 = {https://onlinelibrary.wiley.com/doi/abs/10.1002/cpa.21423},
	bdsk-url-2 = {https://doi.org/10.1002/cpa.21423}}

@inproceedings{Chen-Gopinath2000,
	author = {Chen, Scott and Gopinath, Ramesh},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {T. Leen and T. Dietterich and V. Tresp},
	publisher = {MIT Press},
	title = {Gaussianization},
	url = {https://proceedings.neurips.cc/paper_files/paper/2000/file/3c947bc2f7ff007b86a9428b74654de5-Paper.pdf},
	volume = {13},
	year = {2000},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2000/file/3c947bc2f7ff007b86a9428b74654de5-Paper.pdf}}
@misc{Rippel-Adams2013,
      title={High-Dimensional Probability Estimation with Deep Density Models}, 
      author={Oren Rippel and Ryan Prescott Adams},
      year={2013},
      eprint={1302.5125},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/1302.5125}, 
}
@inproceedings{Dinh+2015,
      title={NICE: Non-linear Independent Components Estimation}, 
      author={Laurent Dinh and David Krueger and Yoshua Bengio},
      year={2015},
    booktitle       = {International Conference on Learning Representations. Poster},
    volume          = {},
    pages           = {},
    url={https://arxiv.org/abs/1410.8516},
}
@inproceedings{Dinh+2017,
title={Density estimation using Real {NVP}},
author={Laurent Dinh and Jascha Sohl-Dickstein and Samy Bengio},
booktitle={International Conference on Learning Representations},
year={2017},
url={https://openreview.net/forum?id=HkpbnH9lx}
}
@inproceedings{Nalisnick+2019,
      title={Hybrid Models with Deep and Invertible Features}, 
      author={Eric Nalisnick and Akihiro Matsukawa and Yee Whye Teh and Dilan Gorur and Balaji Lakshminarayanan},
      year={2019},
        booktitle = {International Conference on Machine Learning},

      url={https://arxiv.org/abs/1902.02767}, 
}

@inproceedings{Zhang+2020Detection,
	abstract = {Open set recognition requires a classifier to detect samples not belonging to any of the classes in its training set. Existing methods fit a probability distribution to the training samples on their embedding space and detect outliers according to this distribution. The embedding space is often obtained from a discriminative classifier. However, such discriminative representation focuses only on known classes, which may not be critical for distinguishing the unknown classes. We argue that the representation space should be jointly learned from the inlier classifier and the density estimator (served as an outlier detector). We propose the OpenHybrid framework, which is composed of an encoder to encode the input data into a joint embedding space, a classifier to classify samples to inlier classes, and a flow-based density estimator to detect whether a sample belongs to the unknown category. A typical problem of existing flow-based models is that they may assign a higher likelihood to outliers. However, we empirically observe that such an issue does not occur in our experiments when learning a joint representation for discriminative and generative components. Experiments on standard open set benchmarks also reveal that an end-to-end trained OpenHybrid model significantly outperforms state-of-the-art methods and flow-based baselines.},
	address = {Cham},
	author = {Zhang, Hongjie and Li, Ang and Guo, Jie and Guo, Yanwen},
	booktitle = {Computer Vision -- ECCV 2020},
	editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
	isbn = {978-3-030-58580-8},
	pages = {102--117},
	publisher = {Springer International Publishing},
	title = {Hybrid Models for Open Set Recognition},
	year = {2020}}

@inproceedings{Charpentier+2020,
	author = {Charpentier, Bertrand and Z\"{u}gner, Daniel and G\"{u}nnemann, Stephan},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
	pages = {1356--1367},
	publisher = {Curran Associates, Inc.},
	title = {Posterior Network: Uncertainty Estimation without OOD Samples via Density-Based Pseudo-Counts},
	url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/0eac690d7059a8de4b48e90f14510391-Paper.pdf},
	volume = {33},
	year = {2020},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2020/file/0eac690d7059a8de4b48e90f14510391-Paper.pdf}}
@misc{Prenger+2018,
      title={WaveGlow: A Flow-based Generative Network for Speech Synthesis}, 
      author={Ryan Prenger and Rafael Valle and Bryan Catanzaro},
      year={2018},
      eprint={1811.00002},
      archivePrefix={arXiv},
      primaryClass={cs.SD},
      url={https://arxiv.org/abs/1811.00002}, 
}

@InProceedings{Kim+2019,
  title = 	 {{F}lo{W}ave{N}et : A Generative Flow for Raw Audio},
  author =       {Kim, Sungwon and Lee, Sang-Gil and Song, Jongyoon and Kim, Jaehyeon and Yoon, Sungroh},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {3370--3378},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/kim19b/kim19b.pdf},
  url = 	 {https://proceedings.mlr.press/v97/kim19b.html},
  abstract = 	 {Most modern text-to-speech architectures use a WaveNet vocoder for synthesizing high-fidelity waveform audio, but there have been limitations, such as high inference time, in practical applications due to its ancestral sampling scheme. The recently suggested Parallel WaveNet and ClariNet has achieved real-time audio synthesis capability by incorporating inverse autoregressive flow (IAF) for parallel sampling. However, these approaches require a two-stage training pipeline with a well-trained teacher network and can only produce natural sound by using probability distillation along with heavily-engineered auxiliary loss terms. We propose FloWaveNet, a flow-based generative model for raw audio synthesis. FloWaveNet requires only a single-stage training procedure and a single maximum likelihood loss, without any additional auxiliary terms, and it is inherently parallel due to the characteristics of generative flow. The model can efficiently sample raw audio in real-time, with clarity comparable to previous two-stage parallel models. The code and samples for all models, including our FloWaveNet, are available on GitHub.}
}

@inproceedings{Kingma-Dhariwal2018,
	author = {Kingma, Durk P and Dhariwal, Prafulla},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
	publisher = {Curran Associates, Inc.},
	title = {Glow: Generative Flow with Invertible 1x1 Convolutions},
	url = {https://proceedings.neurips.cc/paper_files/paper/2018/file/d139db6a236200b21cc7f752979132d0-Paper.pdf},
	volume = {31},
	year = {2018},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2018/file/d139db6a236200b21cc7f752979132d0-Paper.pdf}}
@inproceedings{Kumar+2020,
title={VideoFlow: A Conditional Flow-Based Model for Stochastic Video Generation},
author={Manoj Kumar and Mohammad Babaeizadeh and Dumitru Erhan and Chelsea Finn and Sergey Levine and Laurent Dinh and Durk Kingma},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=rJgUfTEYvH}
}

@inproceedings{Tran+2019,
	author = {Tran, Dustin and Vafa, Keyon and Agrawal, Kumar and Dinh, Laurent and Poole, Ben},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
	publisher = {Curran Associates, Inc.},
	title = {Discrete Flows: Invertible Generative Models of Discrete Data},
	url = {https://proceedings.neurips.cc/paper_files/paper/2019/file/e046ede63264b10130007afca077877f-Paper.pdf},
	volume = {32},
	year = {2019},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2019/file/e046ede63264b10130007afca077877f-Paper.pdf}}

@InProceedings{Ziegler-Rush2019,
  title = 	 {Latent Normalizing Flows for Discrete Sequences},
  author =       {Ziegler, Zachary and Rush, Alexander},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {7673--7682},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/ziegler19a/ziegler19a.pdf},
  url = 	 {https://proceedings.mlr.press/v97/ziegler19a.html},
  abstract = 	 {Normalizing flows are a powerful class of generative models for continuous random variables, showing both strong model flexibility and the potential for non-autoregressive generation. These benefits are also desired when modeling discrete random variables such as text, but directly applying normalizing flows to discrete sequences poses significant additional challenges. We propose a VAE-based generative model which jointly learns a normalizing flow-based distribution in the latent space and a stochastic mapping to an observed discrete space. In this setting, we find that it is crucial for the flow-based distribution to be highly multimodal. To capture this property, we propose several normalizing flow architectures to maximize model flexibility. Experiments consider common discrete sequence tasks of character-level language modeling and polyphonic music generation. Our results indicate that an autoregressive flow-based model can match the performance of a comparable autoregressive baseline, and a non-autoregressive flow-based model can improve generation speed with a penalty to performance.}
}
@misc{Tomczak-Welling2017,
      title={Improving Variational Auto-Encoders using Householder Flow}, 
      author={Jakub M. Tomczak and Max Welling},
      year={2017},
      eprint={1611.09630},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1611.09630}, 
}
@inproceedings{Louizos-Welling2017,
author = {Louizos, Christos and Welling, Max},
title = {Multiplicative normalizing flows for variational Bayesian neural networks},
year = {2017},
publisher = {JMLR.org},
abstract = {We reinterpret multiplicative noise in neural networks as auxiliary random variables that augment the approximate posterior in a variational setting for Bayesian neural networks. We show that through this interpretation it is both efficient and straightforward to improve the approximation by employing normalizing flows (Rezende \& Mohamed, 2015) while still allowing for local reparametrizations (Kingma et al., 2015) and a tractable lower bound (Ranganath et al., 2015; Maal0e et al., 2016). In experiments we show that with this new approximation we can significantly improve upon classical mean field for Bayesian neural networks on both predictive accuracy as well as predictive uncertainty.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {2218–2227},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}
@article{Muller+2019,
author = {M\"{u}ller, Thomas and Mcwilliams, Brian and Rousselle, Fabrice and Gross, Markus and Nov\'{a}k, Jan},
title = {Neural Importance Sampling},
year = {2019},
issue_date = {October 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {38},
number = {5},
issn = {0730-0301},
url = {https://doi.org/10.1145/3341156},
doi = {10.1145/3341156},
abstract = {We propose to use deep neural networks for generating samples in Monte Carlo integration. Our work is based on non-linear independent components estimation (NICE), which we extend in numerous ways to improve performance and enable its application to integration problems. First, we introduce piecewise-polynomial coupling transforms that greatly increase the modeling power of individual coupling layers. Second, we propose to preprocess the inputs of neural networks using one-blob encoding, which stimulates localization of computation and improves inference. Third, we derive a gradient-descent-based optimization for the Kullback-Leibler and the χ2 divergence for the specific application of Monte Carlo integration with unnormalized stochastic estimates of the target distribution. Our approach enables fast and accurate inference and efficient sample generation independently of the dimensionality of the integration domain. We show its benefits on generating natural images and in two applications to light-transport simulation: first, we demonstrate learning of joint path-sampling densities in the primary sample space and importance sampling of multi-dimensional path prefixes thereof. Second, we use our technique to extract conditional directional densities driven by the product of incident illumination and the BSDF in the rendering equation, and we leverage the densities for path guiding. In all applications, our approach yields on-par or higher performance than competing techniques at equal sample count.},
journal = {ACM Trans. Graph.},
month = {oct},
articleno = {145},
numpages = {19},
keywords = {Monte Carlo, deep learning, importance sampling, normalizing flows, path guiding, rendering}
}

@article{Noe+2019,
	abstract = {Molecular dynamics or Monte Carlo methods can be used to sample equilibrium states, but these methods become computationally expensive for complex systems, where the transition from one equilibrium state to another may only occur through rare events. No{\'e} et al. used neural networks and deep learning to generate distributions of independent soft condensed-matter samples at equilibrium (see the Perspective by Tuckerman). Supervised training is used to construct invertible transformations between the coordinates of the complex system of interest and simple Gaussian coordinates of the same dimensionality. Thus, configurations can be sampled in this simpler coordinate system and then transformed back into the complex one using the correct statistical weighting. Science, this issue p. eaaw1147; see also p. 982 By combining deep learning and statistical mechanics, neural networks sample the equilibrium distribution of many-body systems. Computing equilibrium states in condensed-matter many-body systems, such as solvated proteins, is a long-standing challenge. Lacking methods for generating statistically independent equilibrium samples in ``one shot,'' vast computational effort is invested for simulating these systems in small steps, e.g., using molecular dynamics. Combining deep learning and statistical mechanics, we developed Boltzmann generators, which are shown to generate unbiased one-shot equilibrium samples of representative condensed-matter systems and proteins. Boltzmann generators use neural networks to learn a coordinate transformation of the complex configurational equilibrium distribution to a distribution that can be easily sampled. Accurate computation of free-energy differences and discovery of new configurations are demonstrated, providing a statistical mechanics tool that can avoid rare events during sampling without prior knowledge of reaction coordinates.},
	author = {Frank No{\'e} and Simon Olsson and Jonas K{\"o}hler and Hao Wu},
	doi = {10.1126/science.aaw1147},
	eprint = {https://www.science.org/doi/pdf/10.1126/science.aaw1147},
	journal = {Science},
	number = {6457},
	pages = {eaaw1147},
	title = {Boltzmann generators: Sampling equilibrium states of many-body systems with deep learning},
	url = {https://www.science.org/doi/abs/10.1126/science.aaw1147},
	volume = {365},
	year = {2019},
	bdsk-url-1 = {https://www.science.org/doi/abs/10.1126/science.aaw1147},
	bdsk-url-2 = {https://doi.org/10.1126/science.aaw1147}}

@inproceedings{Hoffmann+2019,
      title={NeuTra-lizing Bad Geometry in Hamiltonian Monte Carlo Using Neural Transport}, 
      author={Matthew Hoffman and Pavel Sountsov and Joshua V. Dillon and Ian Langmore and Dustin Tran and Srinivas Vasudevan},
      year={2019},
    booktitle       = {Symposium on Advances in Approximate Bayesian Inference},
      url={https://arxiv.org/abs/1903.03704}, 
}

@InProceedings{Papamakarios+2019,
  title = 	 {Sequential Neural Likelihood: Fast Likelihood-free Inference with Autoregressive Flows},
  author =       {Papamakarios, George and Sterratt, David and Murray, Iain},
  booktitle = 	 {Proceedings of the Twenty-Second International Conference on Artificial Intelligence and Statistics},
  pages = 	 {837--848},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Sugiyama, Masashi},
  volume = 	 {89},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {16--18 Apr},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v89/papamakarios19a/papamakarios19a.pdf},
  url = 	 {https://proceedings.mlr.press/v89/papamakarios19a.html},
  abstract = 	 {We present Sequential Neural Likelihood (SNL), a new method for Bayesian inference in simulator models, where the likelihood is intractable but simulating data from the model is possible. SNL trains an autoregressive flow on simulated data in order to learn a model of the likelihood in the region of high posterior density. A sequential training procedure guides simulations and reduces simulation cost by orders of magnitude. We show that SNL is more robust, more accurate and requires less tuning than related neural-based methods, and we discuss diagnostics for assessing calibration, convergence and goodness-of-fit.}
}

@InProceedings{Greenberg+2019,
  title = 	 {Automatic Posterior Transformation for Likelihood-Free Inference},
  author =       {Greenberg, David and Nonnenmacher, Marcel and Macke, Jakob},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {2404--2414},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/greenberg19a/greenberg19a.pdf},
  url = 	 {https://proceedings.mlr.press/v97/greenberg19a.html},
  abstract = 	 {How can one perform Bayesian inference on stochastic simulators with intractable likelihoods? A recent approach is to learn the posterior from adaptively proposed simulations using neural network-based conditional density estimators. However, existing methods are limited to a narrow range of proposal distributions or require importance weighting that can limit performance in practice. Here we present automatic posterior transformation (APT), a new sequential neural posterior estimation method for simulation-based inference. APT can modify the posterior estimate using arbitrary, dynamically updated proposals, and is compatible with powerful flow-based density estimators. It is more flexible, scalable and efficient than previous simulation-based inference techniques. APT can operate directly on high-dimensional time series and image data, opening up new applications for likelihood-free inference.}
}
@article{Hinton2002,
    author = {Hinton, Geoffrey E.},
    title = "{Training Products of Experts by Minimizing Contrastive Divergence}",
    journal = {Neural Computation},
    volume = {14},
    number = {8},
    pages = {1771-1800},
    year = {2002},
    month = {08},
    abstract = "{It is possible to combine multiple latent-variable models of the same data by multiplying their probability distributions together and then renormalizing. This way of combining individual “expert” models makes it hard to generate samples from the combined model but easy to infer the values of the latent variables of each expert, because the combination rule ensures that the latent variables of different experts are conditionally independent when given the data. A product of experts (PoE) is therefore an interesting candidate for a perceptual system in which rapid inference is vital and generation is unnecessary. Training a PoE by maximizing the likelihood of the data is difficult because it is hard even to approximate the derivatives of the renormalization term in the combination rule. Fortunately, a PoE can be trained using a different objective function called “contrastive divergence” whose derivatives with regard to the parameters can be approximated accurately and efficiently. Examples are presented of contrastive divergence learning using several types of expert on several types of data.}",
    issn = {0899-7667},
    doi = {10.1162/089976602760128018},
    url = {https://doi.org/10.1162/089976602760128018},
    eprint = {https://direct.mit.edu/neco/article-pdf/14/8/1771/815447/089976602760128018.pdf},
}
@inproceedings{Tieleman2008,
author = {Tieleman, Tijmen},
title = {Training restricted Boltzmann machines using approximations to the likelihood gradient},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390290},
doi = {10.1145/1390156.1390290},
abstract = {A new algorithm for training Restricted Boltzmann Machines is introduced. The algorithm, named Persistent Contrastive Divergence, is different from the standard Contrastive Divergence algorithms in that it aims to draw samples from almost exactly the model distribution. It is compared to some standard Contrastive Divergence and Pseudo-Likelihood algorithms on the tasks of modeling and classifying various types of data. The Persistent Contrastive Divergence algorithm outperforms the other algorithms, and is equally fast and simple.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {1064–1071},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}
@inproceedings{Tieleman-Hinton2009,
author = {Tieleman, Tijmen and Hinton, Geoffrey},
title = {Using fast weights to improve persistent contrastive divergence},
year = {2009},
isbn = {9781605585161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1553374.1553506},
doi = {10.1145/1553374.1553506},
abstract = {The most commonly used learning algorithm for restricted Boltzmann machines is contrastive divergence which starts a Markov chain at a data point and runs the chain for only a few iterations to get a cheap, low variance estimate of the sufficient statistics under the model. Tieleman (2008) showed that better learning can be achieved by estimating the model's statistics using a small set of persistent "fantasy particles" that are not reinitialized to data points after each weight update. With sufficiently small weight updates, the fantasy particles represent the equilibrium distribution accurately but to explain why the method works with much larger weight updates it is necessary to consider the interaction between the weight updates and the Markov chain. We show that the weight updates force the Markov chain to mix fast, and using this insight we develop an even faster mixing chain that uses an auxiliary set of "fast weights" to implement a temporary overlay on the energy landscape. The fast weights learn rapidly but also decay rapidly and do not contribute to the normal energy landscape that defines the model.},
booktitle = {Proceedings of the 26th Annual International Conference on Machine Learning},
pages = {1033–1040},
numpages = {8},
location = {Montreal, Quebec, Canada},
series = {ICML '09}
}

@inproceedings{Du-Mordatch2019,
	author = {Du, Yilun and Mordatch, Igor},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
	publisher = {Curran Associates, Inc.},
	title = {{Implicit Generation and Modeling with Energy Based Models}},
	url = {https://proceedings.neurips.cc/paper_files/paper/2019/file/378a063b8fdb1db941e34f4bde584c7d-Paper.pdf},
	volume = {32},
	year = {2019},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2019/file/378a063b8fdb1db941e34f4bde584c7d-Paper.pdf}}


@inproceedings{Nijkaml+2019,
	author = {Nijkamp, Erik and Hill, Mitch and Zhu, Song-Chun and Wu, Ying Nian},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
	publisher = {Curran Associates, Inc.},
	title = {Learning Non-Convergent Non-Persistent Short-Run MCMC Toward Energy-Based Model},
	url = {https://proceedings.neurips.cc/paper_files/paper/2019/file/2bc8ae25856bc2a6a1333d1331a3b7a6-Paper.pdf},
	volume = {32},
	year = {2019},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2019/file/2bc8ae25856bc2a6a1333d1331a3b7a6-Paper.pdf}}
